[
  {
    "title": "Mid-Junior DevOps Engineer - USA",
    "company": "HERE",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/mid-junior-devops-engineer-usa-at-here-4347377348?position=1&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=4WoYN7%2B7XqnzGzCpiEizTg%3D%3D",
    "description": "Mid-Junior DevOps Engineer\n\nLocation: New York, NY / Hybrid Remote / Remote within USA (EST / CST time zone)\n\nWe have an office in New York City and this position can either be based in the office, hybrid remote, or remote within the EST/CST time zones (subject to your existing legal right to work in the jurisdiction).\n\nAbout HERE\n\nEverything works right here‚Ñ¢.\n\nTraditional browsers weren't built for work. In today's enterprise environment‚Äîwhere security threats are constant and productivity is critical‚Äîlegacy browsers fall short. That's why we built HERE, the browser purpose-built for work.\n\nPowered by Chromium, HERE Enterprise Browser combines enterprise-grade security, seamless productivity, and native AI integration in one secure, intelligent workspace. Designed for regulated industries, HERE offers deep policy controls, identity-based access, secure workspace isolation, and full interoperability across SaaS, legacy, and virtualized environments. Our platform enables teams to work faster, more securely, and more intelligently‚Äîwithout compromise.\n\nHERE technology is trusted by 90% of global banks and also used within the U.S. Intelligence Community and other sectors. We're backed by some of the world's most respected financial institutions and venture firms, including Bain Capital Ventures, Bank of America, J.P. Morgan, Wells Fargo and IQT, the not-for-profit strategic investor that accelerates the introduction of groundbreaking technologies to enhance the national security of America and its allies.\n\nAbout the Role\n\nHERE is seeking a mid-junior DevOps Engineer to join our infrastructure team! The primary responsibilities for this role will span CI/CD pipeline engineering and cloud operations, maintaining and improving our GitHub and GitLab CI/CD pipelines, and supporting our AWS cloud infrastructure. In this role, you will gain hands-on experience with real production build systems and cloud platforms- while having the opportunity to work on practical projects that directly impact both our development velocity and operational reliability.\n\nWe're actively evolving toward a cloud-agnostic, multi-cloud architecture and migrating to Kubernetes for container orchestration. While current AWS and ECS experience is essential, having exposure to Azure, GCP, and Kubernetes will position you well for our infrastructure roadmap.\n\nThis role offers the opportunity to collaborate with senior engineers who will provide guidance and mentorship, whilst giving you ownership of projects across the DevOps lifecycle. This is an excellent platform for building practical experience with modern build engineering (CI/CD automation, cloud infrastructure, and deployment practices) within a production environment.\n\nResponsibilities\n\n\nCI/CD Pipeline Development:\nBuild, maintain, and optimize GitLab CI/CD pipelines for multi-platform builds (Windows, macOS, Linux).\nWork with YAML configurations, pipeline stages, artifacts, and deployment workflows.\nCloud Infrastructure Operations:\nHelp maintain and improve AWS infrastructure including ECS/Fargate deployments, RDS databases, Route53 DNS, VPC networking, and IAM policies.\nSupport multi-tenant and multi-region architecture.\nContainer & Deployment Management:\nWork with Docker containers, ECS task definitions, and ECR registries.\nDeploy and manage containerized Node.js applications in production environments.\nRelease Management:\nHelp manage release processes including version promotion, release channels (canary, beta, stable), and automated deployment to staging and production environments.\nDatabase Operations:\nSupport PostgreSQL on AWS RDS‚Äîbackups, SSH tunneling through bastion hosts, read-only user management, and database configuration for multi-tenant environments.\nAutomation & Scripting:\nWrite and maintain automation scripts in Bash, PowerShell, Python, and Node.js.\nBuild tools to improve infrastructure reliability and developer experience.\nInternal Tools Support:\nHelp maintain web-based DevOps tools built with Express.js, React, and TypeScript‚Äîtools for cloud settings management, tenant provisioning, and deployment monitoring.\n\nWhat We're Looking For\n\nIdeally 2 to 4 years of experience with the following core requirements:\n\n\nGitLab CI/CD: Experience with GitLab CI/CD pipelines‚ÄîYAML configuration, stages, jobs, artifacts, rules, dependencies.\nUnderstanding of CI/CD best practices and pipeline optimization.\nAWS Cloud Fundamentals: Practical experience with core AWS services‚ÄîEC2, ECS/Fargate, RDS, Route53, VPC, IAM, Secrets Manager, CloudWatch. Comfortable navigating the AWS Console and CLI.\nMulti-Platform Scripting: Solid scripting skills in Bash (Linux) and PowerShell (Windows). Ability to write maintainable automation scripts for both platforms.\nContainerization: Hands-on Docker experience‚Äîbuilding images, writing Dockerfiles, docker-compose, understanding container networking, and working with ECS/ECR.\nBuild Systems: Experience with build tools and package managers‚Äînpm/Node.js, .NET/NuGet, Python packaging. Understanding of dependency management and build artifacts.\nVersion Control: Strong Git fundamentals‚Äîbranching strategies, merge requests, tagging. Experience with GitHub (or GitLab) workflows and code review practices.\nLinux/Unix & Windows: Comfortable in both environments‚ÄîSSH, file permissions, package managers, systemd, PowerShell. Understanding of cross-platform operational challenges.\nNode.js/JavaScript: Comfortable reading and writing JavaScript/Node.js code. Experience with npm, package.json, and basic Express.js applications for tooling.\n\n\nNice to Have\n\n\nKubernetes experience (EKS, GKE, AKS) or willingness to learn, we're migrating from ECS to K8s\nMulti-cloud experience (Azure, GCP) or cloud-agnostic architecture knowledge\nGitLab Runner administration and configuration\nAWS CDK or CloudFormation for Infrastructure as Code\nTerraform for multi-cloud infrastructure management\nTypeScript development experience\nPostgreSQL database administration and optimization\n.NET build systems and NuGet package management\nReact or frontend framework experience\nAirflow or workflow orchestration tools\nHelm charts and Kubernetes manifest management\n\n\nWhat We're Offering\n\nBenefits -\n\n\nGenerous Paid Time Off, Paid Holidays & Sick Time\nCompetitive & Comprehensive Health Insurance\nThoughtfully-Planned Paid Parental Leave\nFinancial Well-Being Plans (FSA) (401k) (Life Insurance)\nStock Options\nProfessional Development Courses\nEmployee Resource Groups\n\n\nAdditional Perks -\n\n\nOne Medical - Free Membership\nTalkspace - Mental Health Therapy 24/7\nTeam Lunches\nCasual dress code\nCommuter Benefits (NYC employees only)\nCitibike (NYC employees only)\n\n\nLife at HERE\n\nAt HERE, we pride ourselves on fostering a friendly, collaborative, and supportive culture that truly respects the diversity of thought. Our goal is to create a space where employees can learn and innovate, and overall, have a good time doing it. We value and appreciate that our employees have a wide set of interests and experiences and put importance on taking the time to get to know one another and form relationships. From virtual socials and in-person events, to informal meetings and employee resource groups, we make it easy to engage and connect. Our environment promotes a productive, enjoyable learning experience - aligned together, working to create compelling solutions for our clients. Everything works right here.‚Ñ¢\n\nWe are HERE - Read about our recent rebrand from OpenFin to HERE\n\nRecent Awards\n\n\nVoted \"Enterprise Browser of the Year\" by CIO Review (2025)\nVoted \"100 Best Midsize Companies to Work For in NYC\" by BuiltIn (2025)\nVoted \"Top 10 Contact Center Technologies & Capabilities of 2024\" by CX Today (2024)\nVoted \"Best Enterprise Environment for Interoperability\" by TradingTech Insight Awards Europe (2024)\nVoted \"Top 50 Best Startups to Work for in the US\" & \"Top 50 Best Startups to Work for in New York\" by BuiltIn (2024)\nVoted as a \"Best Employer Award\" finalist at the UK FinTech Awards (2023)\nVoted \"Best FinTech Company CEO\" at the FinTech Breakthrough Awards (2023)\nVoted \"Best Internal Talent Team\" by Financial Technologist (2023)\nVoted \"Best Solution for Workflow Automation\" at the Trading Tech Insight Awards (2023)\nVoted \"Top Innovator Across Financial Markets\" in TabbFORUM NOVA Awards (2023)\nVoted \"Best User Interface Innovation\" in the Risk Markets Technology Awards (2023)\nVoted \"Top 100 Most Promising Private FinTech Companies\" by CB Insights (2023)\nVoted \"Most Influential Financial Technology Firm\" by Harrington Starr (2023)\n\n\nRECRUITERS NOTICE: Recruiters - if you wish to reach out to us regarding this job posting, you may reach out to externalrecruitment@here.io in order for your communication to be reviewed. HERE will review these communications if external help is needed for a position. Agencies may not contact individuals within our organization with solicitations. Firms that do not follow these guidelines risk having all communication from their firm being blocked. We thank you in advance for your cooperation in following our process.\n\nSponsorship - While we highly value all of our candidates, we are not offering sponsorship for this role.\n\nSalary Range: $70k - $120k\n\nSalary Range Disclaimer: This base salary range represents the low and high end salary range for this particular position; not all encompassing of the total compensation package. Actual salaries may vary depending upon but not limited to experience, special skill set, education and location. This range represents only one aspect of HERE's total compensation package offered to employees. Other forms of compensation may be stock options, commissions, paid time off and other variable benefits. Learn more about additional HERE compensation benefits above."
  },
  {
    "title": "junior devops engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-ova-work-4309344701?position=2&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=THU4aigkkfKy8A4BQest%2BA%3D%3D",
    "description": "Job Title: Junior DevOps Engineer\n\nLocation: Remote\n\nJob Type: Full-time\n\nExperience Level: Entry-Level (0-2 years)\n\nDepartment: IT / Engineering / DevOps\n\nJob Summary\n\nWe are looking for a motivated and detail-oriented Junior DevOps Engineer to join our growing DevOps team. This role is ideal for someone with a foundational understanding of DevOps practices and a passion for automation, cloud technologies, and continuous integration/deployment. You will assist in maintaining and improving our infrastructure, deployment pipelines, and monitoring systems.\n\nKey Responsibilities\n\n\nAssist in the setup, maintenance, and monitoring of CI/CD pipelines.\nSupport cloud infrastructure (AWS, Azure, GCP) and help manage deployments.\nCollaborate with development and operations teams to ensure reliable software delivery.\nWrite scripts and automation tools to streamline operations and deployments.\nMonitor system performance and troubleshoot issues in development and production environments.\nMaintain documentation for infrastructure and deployment processes.\nLearn and apply best practices in security, scalability, and reliability.\n\n\nRequired Qualifications\n\n\nBachelor's degree in Computer Science, Information Technology, or related field.\nBasic understanding of DevOps principles and software development lifecycle.\nFamiliarity with Linux/Unix systems and shell scripting.\nExposure to cloud platforms (AWS, Azure, or GCP).\nExperience with version control systems (e.g., Git).\nKnowledge of CI/CD tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nStrong problem-solving and communication skills.\nEagerness to learn and grow in a fast-paced environment.\n\n\nPreferred Qualifications\n\n\nInternship or project experience in DevOps or system administration.\nFamiliarity with containerization tools (Docker) and orchestration (Kubernetes).\nExperience with Infrastructure as Code (Terraform, Ansible).\nBasic knowledge of monitoring tools (Prometheus, Grafana, ELK Stack).\n\n\nBenefits\n\n\nCompetitive salary and growth opportunities.\nMentorship from senior engineers.\nHealth and wellness benefits.\nFlexible work hours and remote work options.\nAccess to training and certification programs."
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4341955705?position=3&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=voMTeOJraISKk8dt%2FwGsew%3D%3D",
    "description": "Over 12 -15 years of overall expereince needed.\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nKubernetes (3-4 YOE) and Fieldglass Experience (1-2 YOE)"
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4347005704?position=4&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=e9rtde0cgCWmAvwsXQQv0A%3D%3D",
    "description": "Bachelor's degree in a technical field such as computer science, computer engineering or related field required 0-2 years experience required.\n1-2 years of experience with Kubernetes.\nISBN experience preferred.\nA solid foundation in computer science , with strong competencies in data structures, algorithms, and software design large systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems experience in programming and experience with problem diagnosis and resolution."
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "GliaCell Technologies",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-gliacell-technologies-4338894490?position=5&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=36uXRnltvpOGN%2BEUUQO1iA%3D%3D",
    "description": "An active or rein-statable TS/SCI with Polygraph security clearance is REQUIRED. Please do not apply if you currently do not possess this level of clearance.***\n\n\nAre you a Junior DevOps Engineer who is ready for a new challenge that will launch your career to the next level?\n\n\nTired of being treated like a company drone?\nTired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?\nOur engineers were certainly tired of the same.\n\n\nAt GliaCell our slogan is ‚ÄúWe make It happen‚Äù.\n\n\nWe will immerse you in the latest technologies.\nWe will develop and support your own personalized training program to continue your individual growth.\nWe will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.\n\n\nCulture isn‚Äôt something you need to talk about‚Ä¶if it just exists.\n\nIf this sounds interesting to you, then we‚Äôd like to have a discussion regarding your next adventure! If you want to be a drone, this isn‚Äôt the place for you.\n\nWe Make It Happen!\n\nGliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.\n\nGliaCell‚Äôs Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.\n\nWe Offer\n\n\nLong term job security\nCompetitive salaries & bonus opportunities\nChallenging work you are passionate about\nAbility to work with some amazingly talented people\n\n\nJob Description\n\nGliaCell is seeking a Junior DevOps Engineer on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.\n\nResponsibilities\n\n\nEstablishing a test framework and automated tests utilizing Cucumber and Cypru\nKnowledgeable in Microservices design & architecture, CI/CD, Test frameworks and automation, Agile Methodology.\nExecute load and performance testing, chaos testing, functional testing and end-to-end testin\nAgile development and delivery of software\nCommunication and collaboration: Software Development is a team-oriented discipline. Engineers need to be able to communicate and collaborate effectively with other team members, as well as with stakeholders.\n\n\nRequired Skills\n\n\nPython and Cucumber\n\n\nDesired Skills:\n\n\nAWS services such as Lambdas, Step Functions, EC2 and S3\n\n\nKey Requirements\n\nTo be considered for this position you must have the following:\n\n\nPossess an active or rein-statable TS/SCI with Polygraph security clearance.\nU.S. Citizenship.\nWorks well independently as well as on a team.\n6+ years experience as a Developer in programs and contracts of similar scope, type, and complexity is required. A bachelor‚Äôs degree in a technical discipline from an accredited college or university is required. Five (4) years of development experience may be substituted for a bachelor‚Äôs degree.\n\n\nLocation: Annapolis Junction, MD\n\nSalary Range: The salary range for this full-time position is $50,000 to $120,000. Our salary ranges are determined by position, level, skills, professional experience, relevant education and certifications. The range displayed on each job posting reflects the minimum and maximum target salaries for this position across our projects. Within the range, your salary is determined by your individual benefits package selection. Your recruiter can share more about the specific salary range for your preferred position during the hiring process.\n\nBenefits\n\n\nMedical, Dental, and Vision Coverage for Employee and Dependents\nUp to 25 Days of Paid Time Off\nUp to 40 hours of PTO Carryover\n11 Federal Government Holidays\nWork From Home Opportunities\n401K Company Contribution, Fully Vested Day 1\nDiscretionary, Certification, and Sign-On Bonus Potential\nEmployee Referral Bonus Program\nAnnual Professional Development\n100% Premium Covered for Life & Disability Insurances\nAdditional Voluntary Life Insurance Coverage Available\nEmployee Assistance Program\nTravel Protection Program\nFinancial Planning Assistance\nBereavement and Jury Duty Leave\nMonthly Team and Family Events\nTechnology Budget\nGlobal Entry\nAnnual Swag Budget\n\n\nLearn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/\n\nGliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
  },
  {
    "title": "DevOps Cloud Engineer Based in U.S.A",
    "company": "Advancio",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-cloud-engineer-based-in-u-s-a-at-advancio-4324442139?position=6&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=CgsJD6qhtwkrem4%2FidgvnQ%3D%3D",
    "description": "This is a remote position.\n\nWho We Are:\n\n\nAt Advancio, we are passionate about technology and its ability to transform the world. We are rapidly expanding and building a company where we serve exceptional businesses, hire top talent, and have a lot of fun doing what we love!\n\n\nJob Summary:\n\nWe are seeking a skilled DevOps Cloud Engineer to design, implement, and manage scalable cloud-based infrastructure and DevOps processes. The ideal candidate will have extensive experience with cloud platforms, CI/CD pipelines, and automation tools, ensuring the efficient deployment and operation of applications.\n\n\nWhat will you do:\n\n\nDesign, deploy, and manage cloud infrastructure on platforms such as AWS, Azure, or Google Cloud Platform (GCP).\n\nBuild and maintain CI/CD pipelines to streamline development and deployment processes.\n\nAutomate infrastructure provisioning, configuration, and monitoring using tools like Terraform, Ansible, or similar.\n\nEnsure system reliability, availability, and performance through robust monitoring and alerting.\n\nCollaborate with development teams to optimize the delivery and scalability of applications.\n\nManage containerized workloads using Docker and orchestration platforms such as Kubernetes.\n\nImplement security best practices for cloud environments, including identity management, encryption, and compliance adherence.\n\nStay updated with the latest DevOps tools and methodologies to enhance team efficiency.\n\n\n\n\nRequirements\n\n\n\n\n\n\n5+ years of experience in DevOps, cloud engineering, or related roles.\n\nAdvanced English communication skills, both verbal and written.\n\nProficiency in at least one major cloud platform (AWS, Azure, or GCP).\n\nHands-on experience with CI/CD tools (e.g., Jenkins, GitLab CI/CD, CircleCI).\n\nStrong scripting skills in Python, Bash, or similar languages.\n\nSolid knowledge of infrastructure-as-code (IaC) tools like Terraform or CloudFormation.\n\nExperience with containerization (Docker) and orchestration (Kubernetes).\n\nFamiliarity with monitoring and logging tools like Prometheus, Grafana, or ELK Stack.\n\nStrong understanding of networking, security, and system architecture."
  },
  {
    "title": "DeVops Engineer",
    "company": "Pittsburgh Robotics Network",
    "location": "Pittsburgh, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pittsburgh-robotics-network-4347073200?position=7&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=bYxoP5cUDQ41zaK2mzAdOg%3D%3D",
    "description": "DevOps Engineer\n\nTDK SensEI\n\nPittsburgh, PA\n\nThis position is for our Pittsburgh, PA office - only apply if you are based there or willing to relocate.\n\nAt TDK SensEI, we are transforming how industrial customers utilize and interact with sensor data. We specialize in developing advanced AI solutions capable of running directly on edge devices. By processing data locally, TDK SensEI enhances real-time decision-making, privacy, security, and cost efficiency. Our offerings include automated machine learning tools, AI-powered condition-based monitoring systems, and various sensor devices optimized for low latency and power consumption. Collaborating with leading global companies, we empower teams to effortlessly devise and implement machine learning solutions for industrial applications, all without the need for coding.\n\nWe are seeking a Dev Ops engineer to join our team. In this position, the candidate will be responsible for managing, operating, and provisioning cloud environments such as AWS, Azure, Google cloud. You will work with development, security, and operations teams to deploy, scale, and operate dev environments. You are also responsible for improving and automating the dev environment. You will establish configuration management, automate our infrastructure, implement continuous integration, and train the team in DevOps best practices.\n\nAs a Dev Ops Engineer, Your Responsibilities Will Include\n\n\nDesigning, implementing, and maintaining tools and processes for continuous integration, delivery, and deployment of software\nWorking with developers to deploy and manage code changes\nWorking with operations staff to ensure that systems are up and running smoothly\nAutomating, monitoring, testing, configuring, networking, and Infrastructure as Code (IaC)\nStreamlining and automating processes while troubleshooting existing development procedures\nManaging the creation, release, and configuration of production systems\nArchitecting and optimizing several service components running on AWS environment\n\n\nSkills & Requirements\n\n\nBachelor‚Äôs degree or equivalent experience\nMinimum 2 years of experience in DevOps, infrastructure automation or similar role\nKnowledge of Linux/UNIX administration\nProficiency in Python, JavaScript and other script environments (e.g. bash)\nExperience with containerization technologies, Docker and associated tooling\nExperience designing and implementing CI/CD pipelines\nExperience operating databases such as PostgreSQL or MySQL, especially in cloud-native services like RDS\nAwareness of critical concepts in DevOps and Agile principles\nUS work authorization\n\n\nNice To Have\n\n\nAWS certifications (e.g., AWS Certified DevOps Engineer, AWS Certified Solutions Architect).\nFamiliarity with other cloud providers (Azure, Google Cloud).\nExperience with container orchestration systems such as Kubernetes/EKS"
  },
  {
    "title": "DevOps Engineer",
    "company": "Princeton IT Services, Inc",
    "location": "Englewood Cliffs, NJ",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-princeton-it-services-inc-4338714288?position=8&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Pgf9nt5Gw2GVC2ESZ9thmg%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Englewood Cliffs, NJ\n\nEmployment Type: W2 Only\n\nJob Summary\n\nWe are seeking a DevOps Engineer with strong hands-on experience in Linux, Docker, and Kubernetes to support and optimize our deployment environment in Englewood Cliffs, NJ. This is a W2-only role requiring solid skills in automation, CI/CD, and container orchestration. The ideal candidate will ensure smooth application releases, maintain system stability, and collaborate closely with development teams.\n\nKey Responsibilities\n\n\nManage and support Linux-based systems in production and staging environments.\nBuild, maintain, and optimize CI/CD pipelines for automated deployments.\nCreate, manage, and troubleshoot Docker containers and images.\nDeploy, monitor, and tune Kubernetes clusters and workloads.\nAutomate infrastructure tasks using Shell or Python scripts.\nImplement and manage monitoring and logging tools (Prometheus, Grafana, ELK, etc.).\nTroubleshoot system, container, and cluster-level issues end-to-end.\nWork cross-functionally with development and QA teams to ensure smooth releases.\n\n\nRequired Skills\n\n\n8+ years of DevOps or related experience.\nStrong hands-on experience with Linux administration.\nSolid experience working with Docker for containerization.\nStrong working knowledge of Kubernetes (deployments, scaling, troubleshooting).\nExperience building CI/CD pipelines (Jenkins, GitLab CI, GitHub Actions).\nStrong scripting skills in Shell/Bash/Python.\nExperience with monitoring and logging tools."
  },
  {
    "title": "DevOps Engineer",
    "company": "Lean TECHniques",
    "location": "Johnston, IA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lean-techniques-4336685413?position=9&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Tc9n3v6%2Bgp8MBxrJNOShUA%3D%3D",
    "description": "Maybe you‚Äôre bored and need a new challenge. Or you‚Äôre sick of all the bureaucracy and just want to focus on designing kick-ass software.\n\nWhatever the reason, we want you to know that LT is different. And not just air quotes ‚Äúdifferent,‚Äù but more like ‚Äúbreathing easy for the first time in a long time‚Äù different.\n\nIt‚Äôs a place where you can write your own story and make a difference along the way. At LT, you‚Äôll have the freedom and flexibility to do what you think needs to be done, and you‚Äôll get to do it while working alongside a team of other curious individuals who love a good challenge too.\n\nWe‚Äôre currently looking to add a DevOps Engineer to our crew of nerds. If you‚Äôre someone who has 5+ years of DevOps experience, we'd love to chat!"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337132819?position=10&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=BUU%2FyXgcs7x1IVS82WmFIg%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "DevOps Engineer (35 LPA - 55 LPA)",
    "company": "CodeRound AI",
    "location": "Greater Bloomington Area",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-35-lpa-55-lpa-at-coderound-ai-4308183910?position=11&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Dbj6zWXcRF%2BVQbO8nMNiZA%3D%3D",
    "description": "üöÄ What We‚Äôre Building\n\n\nCodeRound AI matches top 5% tech talent to fastest growing VC funded AI startups.\nCandidates apply once and get UPTO 10 remote as well as onsite interview opportunities IF selected!\nTop-tier product startups in US, UAE & India have hired top engineers & ML folk using CodeRound\n\n\nüß© What You‚Äôll Do\n\n\nBuild and optimize our cloud infrastructure ‚Äî scalable, secure, and cost-effective (mostly AWS).\nSet up and manage CI/CD pipelines to ensure smooth deployment across backend, AI services, and mobile.\nContainerize backend services (FastAPI, Rails) and optimize them for performance.\nImplement monitoring, alerting, and logging to catch issues before users do.\nOptimize database performance (Postgres, Redis) and manage backups and scaling.\nCollaborate with backend, AI, and product teams to deploy new features safely and quickly.\nChampion infra-as-code and automation wherever possible.\n\n\nüí• Why this is exciting\n\n\nYou'll own DevOps for a high-usage, real-world AI platform ‚Äî not just internal tools.\nYou‚Äôll work on real-time, high-stakes flows ‚Äî interviews, scoring, hiring decisions.\nYou‚Äôll work closely with founders, ship weekly, and see the direct impact of your work.\n\n\n‚úÖ You‚Äôll Be Great At This If You\n\n\nHave 4+ years of experience as a DevOps engineer, SRE, or infrastructure engineer.\nAre strong with AWS services (EC2, RDS, ECS/EKS, S3, CloudWatch).\nCan write clean, reusable Terraform or CloudFormation code.\nHave experience setting up CI/CD pipelines and optimizing build/release flows.\nAre comfortable with Docker, Linux servers, and basic networking (VPCs, security groups).\nUnderstand application and database scaling (horizontal/vertical).\n\n\n‚ö° Bonus If You\n\n\nHave experience supporting AI/ML pipelines in production (fine-tuning infra, vector DBs, etc.).\nKnow cost optimization tricks for cloud infra (spot instances, autoscaling groups, etc.).\nAre excited to eventually build a small infra team"
  },
  {
    "title": "Devops Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341915759?position=12&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=etA5ALEog9gY82FJmAUS%2FA%3D%3D",
    "description": "Job Description:\n\n\nServe as a subject matter expert to develop and support DevOps Web Access Management solutions\nInstall, configure, and maintain automation solutions, in support of KeyBank infrastructure\nDevelop Standard Operating Procedures, maintenance plans and provide status reports as required\nPerform daily operational tasks as required for the Web Access Management team\n\n\nQualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "Huntington Beach, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337182535?position=13&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=pd53zZ4Rl7%2Bh4hAeLCQixg%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "Devops Engineer",
    "company": "Hoplite Solutions LLC",
    "location": "Bethesda, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hoplite-solutions-llc-4336082750?position=14&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=eL5ya7JZmHeZwk8tk9HKHw%3D%3D",
    "description": "Hoplite Solutions is hiring DevOps Engineers at all experience levels to join our team in Bethesda, MD. In this mission-critical role, you will provide essential system support to our customer while collaborating closely with software development teams and other key technology stakeholders. You will help maintain, enhance, and support a range of IC enterprise products‚Äîboth legacy systems and new solutions‚Äîwithin an Agile SAFe environment.\n\nAs a DevOps Engineer, you will work hand-in-hand with software engineering teams to deploy and operate systems, automate and optimize processes, and build and maintain tools that support deployment, monitoring, and ongoing operations. You will also troubleshoot and resolve issues across development, test, and production environments, ensuring reliability, efficiency, and continuous improvement across the enterprise.\n\nPrimary Responsibilities:\n\n\nSupports software deployments, cloud infrastructure baselines, and operational availability of production systems\nManaging, building, configuring, administering, operating and maintaining all components that comprise the DevOps environment\nDefining enterprise Continuous Integration/Continuous Deployment processes and best practices\nCodifying DevOps best practices across the enterprise\nDeveloping and maintaining scripts to automate tool deployment to an AWS cloud environment and other tasks\nScripting and maintaining build environments\nWorking with project teams to integrate their products into the DevOps environment\n\n\nBasic Qualifications\n\n\nDemonstrated experience setting up one or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience troubleshooting issues with two or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience working within a software development team and supporting developers and developer activities\nBachelors degree with 4 or more years of prior relevant work experience or Masters with 2 or more years of prior relevant work experience. Will consider additional work experience in lieu of a degree\nTo be considered must have an active TS/SCI with polygraph security clearance\n\n\nPreferred Qualifications\n\n\nAWS Associate Certification (Developer, Solution Architect, or Sys Ops Administrator)\nAWS Professional Certification (DevOps Engineer or Solutions Architect)\nDemonstrated experience in container orchestration using Docker, Vagrant, Kubernetes, or AWS ECS/ECR\nDemonstrated experience with Languages including Java, Python, JavaScript, Ruby, PHP, and Unix shell Scripting\nDemonstrated experience with Ansible, or Puppet\n\n\nHoplite Solutions offers very competitive salaries and an excellent benefits package, to include a 7% employer 401k contribution, fully paid healthcare for our employees, outstanding training benefits, company funded life insurance and short-term disability insurance, and many more.\n\nPowered by JazzHR\n\nwwBe8pS8mn"
  },
  {
    "title": "DevOps Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341985652?position=15&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Tnp%2FEDp%2BS0Rt%2BnPb%2FrCSHA%3D%3D",
    "description": "Qualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments."
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "Indianapolis, IN",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4339356296?position=16&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=2N2C7lNmqhwwpqh3aSTUtw%3D%3D",
    "description": "Who we are‚Ä¶\n\nVerra Mobility is a global leader in smart mobility. We develop technology-enabled solutions that help the world move safely and easily. We are fostering the development of safe cities, working with police departments and municipalities to install over 4,000 red-light, speed, and school bus stop arm safety cameras across North America. We are also creating smart roadways, serving the world's largest commercial fleets and rental car companies to manage tolling transactions and violations for over 8.5 million vehicles. And we are a leading provider of connected systems, processing nearly 165 million transactions each year across 50+ individual tolling authorities.\n\nCulture\n\nVerra Mobility Corporation is a rapidly-growing, entrepreneurial company that operates with a people-first philosophy and approach. The company lives by its core values‚ÄîDo What's Right, Lead with Grace, Win Together, and Own It‚Äîin everything it does for its customers and team members. The company seeks to grow aggressively, both organically and through acquisition, to continue to be the undisputed market leader with these five core competencies: bias for action, customer focus, teamwork, drive for results, and commitment to excellence.\n\nPosition Overview:\n\nWe are seeking an experienced and detail oriented Devops Engineer to join our team. In this role, you will be responsible for creating, maintaining, and securing our Devops pipelines and deployment systems to ensure high levels of performance and availability. This position is ideal for someone with a strong background in CI/CD methodologies particularly in cloud environments.\n\nEssential Responsibilities:\n\n\nSpend 50% writing automation scripts in Python and Bash.\nWrite various CI/CD pipelines for code releases.\nEnsure that pipelines meet both operations and security requirements.\nPartner with developers to identify areas of improvement in the developer experience.\nDesign and implement innovations that improve software velocity, infrastructure resiliency, security and data availability.\nWork with Software and Engineering to ensure new pipelines are created in parallel to code build.\nWork with Architecture on setting the path forward and gathering changes to the technology stack.\nAbility to respond to system issues, drive and participate in high - priority incident calls and emergency activities outside of standard office hours as needed.\nCollaborate with internal and external application, business partners to gain understanding of their business needs and adapt departmental roadmap plans and priorities to address operational challenges.\nWork with QE to ensure all automated testing is run during the deployment of the code.\nAbility to participate in an on-call rotation as needed.\n\n\nQualifications:\n\n\nMust have 5 years of Devops Engineering experience.\nFamiliarity with a wide range of systems engineering tools, including source code repository hubs, continuous integration services, issue tracking, test automation, deployment automation, development team collaboration, project management.\nNeed to have strong scripting skills to create automation in Python preferred or Bash.\nExperience with Cloudformation or Terraform for infrastructure as code.\nUsed continuous integration and continuous development (CI/CD) tools such as Jenkins, Gitlab, or Github Actions, preferred.\nKnowledge of DevOps tools such as, GitHub Actions, CloudFormation, GIT, SVN, Jenkins, JIRA, Rally, Greenhopper, Puppet/Chef Vagrant, Selenium, Azure DevOps (for sprint planning).\nUnderstanding of enterprise GIT repositories including branching and forking.\nHands-on Familiarity with AWS CloudWatch, AWS CloudTrail, AWS X-Ray, Grafana, and Prometheus.\nHands-on experience with Veracode and SonarQube are a plus.\nMust be located in Phoenix, AZ, Indianapolis, IN, or NY and be willing to commute into office 3 days a week.\n\n\nThis position is not eligible for sponsorship now or in the future and is only considering local Arizona, New York, or Indiana talent.\n\n\n\nVerra Mobility Values\n\n\n\nAn ideal candidate for this role naturally works in alignment with the Verra Mobility Core Values:\n\n\nOwn It. We focus on high performance and drive toward breakthrough outcomes. Our employees ensure accountability, optimize and align work, focus on the customer, and cultivate innovation.\nDo What's Right. We champion integrity and good character. Our team members model ethical behavior, demonstrate good judgment and are courageous.\nLead with Grace. We express humility and compassion, and we are authentic and candid. Our employees demonstrate self-awareness, care for others, instill trust, and communicate effectively.\nWin Together. We believe in growing and inspiring people together. We seek people who collaborate, value differences, think and act globally, foster an engaging work environment, and recognize and develop others.\n\n\n\n\nWith your explicit consent which you provided as part of the application process, we will retain candidate personal data solely for the business purpose for which it was collected. In no event will we retain such data more than two (2) years following the closure of the recruitment process relating to the role for which you applied or in the event other related job opportunities arise within the company. Verra Mobility Applicant Privacy Notice\n\nVerra Mobility is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status."
  },
  {
    "title": "DevOps Engineer",
    "company": "Protege",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-protege-4331315574?position=17&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=0%2BlzXAWYrHhy0UF78FIOPQ%3D%3D",
    "description": "Company Overview:\n\nWe are building Protege to solve the biggest unmet need in AI ‚Äî getting access to the right training data. The process today is time intensive, incredibly expensive, and often ends in failure. The Protege platform facilitates the secure, efficient, and privacy-centric exchange of AI training data.\n\nSolving AI‚Äôs data problem is a generational opportunity. We‚Äôre backed by world-class investors and already powering partnerships with some of the most ambitious teams in AI. The company that succeeds will be one of the largest in AI ‚Äî and in tech.\n\nWe‚Äôre a lean, fast-moving, high-trust team of builders who are obsessed with velocity and impact. Our culture is built for people who thrive on ambiguity, own outcomes, and want to shape the future of data and AI.\n\nKey Responsibilities and Scope:\n\n\nAs a DevOps Engineer, you will be a critical part of our engineering team, responsible for safeguarding our AI/ML platforms, data pipelines, and cloud infrastructure\nYou will implement and develop monitoring strategies, and drive controls to protect our most valuable assets\n\n\nQualifications:\n\n\n4+ years of hands-on experience in a DevOps, Architecture, SecOps or Engineering role\nStrong experience with major cloud platforms and building cloud-native services including containerization, threat detection, vulnerability, governance, compliance, etc. with AWS preferred\nProficient in scripting languages like Python, SQL, Typescript or similar\nStrong experience with infra‚Äëas‚Äëcode, monitoring, and reliability for pipelines; contributing to platform guardrails, governance, compliance, etc\nExperience working with cross-functional partners to develop tools and playbooks for best-practices related to Operations\n\n\nAbout You:\n\n\nYou are curious, tenacious, and proactive\nYou are not bothered by ambiguity but embrace finding patterns in complex environments\nExcellent problem-solving skills and adaptability in a dynamic and evolving tech landscape\nExcited to work in a company that deals with moving and transforming large volumes of data\n\n\nBonus if you have these attributes:\n\n\nExperience with cloud providers like GCP and Azure\nPrior startup experience\nSecurity operations and automation experience"
  },
  {
    "title": "Devops Engineer",
    "company": "PDG Consulting",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pdg-consulting-4321885957?position=18&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=8Pne6OGN%2FvlpHrb4GKj2VQ%3D%3D",
    "description": "Overview\n\nWe are seeking a DevOps Engineer to set up, manage, and automate software development operations and processes. The ideal candidate will have strong experience in CI/CD pipelines, cloud service management, and infrastructure monitoring to support efficient, secure, and scalable software delivery.\n\nResponsibilities\n\n\nDesign, implement, and manage CI/CD pipelines to streamline software deployment and integration.\nOversee cloud-based systems and infrastructure management, ensuring reliability and performance.\nAutomate workflows for system administration, documentation, and monitoring.\nSupport the development and deployment of AI chatbot infrastructures and related frameworks.\nCollaborate with developers, QA engineers, and IT teams to optimize the software lifecycle.\n\n\nRequirements\n\n\nMinimum 4 years of experience in ICT systems support, including system administration, documentation, and monitoring.\nHands-on experience with cloud platforms, especially Amazon Web Services (AWS).\nProven experience creating and maintaining AI chatbot infrastructures or similar automation frameworks.\nPrevious experience working within the UN system is an advantage.\nExcellent command of English (required).\nKnowledge of French and Arabic is considered an advantage.\n\n\nPowered by JazzHR"
  },
  {
    "title": "DevOps Engineer - 100% Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-100%25-remote-at-the-dignify-solutions-llc-4347005722?position=19&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=SWwzN6AGVXkNPHMoZ%2BPtpw%3D%3D",
    "description": "Summary: The main function of a DevOps Engineer is to design, develop, implement, test, and maintain business and computer applications software or specialized utility programs including mainframe and client/server applications, and major enhancement of existing systems\n\nJob Responsibilities: Fine-tune and improve a variety of sophisticated software implementation projects Gather and analyze system requirements, document specifications, and develop software solutions to meet client needs and data Analyze and review enhancement requests and specifications Implement system software and customize to client requirements Prepare the detailed software specifications and test plans Code new programs to client's specifications and create test data for testing Modify existing programs to new standards and conduct unit testing of developed programs Create migration packages for system testing, user testing, and implementation Provide quality assurance reviews Perform post-implementation validation of software and resolve any bugs found during testing\n\n\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nSAC Experience (1-2 YOE)\nAriba ATHENA Report generation (Some experience)"
  },
  {
    "title": "CloudOps Engineer",
    "company": "Protera",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/cloudops-engineer-at-protera-4336621571?position=20&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=IY7YKpgqiYPGJ%2BpBnSNOrQ%3D%3D",
    "description": "Summary\n\nAs a CloudOps Engineer at Protera, you will play a crucial role in maintaining and optimizing our cloud infrastructure. You will be responsible for monitoring and managing cloud services, ensuring the performance and reliability of our cloud applications, and automating processes to enhance operational efficiency. You will work closely with development teams to improve deployment practices and manage incidents in a fast-paced environment.\n\nKey Responsibilities\n\n\nMonitor cloud infrastructure performance and reliability to ensure optimal service delivery\nAutomate deployment processes using Infrastructure as Code (IaC) tools like Terraform\nImplement and manage CI/CD pipelines to streamline application releases\nCollaborate with development teams to integrate DevOps practices into application lifecycles\nTroubleshoot cloud architecture and application issues to ensure minimal downtime\nConduct security assessments and implement best practices to secure cloud environments\nDocument processes and maintain configurations and operational standards\n\n\nRequirements\n\nSkills & Qualifications\n\nExperience:\n\n\n3+ years of experience in cloud operations, DevOps, or system administration\n\n\nTechnical Skills:\n\n\nProficient with AWS services and cloud architecture\nExperience with Infrastructure as Code (IaC) tools, particularly Terraform\nStrong understanding of containerization technologies like Docker and orchestration tools such as Kubernetes\nFamiliarity with CI/CD tools such as Jenkins, GitLab CI, or similar\nKnowledge of monitoring tools and log management solutions\nSolid troubleshooting skills across cloud-based systems\n\n\nEducation:\n\n\nBachelor's degree in Computer Science, Information Technology, or a related field is preferred\n\n\nCertifications (Preferred):\n\n\nAWS Certified Solutions Architect or related cloud certification\nDevOps or Kubernetes certifications are a plus\n\n\nPersonal Attributes:\n\n\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills\nAbility to work in a fast-paced environment and handle multiple tasks\n\n\nAbout Protera\n\nProtera Technologies (www.protera.com) is a leading provider of total IT outsourcing solutions for SAP-centric organizations. Founded in the mid-1990s, we are pioneers in providing SAP services on the cloud, managing thousands of workloads across various cloud platforms. With headquarters in Chicago and offices in Greece and India, we are committed to delivering exceptional cloud hosting, application management, and professional services globally.\n\nBenefits\n\nProtera offers a variety of health and wellbeing programs. Benefit options include two PPO Medical plans, Dental, Vision, Health Savings Account, Flexible Spending Accounts, Dependent Care FSA, 401k retirement savings plan, company paid Life Insurance, Flexible PTO policy, Paid Holidays."
  },
  {
    "title": "DevOps Systems Engineer",
    "company": "TensorWave",
    "location": "Las Vegas, NV",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-tensorwave-4338727303?position=21&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=0qqLrHrqBz5IX0E%2FWO%2BqwA%3D%3D",
    "description": "At TensorWave, we‚Äôre leading the charge in AI compute, building a versatile cloud platform that‚Äôs driving the next generation of AI innovation. We‚Äôre focused on creating a foundation that empowers cutting-edge advancements in intelligent computing, pushing the boundaries of what‚Äôs possible in the AI landscape.\n\nAbout The Role\n\nWe are seeking a highly skilled DevOps & Infrastructure Management Engineer to join our growing infrastructure team. This role is ideal for someone who thrives in hardware-centric environments, enjoys hands-on datacenter and system administration work, and can build reliable automation around large-scale infrastructure. You will be responsible for managing enterprise hardware, monitoring systems, network operations, infrastructure automation, and supporting our compute clusters across multiple data centers.\n\nThis role touches every layer of modern infrastructure‚Äîfrom bare metal provisioning, to OS and Kubernetes management, to monitoring and troubleshooting hardware. If you are detail-oriented, resourceful, and comfortable working with both low-level hardware systems and higher-level DevOps tooling, we‚Äôd love to talk.\n\nKey Responsibilities\n\nHardware & Infrastructure Management\n\n\nManage and maintain enterprise-grade server hardware and infrastructure components.\nUtilize out-of-band management systems (iLO, iDRAC, IPMI, Redfish, etc.) for remote operations.\nUse automated hardware management tools (BMC/Redfish-based) to streamline provisioning and maintenance.\nPerform hardware diagnostics and troubleshooting (CPU, memory, disks, PSUs, NICs, etc.).\nHandle vendor interactions, including RMAs, part replacements, and inventory tracking.\nOversee datacenter hardware operations, including racking, cabling, PDU installation, and physical layout.\n\n\nDatacenter & DCIM\n\n\nUse Data Center Infrastructure Management (DCIM) tools for inventory, capacity planning, and environmental tracking.\nManage power delivery and consumption across racks and nodes.\nConfigure and monitor managed PDU systems for power cycling, monitoring, and alerts.\nCollaborate with colocation providers on connectivity, power, security, and maintenance tasks.\n\n\nMonitoring & Observability\n\n\nBuild and maintain infrastructure monitoring and alerting using tools such as Prometheus/Grafana, SNMP, Nagios, CheckMK, or similar platforms.\nImplement automated alerting for hardware health, network status, power issues, and service-level metrics.\nCreate dashboards to give internal teams visibility into system performance and reliability.\n\n\nNetwork Operations\n\n\nManage and configure firewalls, routing, and network segmentation.\nConfigure and troubleshoot VPN technologies (IPsec, OpenVPN, WireGuard).\nOversee subnetting, IP address allocation, and network architecture planning.\nConfigure managed switches, VLANs, port settings, and trunking.\nManage NAT, port forwarding, and related gateway/edge network configurations.\n\n\nSystem Administration (Linux)\n\n\nInstall, configure, and manage Linux servers (Ubuntu/Debian preferred).\nPerform system-level troubleshooting (boot issues, login problems, service failures).\nManage networking configuration (static IPs, DHCP).\nConfigure and maintain filesystems: partitioning, MD RAID, ext4/XFS, LVM, resizing/growing volumes.\nImplement secure access using public key authentication and proper SSH hardening.\nManage certificates for internal systems, including issuance, revocation, HTTPS installation, and rotation.\nHandle basic BIOS configuration relevant to bare metal provisioning or system bring-up.\n\n\nBare Metal Provisioning\n\n\nDeploy and manage hardware provisioning tools such as MAAS, Foreman, or similar systems.\nConfigure and troubleshoot network boot mechanisms (PXE, UEFI Boot, HTTP Boot).\nAutomate provisioning pipelines to rapidly bring new nodes online.\n\n\nContainerization & Orchestration\n\n\nWork with Kubernetes clusters at a foundational level (cluster access, basic resource troubleshooting).\nDeploy workloads using Helm charts and maintain cluster application lifecycle.\nAssist with cluster scaling, node replacements, and security hardening.\n\n\nAutomation & Scripting\n\n\nWrite shell scripts (bash) for automation of system tasks, monitoring, or provisioning.\nUse CLI tooling such as jq, sed, awk, grep, and rsync.\nOptionally automate workflows using languages like Python, Go, PHP, or Perl.\n\n\nRequired Qualifications\n\n\nProven experience managing enterprise-grade hardware at scale.\nStrong understanding of out-of-band management systems (IPMI/BMC/Redfish).\nHands-on expertise with monitoring systems (Prometheus, Grafana, SNMP, Nagios, CheckMK, or similar).\nSolid knowledge of network administration, including firewalls, routing, VPNs, NAT, and managed switches.\nLinux system administration experience (installation, configuration, troubleshooting).\nExperience with filesystems, RAID, partitioning, and general storage management.\nFamiliarity with certificate management, key-based auth, and basic cryptographic functions.\nExperience with bare metal provisioning (MAAS, Foreman, or similar).\nUnderstanding of PXE/UEFI/HTTP boot systems.\nAbility to write functional, maintainable bash scripts for automation.\n\n\nNice to Have\n\n\nExperience with Kubernetes beyond the basics (operators, cluster scaling, CRDs).\nExperience with Helm chart customization.\nFamiliarity with automation languages such as Python, Go, PHP, or Perl.\nPrevious datacenter operations or colocation management experience.\nExposure to high-availability or distributed compute environments.\nKnowledge of infrastructure security and hardening practices.\n\n\nWhat We Bring\n\n\nStock Options\n100% paid Medical, Dental, and Vision insurance\nLife and Voluntary Supplemental Insurance\nShort Term Disability Insurance\nFlexible Spending Account\n401(k)\nFlexible PTO\nPaid Holidays\nParental Leave\nMental Health Benefits through Spring Health"
  },
  {
    "title": "DevOps Administrator",
    "company": "The Amatriot Group",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-administrator-at-the-amatriot-group-4310974393?position=22&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=u%2FPYBucx35YLABr1kxDQ9Q%3D%3D",
    "description": "DevOps Administrator\n\nSalary: $135,000 ‚Äì 170,000\n\nContract Length: 12-month SOW\n\nLocation: Dallas, TX - in-office presence requirement 3 days weekly or more as needed\n\n\nThis represents the potential salary range for this position depending on education level, years of experience and/or certifications in addition to other position specific requirements which may impact salary\n\n\nWe‚Äôre seeking an experienced Administrator to join our Code Management team. The right candidate for this role will lead and execute strategic migrations, optimize CI/CD workflows, and drive infrastructure modernization. They will be critical in moving our automation ecosystem from legacy tools (Jenkins, Bitbucket, Automic) to GitLab, Ansible Automation Platform and Terraform, ensuring robust, scalable, and secure pipelines.\n\nRequired Skills And Experience\n\n\n8+ years of experience in Administering different & complex applications and tools used in the Enterprise\nExperience administering GitLab, Artifactory, Xray, & SonarQube\nExperience with infrastructure-as-code tools (Terraform, Ansible, etc.)\nSolid understanding of containerization (Docker) and orchestration (Kubernetes)\nFamiliarity with cloud platforms (AWS, Azure, IBM Cloud) and cloud-native tooling\nStrong communication skills and a track record of cross-team collaboration\nKnowledge of JFrog Artifactory, BitBucket / GIT, SVN and other SCM tools\nWorking knowledge of different Software Development Lifecycle Methodologies\nKnowledge of desired state configuration, automated deployment, continuous integration, and release engineering tools like Puppet, Chef, Jenkins, Bamboo, Maven, Ant etc\nConfigure and manage GitLab Runners, Groups, Projects, and Permissions at scale\nHarden GitLab for enterprise usage (SAML/SSO, LDAP, RBAC, backup/restore)\nDesign, implement, and optimize complex GitLab CI/CD pipelines using YAML best practices\nLeverage Terraform, Ansible, or similar to provision and manage self-hosted GitLab and runners\nImplement GitOps practices to manage infrastructure and environment configurations\nAutomate operational tasks and incident remediation via pipelines and scripts\nPartner with application teams to onboard them onto GitLab workflows and best practices\nDevelop and maintain clear runbooks, wiki pages, and pipeline templates\nIntegrate monitoring (Prometheus/Grafana, ELK) for GitLab health and pipeline performance\nImplement policies and guardrails to ensure code quality, compliance, and security posture\nTroubleshoot and resolve CI/CD or migration-related incidents in a timely manner\nAvailable for 24/7 On-call support\n\n\nPreferred\n\n\nA BS in Computer Science or equivalent work experience with good scripting/programming skills\nGitLab Certified Administrator\nPrior software experience with build management, configuration management and/or quality testing\nExperience with SCM practices including Agile, continuous integration (CI) and continuous deployment (CD)\n\n\nTeam Culture\n\nOur team is fast paced, fun, highly energetic, motivated and hardworking. We expect our candidates to be integrated into our results-driven and solution-oriented culture from the get-go. Our team attains high-quality results on challenging projects; the belief that outcomes are linked to one's effort rather than chance and the tendency to personally set challenging yet realistic goals."
  },
  {
    "title": "DevOps Engineer",
    "company": "Arize AI",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-arize-ai-4332964631?position=23&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=8vVampvuUEzsnC7p2JJFsQ%3D%3D",
    "description": "About Arize\n\nAI is rapidly transforming the world. As generative AI reshapes industries, teams need powerful ways to monitor, troubleshoot, and optimize their AI systems. That‚Äôs where we come in. Arize AI is the leading AI & Agent Engineering observability and evaluation platform, empowering AI engineers to ship high-performing, reliable agents and applications. From first prototype to production scale, Arize AX unifies build, test, and run in a single workspace‚Äîso teams can ship faster with confidence.\n\nWe‚Äôre a Series C company backed by top-tier investors, with over $135M in funding and a rapidly growing customer base of 150+ leading enterprises and Fortune 500 companies. Customers like Booking.com, Uber, Siemens, and PepsiCo leverage Arize to deliver AI that works.\n\nThe Team\n\nOur On-Prem engineering team is responsible for the deployment of Arize in customer environments. In addition to working with customers in defining infrastructure requirements, the team designs and develops software and tooling that enables the management of these systems at large scale. The On-Prem team has grown to be expert in Kubernetes and cloud deployment on GCP, Azure, and AWS as well as dealing with networking and security aspects of on-premise deployments. The team is dynamic and relies on few talented individuals with a high degree of autonomy and initiative.\n\nWhat You‚Äôll Do\n\n\nWork hands-on with the infrastructure that supports our distributed & highly scalable services in both SaaS and on-prem offerings\nGather requirements from customers and adapt manifests and software to support new environments\nUse and augment monitoring tools to observe platform health, ensure performance and reliability\nInteract with the product team to test new features and package new on-prem releases\nAutomate and optimize the release pipeline to make it as frictionless as possible\nExhibit continuous curiosity for emerging technology that could solve our challenges\n\n\nWhat will set you apart:\n\n\n3+ years of experience as a DevOps Engineer, Cloud Engineer, Infrastructure Engineer or similar\nExcellent communication skills and ability to work directly with customers to understand and address their infrastructure needs\nExperience and fluency in Kubernetes\nA self starter with an ability to thrived in a fast paced environment\nExperience working with multiple cloud providers (AWS, GCP, Azure) and understanding how to adapt cloud-native architectures for on-premises environments\nStrong troubleshooting skills\n\n\nThe estimated annual salary for this role is between $100,000 - $185,000, plus a competitive equity package. Actual compensation is determined based upon a variety of job related factors that may include: transferable work experience, skill sets, and qualifications. Total compensation also includes a comprehensive benefit package, including: medical, dental, vision, 401(k) plan, unlimited paid time off, generous parental leave plan, and others for mental and wellness support.\n\nWhile we are a remote-first company, we have opened offices in New York City and the San Francisco Bay Area, as an option for those in those cities who wish to work in-person. For all other employees, there is a WFH monthly stipend to pay for co-working spaces.\n\nMore About Arize\n\nArize‚Äôs mission is to make the world‚Äôs AI work‚Äîand work for people.\n\nOur founders came together through a shared frustration: while investments in AI are growing rapidly across every industry, organizations face a critical challenge‚Äîunderstanding whether AI is performing and how to improve it at scale.\n\nLearn more about what we're doing here:\n\nhttps://techcrunch.com/2025/02/20/arize-ai-hopes-it-has-first-mover-advantage-in-ai-observability/\n\nhttps://arize.com/blog/arize-ai-raises-70m-series-c-to-build-the-gold-standard-for-ai-evaluation-observability/\n\nDiversity & Inclusion @ Arize\n\nOur company's mission is to make AI work and make AI work for the people, we hope to make an impact in bias industry-wide and that's a big motivator for people who work here. We actively hope that individuals contribute to a good culture\n\n\nRegularly have chats with industry experts, researchers, and ethicists across the ecosystem to advance the use of responsible AI\nCulturally conscious events such as LGBTQ trivia during pride month\nWe have an active Lady Arizers subgroup"
  },
  {
    "title": "DevOps Engineer",
    "company": "Sustainment",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-sustainment-4335637240?position=24&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=w59fhbeTgmPDAivGVhenfQ%3D%3D",
    "description": "Company Overview: Sustainment is an AI-native software platform that helps US-based manufacturers easily find and work with the critical suppliers they need to build and manage their supply chains. Our vision is to reimagine American manufacturing as a hyperconnected, secure, and resilient ecosystem of local and regional suppliers who can more easily connect, interact, and do business with the industry and government customers that rely on them. We are a dual-use technology platform that supports both DoD and commercial customers in pursuit of our vision.\n\nJob Overview: We are looking for a DevOps/MLOps Engineer to drive the reliability, scalability, and performance of our AI-native procurement platform. The primary focus of the role is to build and maintain robust infrastructure, automate ML model deployment pipelines, and ensure database performance and reliability. You will be responsible for high-quality, secure deliverables that meet stringent compliance requirements (SOC 2, FedRAMP, CMMC Level 2) and for helping to create, evangelize, and enforce the standards necessary to meet team and company goals for operational excellence and mission-critical uptime.\n\nResponsibilities:\n\n\nBuild partnerships and work collaboratively with engineering, AI, and product teams to meet shared objectives\nOperate effectively in ambiguous situations, especially when scaling AI workloads and managing complex infrastructure transitions\nBuild and optimize DevOps pipelines including ML model training, versioning, deployment, monitoring, and retraining workflows\nAdminister and optimize PostgreSQL databases including performance tuning, query optimization, backup/recovery, and high availability configurations\nTroubleshoot and resolve infrastructure, database, and pipeline issues in a resilient, performant manner\nImplement and maintain infrastructure as code using tools like Terraform or Cloudformation\nMonitor system health, performance, and database metrics using observability tools and respond to alerts proactively\nEnsure security best practices and compliance requirements are met across all infrastructure and database layers\nParticipate in multi-resource projects in an agile environment\nEvaluate and recommend industry standards, tools, and methods for DevOps, MLOps, and database management\nDocument infrastructure architecture, runbooks, and contribute to architecture reviews\n\n\nQualifications:\n\n\nBachelor's degree (computer science, engineering, or related) or equivalent work experience\n2+ years of experience with cloud infrastructure (AWS preferred), container orchestration (Kubernetes), and CI/CD tools\n2+ years of database administration experience with PostgreSQL or similar relational databases\nExperience with ML model deployment, monitoring, and lifecycle management (MLOps)\nStrong understanding of infrastructure as code (Terraform), GitOps practices, and declarative configuration management\nExperience with security compliance frameworks (SOC 2, FedRAMP, or CMMC is a plus)\nProduct-driven mindset with deep empathy for internal developer experience and system reliability\nStrong desire to work in a startup with interest to take on projects from zero to one with collaboration with the rest of the team\nLove working hard and enjoy a fast-paced, ambiguous environment\nExperience with distributed systems, microservices architecture, and reactive systems\nOpen mindset to exploring new tools and frameworks in the rapidly evolving DevOps/MLOps landscape\nPassion for operational excellence and automation\nExperience supporting cross-team efforts to roll out new infrastructure capabilities or ML features\nPassion for learning and continuous improvement\nStrong written and verbal communication skills, and ability to explain complex technical concepts\nExperience working in a Scrum/agile environment\nExperience with AWS GovCloud, defense/government sector compliance, or working in an early startup environment on SaaS products is a plus\n\n\nCore Technologies:\n\n\nAWS (including GovCloud), Kubernetes, Docker, Terraform\nPostgreSQL\nGitLab CI/CD, ArgoCD, Tilt\nModel versioning, experiment tracking, ML pipeline orchestration\nDatadog, CloudWatch\nPython, Bash, experience with .NET ecosystem a plus\nIAM, secrets management, encryption, audit logging, compliance automation\n\n\nSustainment offers a competitive benefits package for full time employees including medical, dental, vision, paid time off, company holidays, and 401K matching.\n\nSustainment is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.\n\nApplicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.\n\nSustainment participates in E-Verify."
  },
  {
    "title": "Devops",
    "company": "The Dignify Solutions, LLC",
    "location": "Phoenix, AZ",
    "link": "https://www.linkedin.com/jobs/view/devops-at-the-dignify-solutions-llc-4347025595?position=25&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=q4Rw4c9l8LicRPyDg2ohgg%3D%3D",
    "description": "Must have is\n\n\nAssociate should be in Phoenix from day 1 of the project\nAt least 5 years of experience in Devops area.\nStrong skill in CI/CD pipeline, Jenkins, Github\nAdditional knowledge on any build related tools is an added advantage.\n\n\nJava 8 knowledge\n\nDocker\n\nKaffka\n\nKibana"
  },
  {
    "title": "DevOps Engineer I",
    "company": "Trustwell",
    "location": "Portland, OR",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-i-at-trustwell-4321600458?position=26&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=%2F3o7FP0nMNdz8JS9k5SbgQ%3D%3D",
    "description": "Role{{:}} DevOps Engineer I\n\nFLSA{{:}} Full Time | Exempt | Salaried | Remote\n\nReports to{{:}} Director of DevOps\n\nNote{{:}} Candidate preferred to reside in PST. If not, candidate will be required to support PST working hours.\n\nTrustwell is looking for ambitious, energetic problem-solvers who enjoy a fast-paced team environment filled with challenges and career growth opportunities in a rapidly growing tech firm. Trustwell is on a mission to change the food industry. Combining FoodLogiQ's supply chain management software with Genesis' nutritional analysis and label development solution, the Trustwell Connect platform creates the food industry's only full-scale solution connecting product development and regulatory-compliant labeling with supplier compliance, enhanced traceability, and automated recall management. From food and supplement manufacturers to retail grocers and restaurant chains, more than 2,500 food companies around the world use Trustwell software as their trusted source for compliance and quality solutions in the food industry. For more information, visit www.trustwell.com.\n\nScope of Position{{:}} The DevOps Engineer I will be part of a dynamic and agile team responsible for building and maintaining the platforms, systems, and services that power our customer-facing products. Working closely with Software Engineers and Engineering Leadership, you'll help shape architecture, implement best practices, and stay ahead of the curve in DevOps principles. You'll champion operational excellence and continuous improvement across the team.\n\nEssential Duties & Responsibilities include but not limited to{{:}}\n\n\nContribute actively to an Agile delivery team, ensuring consistent, high-quality, and reliable software releases.\nCollaborate closely with developers and cross-functional partners to design, build, and deploy best-in-class, scalable software solutions.\nChampion an automation-first, code-centric mindset, driving efficiency and consistency across deployment, monitoring, and maintenance processes.\nSupport production operations through participation in incident response, troubleshooting, and on-call rotations to maintain system reliability and uptime.\nDevelop, implement, and maintain monitoring and alerting tools to ensure optimal application performance, health, and availability.\nDesign infrastructure and deployment solutions with scalability, resilience, and long-term maintainability as core principles‚Äîavoiding short-term workarounds.\nProactively identify and eliminate operational bottlenecks and unnecessary complexity, contributing to continuous improvement initiatives.\nEngage in architectural reviews and solution design discussions, providing input that enhances performance, reliability, and security.\nPerform other related duties as assigned, contributing to the overall success of the DevOps function and technology organization.\n\n\nEducation/Experience{{:}}\n\n\nBachelor's degree in Computer Science, Engineering, or a related field required. Will consider relevant experience/certifications in lieu of degree.\n2+ years of experience in an SRE (Site Reliability Engineer) or equivalent engineering role\n2+ years of experience as a DevOps Engineer or in a similar capacity\n3+ years of hands-on experience managing and supporting production cloud environments (AWS, Azure, or GCP)\nExtensive experience with DataDog, including APM, RUM, Synthetic Monitoring, Infrastructure Monitoring, and Dashboard development\n\n\nRequired Skills/Abilities{{:}}\n\n\nStrong, hands-on experience with Infrastructure-as-Code (IaC) and configuration management tools such as Terraform, CloudFormation, and/or Ansible\nProven experience designing and managing cloud architectures in AWS and Microsoft Azure, with expertise in containerization and orchestration (Docker, Kubernetes, etc.)\nDemonstrated experience building, maintaining, and optimizing CI/CD pipelines using tools such as CircleCI, TeamCity, GitHub Actions, or Jenkins\nBackground in delivering infrastructure initiatives within an Agile development environment\nCollaborative mindset with the ability to partner effectively across cross-functional teams to achieve shared goals\nStrong \"automation-first\" mindset with a focus on scalability, reliability, and efficiency\n\n\nTotal Rewards Package{{:}}\n\n\nFull healthcare benefits, including medical, dental, and vision.\nSupplemental benefits, including STD, LTD, HSA, 401k, etc.\nResponsible Time Off (PTO) + Holiday Pay\nExcellent culture, growth opportunities, plus much more...\n\n\nWhat to expect - the Hiring Process!\n\n\nInterview with Human Resources\nInterview with Hiring Manager\nPeer Panel Interview(s)\nOffer of Employment (Background Screening/References)\n\n\nHiring Eligibility{{:}} This is a fully remote position open to candidates located anywhere within the United States. Eligibility to work remotely is subject to company policy and applicable state laws. Candidates must have work authorization to work for any U.S. based employer. Please note that certain benefits, taxes, or employment terms may vary by state.\n\nCompensation{{:}} The compensation for this position starts at $80,000 per annum, with the potential for higher placement based on a candidate's experience, education, and overall qualifications. In addition to base salary, this role is bonus eligible‚Äîup to 10% annually, contingent on company performance and achievement of organizational objectives.\n\nTrustwell is an equal employment opportunity employer committed to hiring and retaining a diverse workforce. Applicants receive fair and impartial consideration without regard to race, sex, sexual orientation, gender identity, color, religion, national origin, age, disability, veteran status, religion, or other legally protected class. If you need accommodation for any part of the employment process due to a medical condition, or any disability, please contact a member of our human resources team.\n\nAcceptable Background and References Required; Upon any conditional offers made by Trustwell.\n\nEqual Opportunity Employer/ DFWP/ Affirmative Action"
  },
  {
    "title": "DevOps Engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4338475165?position=27&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=fKh6rwkEgdcRHSxUWRR1bw%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Remote\n\nEmployment Type: Full-Time\n\nJob Summary\n\nWe are looking for a skilled DevOps Engineer to join our technology team. The ideal candidate will design, implement, and manage CI/CD pipelines, automate infrastructure, and ensure smooth deployment processes across development and production environments. This role requires strong knowledge of cloud platforms, containerization, and scripting.\n\nKey Responsibilities\n\n\nDesign, build, and maintain CI/CD pipelines for application deployment.\nAutomate infrastructure provisioning using tools like Terraform or Ansible.\nManage containerized environments using Docker and Kubernetes.\nMonitor system performance and implement proactive solutions for scalability and reliability.\nCollaborate with development and operations teams to streamline workflows.\nEnsure security and compliance in cloud and on-prem environments.\nTroubleshoot and resolve issues in production and staging environments.\n\n\nQualifications\n\n\nBachelor's degree in Computer Science, Engineering, or related field.\n25 years of experience in DevOps or related roles.\nProficiency in cloud platforms (AWS, Azure, GCP).\nHands-on experience with CI/CD tools (Jenkins, GitLab CI, GitHub Actions).\nStrong knowledge of containerization (Docker, Kubernetes).\nFamiliarity with Infrastructure as Code (Terraform, Ansible).\nScripting skills in Python, Bash, or similar languages.\n\n\nPreferred Skills\n\n\nExperience with monitoring tools (Prometheus, Grafana).\nKnowledge of security best practices in DevOps.\nFamiliarity with microservices architecture.\n\n\nBenefits\n\n\nCompetitive salary and performance bonuses.\nHealth insurance and retirement plans.\nFlexible work options and professional development opportunities."
  },
  {
    "title": "DevOps Engineer (JIRA)",
    "company": "Rubix Solutions",
    "location": "Washington, DC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-jira-at-rubix-solutions-4335995983?position=28&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=D7zCwi59KqXK1oLdGUld9w%3D%3D",
    "description": "The DevOps Engineer (specifically Jira Platform Engineer) will serve as the technical owner of our Jira SaaS in government cloud, and is responsible for administering, configuring, and integrating the application within our enterprise technology ecosystem. This position plays a pivotal role in consolidating our collaborative planning tools‚Äîtransitioning from Azure DevOps (on-prem) and GitLab (on-prem) to Jira‚Äîwhile enabling and scaling Agile practices across the organization.\n\nThe ideal candidate is both technically proficient and strategically minded, with a strong understanding of Agile methodologies, DevSecOps workflows, and enterprise system\n\nintegration.\n\nResponsibilities\n\n\nAdminister, configure, and optimize Jira SaaS to meet enterprise project management and Agile delivery needs.\nDesign and maintain custom workflows, issue types, screens, fields, and automation rules aligned with organizational Agile frameworks and guidelines.\nManage user permissions, group roles, and security schemes to ensure governance and compliance.\nMonitor Jira license utilization, user growth, and application usage to ensure efficient use of subscriptions.\nCollaborate with procurement teams to support renewal, optimization, and budget decisions.\nDesign, implement, and maintain seamless integrations between Jira with other enterprise systems, such as GitLab (source control & CI/CD), and ServiceNow (ITSM), using Okta,REST APIs, webhooks, middleware, and scripting.\nAutomate data synchronization across platforms to support traceability from planning to release.\nTroubleshoot and optimize integration pipelines to ensure performance, security, and Scalability.\nPartner with infrastructure and cybersecurity teams to align integrations with enterprise security and compliance standards.\nDevelop and maintain technical documentation, standards, and best practices.\nPartner with the Agile Transformation Office to translate Agile practices into effective Jira configurations and usage patterns.\nProvide technical guidance and mentoring to Scrum Masters, Product Owners, and teams on best-practice tool utilization.\nSupport reporting and analytics initiatives, ensuring reliable Agile metrics (velocity, burndown, cycle time, etc.).\n\n\nRequirements\n\n\nMust be able to obtain and maintain Moderate Risk Public Trust (MRPT) facility credentials/authorization. Note: US Citizenship is required for MRPT facility credentials/authorization at this work location.\nBachelor‚Äôs degree in Computer Science, Information Systems, or a related field (or equivalent experience).\n3+ years of hands-on experience administering and/or engineering Jira (Self-hosting or SaaS).\nProven, hands-on experience developing custom integrations with enterprise platforms, especially GitLab and ServiceNow.\nProficiency in scripting (Python, PowerShell, or JavaScript) and REST API integration.\nStrong understanding of Agile methodologies (Scrum, Kanban, SAFe) and DevSecOps principles.\nExperience in enterprise migrations from Azure DevOps or similar tools to Jira.\nFamiliarity with Atlassian ecosystem (Confluence, Bitbucket) and marketplace apps.\nExperience working in a DevSecOps or Platform Engineering environment.\nExperience with information security, privacy, and risk assessment standards including FISMA, SOX, FedRAMP, etc. is preferred.\nFederal government experience is preferred.\nAtlassian Certified Professional (ACP-620, ACP-120, or equivalent) preferred."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4291046434?position=29&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Vh%2FkXp%2BkyoTdqG9ayhvJbw%3D%3D",
    "description": "About Chartmetric\n\nChartmetric, Inc. is a 10-year-old startup specializing in music data analytics. We are trusted by Universal MusicGroup, Sony, Warner, and Apple Music, as well as hundreds of other music companies and industry professionals. Our team has created a self-service data dashboard for the music industry to better understand the activity happening around artists. Together, we combine hundreds of thousands of real-time data points across iTunes, Spotify, YouTube, Google, Amazon, X, and others through our beautifully designed tool in order to make sense of the increasingly complex landscape of the music industry.\n\nAbout The Role\n\n\nWe are seeking a talented DevOps / Developer Experience Engineer to join our team and play a pivotal role in enhancing our development infrastructure and streamlining the developer workflow. This position combines traditional DevOps responsibilities with a focus on creating exceptional developer experiences through tooling, automation, and process optimization.\n\n\nWhat You'll Do\n\n\nInfrastructure & Operations\nDesign, implement, and maintain scalable cloud infrastructure using Infrastructure as Code (IaC) principles\nManage CI/CD pipelines and deployment processes across multiple environments\nMonitor system performance, reliability, and security, implementing proactive solutions\nAutomate operational tasks and eliminate manual toil through scripting and tooling\nEnsure high availability and disaster recovery capabilities\n\n\nDeveloper Experience\nBuild and maintain internal developer tools and platforms that improve productivity\nStreamline onboarding processes for new developers and reduce time-to-first-commit\nDesign and implement developer-friendly APIs, SDKs, and documentation\nCreate self-service capabilities that reduce dependencies and waiting times\nGather feedback from development teams and iterate on tooling based on pain points\n\n\nCollaboration & Process Improvement\nWork closely with engineering teams to understand workflow challenges and requirements\nChampion best practices for code deployment, testing, and monitoring\nLead initiatives to improve development velocity and reduce friction\nParticipate in incident response and post-mortem analysis\nMentor team members on DevOps practices and tooling\n\nWhat We're Looking For\n\n\nTechnical Skills\n3+ years of experience in DevOps, SRE, or Platform Engineering roles\nStrong proficiency with cloud platforms (AWS, GCP, or Azure)\nExperience with Infrastructure as Code tools (Terraform, CloudFormation, or Pulumi)\nHands-on experience with containerization (Docker) and orchestration (Kubernetes)\nProficiency in CI/CD tools (Jenkins, GitLab CI, GitHub Actions, or similar)\nStrong scripting skills in Python, Bash, or Go\nExperience with monitoring and observability tools (Prometheus, Grafana, ELK stack, or similar)\n\n\nDeveloper Experience Focus\nExperience building internal tools and platforms for development teams\nUnderstanding of software development lifecycle and common developer pain points\nFamiliarity with API design and developer-facing documentation\nExperience with version control systems and Git workflows\nKnowledge of testing frameworks and quality assurance processes\n\n\nSoft Skills\nStrong problem-solving abilities and analytical thinking\nExcellent communication skills and ability to work with cross-functional teams\nCustomer-focused mindset with emphasis on developer productivity\nProactive approach to identifying and resolving issues\nAbility to balance technical debt with feature delivery\n\n\nPreferred Qualifications\nKnowledge of security best practices and compliance frameworks\nBackground in software development or engineering\nFamiliarity with cost optimization strategies for cloud infrastructure\nPrevious experience in a high-growth or scaling environment\n\nWhat We Offer\n\n\nCompetitive salary and equity package\nComprehensive health, dental, and vision insurance\nOpportunity to shape developer experience across the organization\nAccess to cutting-edge tools and technologies\n\n\nTeam Culture\n\n\nWe believe that great developer experiences lead to better products and happier teams. Our DevOps/DX team operates as enablers and force multipliers, working collaboratively to remove friction from the development process. We value automation, measurement, and continuous improvement, always asking \"how can we make this better for our developers?\"\n\n\nThe Pay Range For This Role Is\n\n135,000 - 165,000 USD per year(San Mateo)\n\n120,000 - 150,000 USD per year(New York)"
  },
  {
    "title": "DevOps Engineer",
    "company": "Broad Reach Partners",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-broad-reach-partners-4303987210?position=30&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=UGEmD%2Fmb2TkDdKzHYhrqLg%3D%3D",
    "description": "We are seeking a Senior DevOps Engineer to join our team and play a critical role in designing, building, and optimizing the CI/CD pipelines that power our software delivery across on-prem and cloud environments.\n\nIn this role, you will work hand-in-hand with our development, operations, and security teams worldwide to implement best practices, automate deployments, and ensure our platforms are reliable, secure, and scalable. If you thrive on solving complex technical challenges, have a passion for automation, and want to influence how enterprise platforms evolve and modernize, this is an ideal opportunity for you.\n\nAs a Senior DevOps Engineer, your expertise will drive the continuous integration, delivery, and deployment (CI/CD) pipelines delivering software to both on-prem and cloud (AWS primarily) environments. You will work closely with our development, operations, and security teams distributed across the globe. This role requires a deep understanding of DevSecOps best practices and a strong ability to troubleshoot complex issues.\n\nYour Responsibilities In This Role Will Include\n\n\nDesign, Develop and Maintain automated build and deployment pipelines using GitLab/GitHub/Jenkins to enhance software delivery.\nIdentify opportunities for automation and ensure continuous security, quality in application development by automating security checks, test executions in build and deployment pipelines.\nDeploy and manage Kubernetes workloads to AWS EKS(A) using Helm, ArgoCD\nCollaborate with development, operations and security team to build secure, optimized and efficient pipelines.\nCreate comprehensive documentation on pipeline functionality and provide training to required members.\nProactively monitor system performance and identify potential issues before they become critical.\nParticipate in on-call rotation.\nEngage in continuous learning and actively advocate for Dev(Sec)Ops, GitOps best practices and standards across the team.\n\n\nWe are looking for you to have the following skills and experience:\n\n\n8+ years of experience as a DevOps Engineer, Site Reliability Engineer, or equivalent\nStrong knowledge of DevOps practices, continuous integration, continuous delivery, and related tools.\n3+ years of experience with Amazon Web Services (AWS) or Microsoft Azure\n3+ years of experience with Kubernetes clusters\nProficiency with public cloud environments (AWS preferred)\nExperience with tools like New Relic and Graylog\nAdvanced proficiency working with CI/CD pipelines such as GitHub Actions/GitLab/Jenkins\nExpert in containerization technologies such as Docker and orchestration tools like Kubernetes.\nProficiency in scripting language, like Bash, Groovy, Python\nExcellent debugging and troubleshooting skills.\nAbility to prioritize tasks efficiently and independently under minimal supervision.\n\n\nNice to Have\n\n\nAWS Cloud certification\nFamiliar with .NET applications.\nKnowledge in Terraform, Ansible, monitoring tools\n\n\nWe are located in the Alpharetta/Cumming area of Atlanta and are working in the office several days each week so YOU MUST LIVE WITHIN COMMUTING DISTANCE OF ALPHARETTA, GA to be considered for this role. We cannot sponsor at this time.\n\nIf this opportunity is a good match for your skills, experience and interest, please apply now so we can follow up with you with more details."
  },
  {
    "title": "AWS DevOps Specialist",
    "company": "Focus School Software",
    "location": "St. Petersburg, FL",
    "link": "https://www.linkedin.com/jobs/view/aws-devops-specialist-at-focus-school-software-4333597594?position=31&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=2Ep3XIthQ5ZQ3Iq%2BRnL5cw%3D%3D",
    "description": "Focus School Software is a fast-growing school management software company. We thrive on creating some of the most innovative features on the market today, helping educators to meet their evolving needs in classrooms, district management, state reporting compliance, and other facets of student-centered education and technology.\n\nWe are seeking an experienced and proactive AWS DevOps Specialist to join our growing infrastructure team. This role is ideal for someone passionate about automation, cloud infrastructure, and scalable, secure systems. The ideal candidate brings expertise in AWS services, infrastructure as code, cost reduction strategies and DevOps best practices. You will play a key role in improving system performance, reliability, and security, while contributing to CI/CD pipelines and participating in an on-call rotation. This is a great opportunity for anyone who has a multitude of skills in DevOps and System Administration and can wear many hats and loves problem solving.\n\nKey Responsibilities\n\n\nAutomation & Configuration Management\nDesign, develop, and maintain automation using Ansible and Ansible Tower.\nDeploy and configure RHEL based systems.\nDatabase maintenance and performance, routing, pooling and role management.\n\n\nCloud Infrastructure (AWS)\n\n\nArchitect and manage services including RDS (PostgreSQL), EC2, EKS, CloudFormation, CloudFront, GuardDuty, AWS VPN, AWS AD, and more.\nImplement Autoscaling strategies and container orchestration with EC2 Autoscaling or EKS. Continuously monitor performance and feedback on systems.\nMonitor and improve database performance, sharding strategies, and health metrics.\nMonitor backups and maintain recovery point objectives for disaster recovery.\n\n\nSecurity & Compliance\n\n\nSupport SOC II compliance initiatives through infrastructure hardening, monitoring, and alerting.\nLeverage AWS security tools and best practices to ensure compliance and threat mitigation.\n\n\nNetworking & Connectivity\n\n\nManage VPCs, subnets, security groups, VPNs, and endpoint connectivity for both internal and external integrations.\nManaging routes, DNS and VPN connectivity. Help internal users maintain their VPN connections.\nCost Optimization\nAnalyze AWS billing, usage reports, and recommend cost-saving strategies.\n\n\nDevOps & CI/CD\n\n\nBuild and maintain CI/CD pipelines, enabling delivery of automation code from development to production.\nEnsure high availability and zero-downtime deployments through automation and best practices.\nDevelop and maintain local dev environments for developers.\n\n\nCollaboration & Culture\n\n\nWork cooperatively in cross-functional teams, embracing a culture where the best ideas win.\nProactively identify infrastructure problems and lead with creative, scalable solutions.\n\n\nEndpoint & Systems Management\n\n\nOversee and manage end-user systems and infrastructure endpoints to maintain security and stability.\nPatch management and remediation, ensure established timelines and policies are followed.\n\n\nOn-Call Participation\n\n\nParticipate in an on-call rotation to respond to production incidents and infrastructure issues.\n\n\nRequirements\n\n\nAnsible, Ansible Tower, working in RHEL based environments.\nLinux/RHEL expert, be able to design, deploy and fix everything from a systemd service to managing SFTP.\nAbility to manage a Git repository, and perform peer review on automation code.\nMonitoring tools such as Splunk, Grafana or similar.\nWorking knowledge of NGINX, basic webserver stacks.\nUnderstanding of AWS, particularly RDS (PostgreSQL), CloudFormation, EC2, EKS\nKubernetes and containerization\nExperience with database sharding and performance tuning\nCI/CD pipeline design and implementation\nFamiliarity with SOC II compliance frameworks\nExperience managing AWS networking, VPNs, and AWS AD\nSecurity-first mindset with experience using AWS Org, AWS Tower, CloudFront, and related tools to ensure compliance.\nStrong interpersonal skills with a collaborative mindset.\n\n\nNice-to-Have\n\n\nLAMP/LNPP Stack experience\nSVN Familiarity\nActive Directory experience, basic Windows management\nExperience with endpoint management platforms\nBackground in proactive monitoring/observability tooling\nPrior involvement in security audits or compliance initiatives\n\n\nFocus School Software‚Äôs compensation package offers the following benefits:\n\n\nMedical Insurance\nDental/Vision Insurance\nLife Insurance\nShort and Long Term Disability Insurance\n401(k) after 6 months\nPaid Holidays\nPaid Vacation and Sick Time\nRemote Position"
  },
  {
    "title": "DevOps Engineer",
    "company": "Rain",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rain-4318510257?position=32&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=NSVwZMZsEqJm8zoQYJWUbw%3D%3D",
    "description": "Rain is empowering the next generation of money and financial products globally. We‚Äôre a lean and mighty team of passionate builders and veteran founders. We are looking for a DevOps engineer to join us in building a cutting edge platform at the intersection of real-world payments and digital money. You will have the opportunity to deliver massive impact at a small and quickly growing company that is funded by some of the top investors in fintech and crypto. Rain is backed by great investors including Lightspeed, Norwest, Khosla, along with great companies like Coinbase, Circle, and Uniswap.\n\nMany of our engineers are based in NYC but we are open to fully remote candidates.\n\nOur Ethos\n\nWe believe in an open and flat structure. You will be able to grow into the role that most aligns with your goals. Our team members at all levels have the freedom to explore ideas and impact the roadmap and vision of our company.\n\nWhat You'll Do\n\n\nBe a critical part of the technical infrastructure roadmap\nManage our cloud environments across GCP and AWS\nScale our infrastructure to millions of end users globally\nHelp drive the architectural decisions of a rapidly evolving product\nLead the creation and maintenance of our CI/CD pipelines to enable rapid, reliable deployments\nCollaborate with the engineering team to improve infrastructure performance\nBuild infrastructure to interact with millions of smart contracts across dozens of blockchains\nAutomate security controls and compliance processes to protect sensitive financial data\n\n\nWhat We're Looking For\n\n\nStrong experience with Infrastructure as Code, particularly Terraform, for managing cloud resources at scale\nProven track record designing and implementing CI/CD pipelines and automation workflows\nExperience managing production environments in cloud providers\nExperience with monitoring, logging, and observability tools\n\n\nNice to haves, but not mandatory\n\n\nExperience in fintech (neobank or card issuing experience gets extra brownie points)\nExperience with blockchain infrastructure\n\n\nOur perks enable working at Rain to be a fulfilling, healthy and happy experience.\n\nUnlimited time off üõº Unlimited vacation can be daunting, so at Rain we require our teammates to take 10 days minimum for themselves.\n\nFlexible working ‚òï We support a flexible workplace, if you feel comfortable at home please work from home. If you‚Äôd like to work with others in an office feel free to come in. We want everyone to be able to work in the environment in which they are their most confident and productive selves.\n\nFlexible Benefits üß† Easy-to-access benefits, for all employees based in the US, Rain pays a percentage of your benefits for the employee and for your dependents. We offer comprehensive health, dental and vision plans as well as a 100% company-subsidized life insurance plan.\n\nEquity plan üì¶ On top of a competitive salary, we offer every Rain employee an equity option plan so we can all can benefit from our success.\n\nRain Cards üåßÔ∏è We want our teammates to be knowledgeable about our core products and services and to support this mission we issue a card for our team to utilize the card for testing.\n\nHealth and Wellness üìö High performance begins from within. Our members are welcome to use their company card for eligible health and wellness spending like gym memberships, fitness classes and other wellness items.\n\nTeam summits ‚ú® Summits play an important role at Rain! Time spent together helps us get to know each other, strengthen our relationships, and build a common destiny. Stay tuned for upcoming destinations!"
  },
  {
    "title": "DevOps Engineer",
    "company": "Jasper",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-jasper-4318500931?position=33&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=sQ4ga8%2FZC2D2RhG5qvGnmQ%3D%3D",
    "description": "Jasper is the leading AI marketing platform, enabling the world's most innovative companies to reimagine their end-to-end marketing workflows and drive higher ROI through increased brand consistency, efficiency, and personalization at scale.\n\nJasper has been recognized as \"one of the Top 15 Most Innovative AI Companies of 2024\" by Fast Company and is trusted by nearly 20% of the Fortune 500 ‚Äì including Prudential, Ulta Beauty, and Wayfair. Founded in 2021, Jasper is a remote-first organization with team members across the US, France, and Australia.\n\nAbout The Role\n\nWe're looking for an experienced DevOps Engineer to join our Platform team. This is a highly autonomous, high-impact role that blends Ops practices, infrastructure engineering, and delivery pipeline optimization. You'll work with a focused, collaborative, and fast-moving team where your contributions will directly impact system reliability, developer velocity, and our ability to safely deliver AI-powered products at scale. Candidates should also have a solid background in Cloud, IaC, and Kubernetes, and a drive to produce excellent solutions for a variety of challenges.\n\nThis fully remote role reports to the Staff Dev Ops Engineer and is open to candidates located anywhere in the continental US.\n\nWhat You‚Äôll Do\n\n\nDesign, implement, and operate cloud-native infrastructure that scales efficiently, fails gracefully, and optimizes for performance and cost.\nBuild and refine software delivery pipelines to enable safe, fast, and frequent deployments with robust testing, rollback, and progressive release mechanisms.\nDevelop infrastructure-as-code solutions using Terraform and Helm to create self-healing, automated, and observable systems.\nCollaborate with ML and product teams to support AI model training and inference through scalable compute and storage infrastructure.\nIdentify and eliminate single points of failure, performance bottlenecks, and scalability limits through proactive monitoring and reliability engineering practices.\nImplement and enforce security best practices, including secrets management, access control, and compliance across all infrastructure layers.\n\n\nWhat You‚Äôll Bring\n\n\nDeep experience running Kubernetes in production (cluster management, networking, storage, security).\nExpertise with Terraform, Helm, and configuration management to build reproducible, version-controlled infrastructure.\nProven success designing and maintaining CI/CD pipelines (GitHub Actions, Argo CD, Jenkins, etc.) balancing speed and safety.\nStrong background in observability (especially Datadog) ‚Äî skilled at instrumentation, dashboard creation, and intelligent alerting.\nSolid scripting skills in Python, Go, or Bash, with a focus on automation and operational efficiency.\nPractical knowledge of Google Cloud Platform and cloud-native architectures.\nExperience supporting multi-language environments (TypeScript, Python, Go) and AI/ML workloads, including GPU-based compute.\nFamiliarity with container security, secrets management, and policy enforcement.\n(Bonus) History of open source contributions in infrastructure, CI/CD, or observability projects.\n\n\nCompensation Range\n\nAt Jasper, we believe in pay transparency and are committed to providing our employees and candidates with access to information about our compensation practices. The expected base salary range offered for this role is $170,000 - $200,000. Compensation may vary based on relevant experience, skills, competencies, and certifications.\n\nBenefits & Perks\n\n\nComprehensive Health, Dental, and Vision coverage beginning on the first day for employees and their families\n401(k) program with up to 2% company matching\nEquity grant participation\nFlexible PTO with a FlexExperience budget ($900 annually) to help you make the most of your time away from work\nFlexWellness program ($1,800 annually) to help support your personal health goals\nGenerous budget for home office set up\n$1,500 annual learning and development stipend\n16 weeks of paid parental leave\n\n\nOur goal is to be a diverse workforce that is representative at all job levels as we know the more inclusive we are, the better our product will be. We are committed to celebrating and supporting our differences and that diversity is essential to innovation and makes us better able to serve our customers. We hire people of all levels and backgrounds who are excited to learn and develop their skills.\n\nWe are an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state or federal laws.\n\nBy submitting this application, you acknowledge that you have reviewed and agree to Jasper's CCPA Notice to Candidates, available at legal.jasper.ai/#ccpa."
  },
  {
    "title": "DevOps / Systems Engineer",
    "company": "Collate",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-collate-4302854141?position=34&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=5tNGYWWDGn%2FPBd2%2Fgr1z2w%3D%3D",
    "description": "About Collate\n\nCollate is an AI document generation platform for life sciences. We automate paperwork with AI, helping our customers get life-saving innovations to patients years faster. Collate is an end-to-end solution, powering every step of drug, diagnostic, and medical device development‚Äîfrom concept to market.\n\nOur CEO Surbhi Sarna is a former General Partner at Y Combinator. Surbhi founded nVision Medical, which developed a new method to detect ovarian cancer and was acquired by Boston Scientific. Our CTO Nate Smith is a former Visiting Partner at Y Combinator and founder of Lever. Our AI researchers, engineers, and designers have worked at Google, Nvidia, Meta, Netflix, Amazon, AirBnB, Hippocratic AI, and Grail, and 40% of our team are former founders.\n\nWe‚Äôre a small, elite team, with over $30M in seed funding from top investors (Redpoint, First Round Capital, Conviction, and Y Combinator) and leaders in healthcare and AI. This is a rare chance to join at the ground floor of a company with world-changing potential, experienced founders, and resources to execute at scale.\n\nAbout The Role\n\nWe‚Äôre looking for a DevOps / Systems Engineer to own the infrastructure that powers Collate‚Äôs products. You‚Äôll build the systems and tooling that keep our platform reliable, secure, and fast as we scale.\n\nThis role is broad by design ‚Äî from managing CI/CD pipelines and cloud infrastructure to handling light security responsibilities like certificate management. You‚Äôll partner closely with backend, AI, and product engineers to ensure our systems are both easy to develop on and safe to deploy at scale.\n\nAt Collate, infrastructure isn‚Äôt just about uptime ‚Äî it‚Äôs about trust. The work you do will help ensure that the AI we build for healthcare runs with reliability and security in mind.\n\nWhat You‚Äôll Do\n\n\nDesign and maintain cloud infrastructure to support Collate‚Äôs products as we grow from prototypes to production scale\nDevelop CI/CD pipelines and automation that accelerate developer velocity and reduce operational friction\nManage core system reliability, including monitoring, logging, and incident response\nTake on light security responsibilities, such as handling certificates, secrets management, and supporting compliance needs\nCollaborate closely with engineering teams to design infrastructure that balances speed, safety, and scale\nContinuously improve internal tooling and workflows, helping the team move faster with confidence\nLeverage tooling including AWS, Terraform, Kubernetes, Helm, ArgoCD, Grafana, and Github Actions\n\n\n\nWhat We‚Äôre Looking For\n\n\nHands-on experience with cloud infrastructure (AWS, GCP, or similar) and modern DevOps practices\nProficiency with infrastructure-as-code and CI/CD tooling\nFamiliarity with monitoring, observability, and incident management\nInterest or experience in light security work, including certificates, secrets management, or compliance support\nA pragmatic approach: able to balance iteration speed with building for long-term reliability\nMotivation to work in an early-stage startup where your infrastructure decisions shape the foundation of the company\n\n\n\nWhy Join Collate?\n\nImpact: Build systems and experiences that touch real patients and providers, improving healthcare outcomes.\n\nOwnership: Shape both our product experience and our engineering culture from the start.\n\nLearning: Collaborate with a uniquely interdisciplinary team‚ÄîAI researchers, healthcare leaders, and experienced startup builders.\n\nUpside: Join a company early enough to have meaningful equity and career-defining impact.\n\nThe base salary range for this role is $150,000‚Äì$300,000 USD annually, depending on experience and level (Tier 1, San Francisco)\n\nWe may use artificial intelligence (AI) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. These tools assist our recruitment team but do not replace human judgment. Final hiring decisions are ultimately made by humans. If you would like more information about how your data is processed, please contact us."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "San Mateo, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4304688090?position=35&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=d1qAUHt%2FpgYsD%2FDXSFpJCA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps With Azure Experience -100%Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "New Jersey, United States",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-with-azure-experience-100%25remote-at-the-dignify-solutions-llc-4341845867?position=36&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=JiMtF1is0D3S3JSUH9%2BVpQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer - All Levels",
    "company": "CodeRabbit",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-all-levels-at-coderabbit-4318518267?position=37&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=aDVYWbGuwPRTkYCR6m15uA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Uffizio",
    "location": "Michigan, United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-uffizio-4324397378?position=38&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=h5GF7WpeSDM98EpMkI1V0g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Assistant (Entry-Level)",
    "company": "45PRESS",
    "location": "Canfield, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-assistant-entry-level-at-45press-4301017192?position=39&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=ibkJuX%2BlJomxXk%2FoYN0otA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Senior DevOps Engineer",
    "company": "CEIPAL",
    "location": "Charlotte, NC",
    "link": "https://www.linkedin.com/jobs/view/senior-devops-engineer-at-ceipal-4305453358?position=40&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=pNh4GDOlzR9Nf%2Bx97GkdxQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4310657957?position=41&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=UWtIJiwj4kEuJBQxRUWDXQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Hudu",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hudu-4323191230?position=42&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=G5DAdhE4T0ub1J2ceS5RBg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4335667666?position=43&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=YwJAEU0LZIQX5OAcB65ahg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Staff Engineer: DevOps",
    "company": "Dispel",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/staff-engineer-devops-at-dispel-4339045806?position=44&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=IQYx16VKEJ34dd0qmsP1Aw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "IT Automation LLC",
    "location": "Cary, NC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-it-automation-llc-4324192032?position=45&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=tr9QBOrtMWgmmH2g0ODI2A%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CHEQUESPREAD PLC",
    "location": "Valley Forge, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chequespread-plc-4288904252?position=46&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=9cKzcsNRAHFfA7VwsIujjg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "eSimplicity",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-esimplicity-4315888714?position=47&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=nkLaIDoXZRNA%2BhWW4RPjuA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud/DevOps Engineer",
    "company": "Tagup, Inc.",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-engineer-at-tagup-inc-4333051833?position=48&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=N%2BmXkuHZmLx5RW5dk8lpeg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mark43",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mark43-4309062970?position=49&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=FayPW506ncK%2BfiLQnXecaQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CMG (Capital Markets Gateway)",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cmg-capital-markets-gateway-4338419750?position=50&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=y2AsIcVknVztZIKalzz8Eg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mintlify",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mintlify-4318506680?position=51&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=2Y9X1gJn4l7tsgd73kAuAw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Support Engineer",
    "company": "Porter",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-support-engineer-at-porter-4295124575?position=52&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=R%2Fk%2FJ7fjJ8xRsUn0t55Ypw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Northstrat Incorporated",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-northstrat-incorporated-4304125676?position=53&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=jjRJVdeAcuc2ZN3kP3cDoQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Paramount",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-paramount-4335876548?position=54&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=2hAyBKZq34rXa%2Fl7X2Kb0Q%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "SmartVault",
    "location": "Houston, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-smartvault-4297941616?position=55&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=lrowBtEfvKtI8PYtLrDkNQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "RSC2, Inc.",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rsc2-inc-4311252822?position=56&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=WVM7n66pfEu%2FkhbMR%2F%2FwKA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps Support Engineer",
    "company": "Nihon Kohden Digital Health Solutions",
    "location": "Irvine, CA",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-support-engineer-at-nihon-kohden-digital-health-solutions-4295710052?position=57&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=7zUVTEq4rNcCLi4qi2wpMg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "PingWind",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pingwind-4316019938?position=58&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=Uk9C47GhKQQBe8aFzW2kUw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Cymertek Corporation",
    "location": "San Antonio, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cymertek-corporation-4336305401?position=59&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=ctJAt8aQPy9cSXvURMWOyA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Ryan",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ryan-4284462825?position=60&pageNum=0&refId=a6vM9FZU54yRH5LeMf3gmg%3D%3D&trackingId=RTaVMNvWcMWGQ6waVwtTpQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Mid-Junior DevOps Engineer - USA",
    "company": "HERE",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/mid-junior-devops-engineer-usa-at-here-4347377348?position=1&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=xHaOhXPaHGOIJI2%2Fe%2BoFtA%3D%3D",
    "description": "Mid-Junior DevOps Engineer\n\nLocation: New York, NY / Hybrid Remote / Remote within USA (EST / CST time zone)\n\nWe have an office in New York City and this position can either be based in the office, hybrid remote, or remote within the EST/CST time zones (subject to your existing legal right to work in the jurisdiction).\n\nAbout HERE\n\nEverything works right here‚Ñ¢.\n\nTraditional browsers weren't built for work. In today's enterprise environment‚Äîwhere security threats are constant and productivity is critical‚Äîlegacy browsers fall short. That's why we built HERE, the browser purpose-built for work.\n\nPowered by Chromium, HERE Enterprise Browser combines enterprise-grade security, seamless productivity, and native AI integration in one secure, intelligent workspace. Designed for regulated industries, HERE offers deep policy controls, identity-based access, secure workspace isolation, and full interoperability across SaaS, legacy, and virtualized environments. Our platform enables teams to work faster, more securely, and more intelligently‚Äîwithout compromise.\n\nHERE technology is trusted by 90% of global banks and also used within the U.S. Intelligence Community and other sectors. We're backed by some of the world's most respected financial institutions and venture firms, including Bain Capital Ventures, Bank of America, J.P. Morgan, Wells Fargo and IQT, the not-for-profit strategic investor that accelerates the introduction of groundbreaking technologies to enhance the national security of America and its allies.\n\nAbout the Role\n\nHERE is seeking a mid-junior DevOps Engineer to join our infrastructure team! The primary responsibilities for this role will span CI/CD pipeline engineering and cloud operations, maintaining and improving our GitHub and GitLab CI/CD pipelines, and supporting our AWS cloud infrastructure. In this role, you will gain hands-on experience with real production build systems and cloud platforms- while having the opportunity to work on practical projects that directly impact both our development velocity and operational reliability.\n\nWe're actively evolving toward a cloud-agnostic, multi-cloud architecture and migrating to Kubernetes for container orchestration. While current AWS and ECS experience is essential, having exposure to Azure, GCP, and Kubernetes will position you well for our infrastructure roadmap.\n\nThis role offers the opportunity to collaborate with senior engineers who will provide guidance and mentorship, whilst giving you ownership of projects across the DevOps lifecycle. This is an excellent platform for building practical experience with modern build engineering (CI/CD automation, cloud infrastructure, and deployment practices) within a production environment.\n\nResponsibilities\n\n\nCI/CD Pipeline Development:\nBuild, maintain, and optimize GitLab CI/CD pipelines for multi-platform builds (Windows, macOS, Linux).\nWork with YAML configurations, pipeline stages, artifacts, and deployment workflows.\nCloud Infrastructure Operations:\nHelp maintain and improve AWS infrastructure including ECS/Fargate deployments, RDS databases, Route53 DNS, VPC networking, and IAM policies.\nSupport multi-tenant and multi-region architecture.\nContainer & Deployment Management:\nWork with Docker containers, ECS task definitions, and ECR registries.\nDeploy and manage containerized Node.js applications in production environments.\nRelease Management:\nHelp manage release processes including version promotion, release channels (canary, beta, stable), and automated deployment to staging and production environments.\nDatabase Operations:\nSupport PostgreSQL on AWS RDS‚Äîbackups, SSH tunneling through bastion hosts, read-only user management, and database configuration for multi-tenant environments.\nAutomation & Scripting:\nWrite and maintain automation scripts in Bash, PowerShell, Python, and Node.js.\nBuild tools to improve infrastructure reliability and developer experience.\nInternal Tools Support:\nHelp maintain web-based DevOps tools built with Express.js, React, and TypeScript‚Äîtools for cloud settings management, tenant provisioning, and deployment monitoring.\n\nWhat We're Looking For\n\nIdeally 2 to 4 years of experience with the following core requirements:\n\n\nGitLab CI/CD: Experience with GitLab CI/CD pipelines‚ÄîYAML configuration, stages, jobs, artifacts, rules, dependencies.\nUnderstanding of CI/CD best practices and pipeline optimization.\nAWS Cloud Fundamentals: Practical experience with core AWS services‚ÄîEC2, ECS/Fargate, RDS, Route53, VPC, IAM, Secrets Manager, CloudWatch. Comfortable navigating the AWS Console and CLI.\nMulti-Platform Scripting: Solid scripting skills in Bash (Linux) and PowerShell (Windows). Ability to write maintainable automation scripts for both platforms.\nContainerization: Hands-on Docker experience‚Äîbuilding images, writing Dockerfiles, docker-compose, understanding container networking, and working with ECS/ECR.\nBuild Systems: Experience with build tools and package managers‚Äînpm/Node.js, .NET/NuGet, Python packaging. Understanding of dependency management and build artifacts.\nVersion Control: Strong Git fundamentals‚Äîbranching strategies, merge requests, tagging. Experience with GitHub (or GitLab) workflows and code review practices.\nLinux/Unix & Windows: Comfortable in both environments‚ÄîSSH, file permissions, package managers, systemd, PowerShell. Understanding of cross-platform operational challenges.\nNode.js/JavaScript: Comfortable reading and writing JavaScript/Node.js code. Experience with npm, package.json, and basic Express.js applications for tooling.\n\n\nNice to Have\n\n\nKubernetes experience (EKS, GKE, AKS) or willingness to learn, we're migrating from ECS to K8s\nMulti-cloud experience (Azure, GCP) or cloud-agnostic architecture knowledge\nGitLab Runner administration and configuration\nAWS CDK or CloudFormation for Infrastructure as Code\nTerraform for multi-cloud infrastructure management\nTypeScript development experience\nPostgreSQL database administration and optimization\n.NET build systems and NuGet package management\nReact or frontend framework experience\nAirflow or workflow orchestration tools\nHelm charts and Kubernetes manifest management\n\n\nWhat We're Offering\n\nBenefits -\n\n\nGenerous Paid Time Off, Paid Holidays & Sick Time\nCompetitive & Comprehensive Health Insurance\nThoughtfully-Planned Paid Parental Leave\nFinancial Well-Being Plans (FSA) (401k) (Life Insurance)\nStock Options\nProfessional Development Courses\nEmployee Resource Groups\n\n\nAdditional Perks -\n\n\nOne Medical - Free Membership\nTalkspace - Mental Health Therapy 24/7\nTeam Lunches\nCasual dress code\nCommuter Benefits (NYC employees only)\nCitibike (NYC employees only)\n\n\nLife at HERE\n\nAt HERE, we pride ourselves on fostering a friendly, collaborative, and supportive culture that truly respects the diversity of thought. Our goal is to create a space where employees can learn and innovate, and overall, have a good time doing it. We value and appreciate that our employees have a wide set of interests and experiences and put importance on taking the time to get to know one another and form relationships. From virtual socials and in-person events, to informal meetings and employee resource groups, we make it easy to engage and connect. Our environment promotes a productive, enjoyable learning experience - aligned together, working to create compelling solutions for our clients. Everything works right here.‚Ñ¢\n\nWe are HERE - Read about our recent rebrand from OpenFin to HERE\n\nRecent Awards\n\n\nVoted \"Enterprise Browser of the Year\" by CIO Review (2025)\nVoted \"100 Best Midsize Companies to Work For in NYC\" by BuiltIn (2025)\nVoted \"Top 10 Contact Center Technologies & Capabilities of 2024\" by CX Today (2024)\nVoted \"Best Enterprise Environment for Interoperability\" by TradingTech Insight Awards Europe (2024)\nVoted \"Top 50 Best Startups to Work for in the US\" & \"Top 50 Best Startups to Work for in New York\" by BuiltIn (2024)\nVoted as a \"Best Employer Award\" finalist at the UK FinTech Awards (2023)\nVoted \"Best FinTech Company CEO\" at the FinTech Breakthrough Awards (2023)\nVoted \"Best Internal Talent Team\" by Financial Technologist (2023)\nVoted \"Best Solution for Workflow Automation\" at the Trading Tech Insight Awards (2023)\nVoted \"Top Innovator Across Financial Markets\" in TabbFORUM NOVA Awards (2023)\nVoted \"Best User Interface Innovation\" in the Risk Markets Technology Awards (2023)\nVoted \"Top 100 Most Promising Private FinTech Companies\" by CB Insights (2023)\nVoted \"Most Influential Financial Technology Firm\" by Harrington Starr (2023)\n\n\nRECRUITERS NOTICE: Recruiters - if you wish to reach out to us regarding this job posting, you may reach out to externalrecruitment@here.io in order for your communication to be reviewed. HERE will review these communications if external help is needed for a position. Agencies may not contact individuals within our organization with solicitations. Firms that do not follow these guidelines risk having all communication from their firm being blocked. We thank you in advance for your cooperation in following our process.\n\nSponsorship - While we highly value all of our candidates, we are not offering sponsorship for this role.\n\nSalary Range: $70k - $120k\n\nSalary Range Disclaimer: This base salary range represents the low and high end salary range for this particular position; not all encompassing of the total compensation package. Actual salaries may vary depending upon but not limited to experience, special skill set, education and location. This range represents only one aspect of HERE's total compensation package offered to employees. Other forms of compensation may be stock options, commissions, paid time off and other variable benefits. Learn more about additional HERE compensation benefits above."
  },
  {
    "title": "junior devops engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-ova-work-4309344701?position=2&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=DY89pQYc6%2B%2FaeJ1xyvDHlA%3D%3D",
    "description": "Job Title: Junior DevOps Engineer\n\nLocation: Remote\n\nJob Type: Full-time\n\nExperience Level: Entry-Level (0-2 years)\n\nDepartment: IT / Engineering / DevOps\n\nJob Summary\n\nWe are looking for a motivated and detail-oriented Junior DevOps Engineer to join our growing DevOps team. This role is ideal for someone with a foundational understanding of DevOps practices and a passion for automation, cloud technologies, and continuous integration/deployment. You will assist in maintaining and improving our infrastructure, deployment pipelines, and monitoring systems.\n\nKey Responsibilities\n\n\nAssist in the setup, maintenance, and monitoring of CI/CD pipelines.\nSupport cloud infrastructure (AWS, Azure, GCP) and help manage deployments.\nCollaborate with development and operations teams to ensure reliable software delivery.\nWrite scripts and automation tools to streamline operations and deployments.\nMonitor system performance and troubleshoot issues in development and production environments.\nMaintain documentation for infrastructure and deployment processes.\nLearn and apply best practices in security, scalability, and reliability.\n\n\nRequired Qualifications\n\n\nBachelor's degree in Computer Science, Information Technology, or related field.\nBasic understanding of DevOps principles and software development lifecycle.\nFamiliarity with Linux/Unix systems and shell scripting.\nExposure to cloud platforms (AWS, Azure, or GCP).\nExperience with version control systems (e.g., Git).\nKnowledge of CI/CD tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nStrong problem-solving and communication skills.\nEagerness to learn and grow in a fast-paced environment.\n\n\nPreferred Qualifications\n\n\nInternship or project experience in DevOps or system administration.\nFamiliarity with containerization tools (Docker) and orchestration (Kubernetes).\nExperience with Infrastructure as Code (Terraform, Ansible).\nBasic knowledge of monitoring tools (Prometheus, Grafana, ELK Stack).\n\n\nBenefits\n\n\nCompetitive salary and growth opportunities.\nMentorship from senior engineers.\nHealth and wellness benefits.\nFlexible work hours and remote work options.\nAccess to training and certification programs."
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4341955705?position=3&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=wSeowlAg3U9jd%2BOARVllBg%3D%3D",
    "description": "Over 12 -15 years of overall expereince needed.\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nKubernetes (3-4 YOE) and Fieldglass Experience (1-2 YOE)"
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4347005704?position=4&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=xoXW%2BJGzyU889cXegsu2tw%3D%3D",
    "description": "Bachelor's degree in a technical field such as computer science, computer engineering or related field required 0-2 years experience required.\n1-2 years of experience with Kubernetes.\nISBN experience preferred.\nA solid foundation in computer science , with strong competencies in data structures, algorithms, and software design large systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems experience in programming and experience with problem diagnosis and resolution."
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "GliaCell Technologies",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-gliacell-technologies-4338894490?position=5&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=nuAwBPVqxsdtYM3iL8GQ0g%3D%3D",
    "description": "An active or rein-statable TS/SCI with Polygraph security clearance is REQUIRED. Please do not apply if you currently do not possess this level of clearance.***\n\n\nAre you a Junior DevOps Engineer who is ready for a new challenge that will launch your career to the next level?\n\n\nTired of being treated like a company drone?\nTired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?\nOur engineers were certainly tired of the same.\n\n\nAt GliaCell our slogan is ‚ÄúWe make It happen‚Äù.\n\n\nWe will immerse you in the latest technologies.\nWe will develop and support your own personalized training program to continue your individual growth.\nWe will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.\n\n\nCulture isn‚Äôt something you need to talk about‚Ä¶if it just exists.\n\nIf this sounds interesting to you, then we‚Äôd like to have a discussion regarding your next adventure! If you want to be a drone, this isn‚Äôt the place for you.\n\nWe Make It Happen!\n\nGliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.\n\nGliaCell‚Äôs Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.\n\nWe Offer\n\n\nLong term job security\nCompetitive salaries & bonus opportunities\nChallenging work you are passionate about\nAbility to work with some amazingly talented people\n\n\nJob Description\n\nGliaCell is seeking a Junior DevOps Engineer on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.\n\nResponsibilities\n\n\nEstablishing a test framework and automated tests utilizing Cucumber and Cypru\nKnowledgeable in Microservices design & architecture, CI/CD, Test frameworks and automation, Agile Methodology.\nExecute load and performance testing, chaos testing, functional testing and end-to-end testin\nAgile development and delivery of software\nCommunication and collaboration: Software Development is a team-oriented discipline. Engineers need to be able to communicate and collaborate effectively with other team members, as well as with stakeholders.\n\n\nRequired Skills\n\n\nPython and Cucumber\n\n\nDesired Skills:\n\n\nAWS services such as Lambdas, Step Functions, EC2 and S3\n\n\nKey Requirements\n\nTo be considered for this position you must have the following:\n\n\nPossess an active or rein-statable TS/SCI with Polygraph security clearance.\nU.S. Citizenship.\nWorks well independently as well as on a team.\n6+ years experience as a Developer in programs and contracts of similar scope, type, and complexity is required. A bachelor‚Äôs degree in a technical discipline from an accredited college or university is required. Five (4) years of development experience may be substituted for a bachelor‚Äôs degree.\n\n\nLocation: Annapolis Junction, MD\n\nSalary Range: The salary range for this full-time position is $50,000 to $120,000. Our salary ranges are determined by position, level, skills, professional experience, relevant education and certifications. The range displayed on each job posting reflects the minimum and maximum target salaries for this position across our projects. Within the range, your salary is determined by your individual benefits package selection. Your recruiter can share more about the specific salary range for your preferred position during the hiring process.\n\nBenefits\n\n\nMedical, Dental, and Vision Coverage for Employee and Dependents\nUp to 25 Days of Paid Time Off\nUp to 40 hours of PTO Carryover\n11 Federal Government Holidays\nWork From Home Opportunities\n401K Company Contribution, Fully Vested Day 1\nDiscretionary, Certification, and Sign-On Bonus Potential\nEmployee Referral Bonus Program\nAnnual Professional Development\n100% Premium Covered for Life & Disability Insurances\nAdditional Voluntary Life Insurance Coverage Available\nEmployee Assistance Program\nTravel Protection Program\nFinancial Planning Assistance\nBereavement and Jury Duty Leave\nMonthly Team and Family Events\nTechnology Budget\nGlobal Entry\nAnnual Swag Budget\n\n\nLearn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/\n\nGliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
  },
  {
    "title": "DevOps Cloud Engineer Based in U.S.A",
    "company": "Advancio",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-cloud-engineer-based-in-u-s-a-at-advancio-4324442139?position=6&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=uLqSGjoKo22trA1SVgOuMw%3D%3D",
    "description": "This is a remote position.\n\nWho We Are:\n\n\nAt Advancio, we are passionate about technology and its ability to transform the world. We are rapidly expanding and building a company where we serve exceptional businesses, hire top talent, and have a lot of fun doing what we love!\n\n\nJob Summary:\n\nWe are seeking a skilled DevOps Cloud Engineer to design, implement, and manage scalable cloud-based infrastructure and DevOps processes. The ideal candidate will have extensive experience with cloud platforms, CI/CD pipelines, and automation tools, ensuring the efficient deployment and operation of applications.\n\n\nWhat will you do:\n\n\nDesign, deploy, and manage cloud infrastructure on platforms such as AWS, Azure, or Google Cloud Platform (GCP).\n\nBuild and maintain CI/CD pipelines to streamline development and deployment processes.\n\nAutomate infrastructure provisioning, configuration, and monitoring using tools like Terraform, Ansible, or similar.\n\nEnsure system reliability, availability, and performance through robust monitoring and alerting.\n\nCollaborate with development teams to optimize the delivery and scalability of applications.\n\nManage containerized workloads using Docker and orchestration platforms such as Kubernetes.\n\nImplement security best practices for cloud environments, including identity management, encryption, and compliance adherence.\n\nStay updated with the latest DevOps tools and methodologies to enhance team efficiency.\n\n\n\n\nRequirements\n\n\n\n\n\n\n5+ years of experience in DevOps, cloud engineering, or related roles.\n\nAdvanced English communication skills, both verbal and written.\n\nProficiency in at least one major cloud platform (AWS, Azure, or GCP).\n\nHands-on experience with CI/CD tools (e.g., Jenkins, GitLab CI/CD, CircleCI).\n\nStrong scripting skills in Python, Bash, or similar languages.\n\nSolid knowledge of infrastructure-as-code (IaC) tools like Terraform or CloudFormation.\n\nExperience with containerization (Docker) and orchestration (Kubernetes).\n\nFamiliarity with monitoring and logging tools like Prometheus, Grafana, or ELK Stack.\n\nStrong understanding of networking, security, and system architecture."
  },
  {
    "title": "DeVops Engineer",
    "company": "Pittsburgh Robotics Network",
    "location": "Pittsburgh, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pittsburgh-robotics-network-4347073200?position=7&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=woNVKIYyC5ACM9jNZfAVEg%3D%3D",
    "description": "DevOps Engineer\n\nTDK SensEI\n\nPittsburgh, PA\n\nThis position is for our Pittsburgh, PA office - only apply if you are based there or willing to relocate.\n\nAt TDK SensEI, we are transforming how industrial customers utilize and interact with sensor data. We specialize in developing advanced AI solutions capable of running directly on edge devices. By processing data locally, TDK SensEI enhances real-time decision-making, privacy, security, and cost efficiency. Our offerings include automated machine learning tools, AI-powered condition-based monitoring systems, and various sensor devices optimized for low latency and power consumption. Collaborating with leading global companies, we empower teams to effortlessly devise and implement machine learning solutions for industrial applications, all without the need for coding.\n\nWe are seeking a Dev Ops engineer to join our team. In this position, the candidate will be responsible for managing, operating, and provisioning cloud environments such as AWS, Azure, Google cloud. You will work with development, security, and operations teams to deploy, scale, and operate dev environments. You are also responsible for improving and automating the dev environment. You will establish configuration management, automate our infrastructure, implement continuous integration, and train the team in DevOps best practices.\n\nAs a Dev Ops Engineer, Your Responsibilities Will Include\n\n\nDesigning, implementing, and maintaining tools and processes for continuous integration, delivery, and deployment of software\nWorking with developers to deploy and manage code changes\nWorking with operations staff to ensure that systems are up and running smoothly\nAutomating, monitoring, testing, configuring, networking, and Infrastructure as Code (IaC)\nStreamlining and automating processes while troubleshooting existing development procedures\nManaging the creation, release, and configuration of production systems\nArchitecting and optimizing several service components running on AWS environment\n\n\nSkills & Requirements\n\n\nBachelor‚Äôs degree or equivalent experience\nMinimum 2 years of experience in DevOps, infrastructure automation or similar role\nKnowledge of Linux/UNIX administration\nProficiency in Python, JavaScript and other script environments (e.g. bash)\nExperience with containerization technologies, Docker and associated tooling\nExperience designing and implementing CI/CD pipelines\nExperience operating databases such as PostgreSQL or MySQL, especially in cloud-native services like RDS\nAwareness of critical concepts in DevOps and Agile principles\nUS work authorization\n\n\nNice To Have\n\n\nAWS certifications (e.g., AWS Certified DevOps Engineer, AWS Certified Solutions Architect).\nFamiliarity with other cloud providers (Azure, Google Cloud).\nExperience with container orchestration systems such as Kubernetes/EKS"
  },
  {
    "title": "DevOps Engineer",
    "company": "Princeton IT Services, Inc",
    "location": "Englewood Cliffs, NJ",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-princeton-it-services-inc-4338714288?position=8&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=k%2Fa4ecHaA7u8zQRszZwicA%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Englewood Cliffs, NJ\n\nEmployment Type: W2 Only\n\nJob Summary\n\nWe are seeking a DevOps Engineer with strong hands-on experience in Linux, Docker, and Kubernetes to support and optimize our deployment environment in Englewood Cliffs, NJ. This is a W2-only role requiring solid skills in automation, CI/CD, and container orchestration. The ideal candidate will ensure smooth application releases, maintain system stability, and collaborate closely with development teams.\n\nKey Responsibilities\n\n\nManage and support Linux-based systems in production and staging environments.\nBuild, maintain, and optimize CI/CD pipelines for automated deployments.\nCreate, manage, and troubleshoot Docker containers and images.\nDeploy, monitor, and tune Kubernetes clusters and workloads.\nAutomate infrastructure tasks using Shell or Python scripts.\nImplement and manage monitoring and logging tools (Prometheus, Grafana, ELK, etc.).\nTroubleshoot system, container, and cluster-level issues end-to-end.\nWork cross-functionally with development and QA teams to ensure smooth releases.\n\n\nRequired Skills\n\n\n8+ years of DevOps or related experience.\nStrong hands-on experience with Linux administration.\nSolid experience working with Docker for containerization.\nStrong working knowledge of Kubernetes (deployments, scaling, troubleshooting).\nExperience building CI/CD pipelines (Jenkins, GitLab CI, GitHub Actions).\nStrong scripting skills in Shell/Bash/Python.\nExperience with monitoring and logging tools."
  },
  {
    "title": "DevOps Engineer",
    "company": "Lean TECHniques",
    "location": "Johnston, IA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lean-techniques-4336685413?position=9&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=YJJMdiOubtmtb%2FCA6zw03g%3D%3D",
    "description": "Maybe you‚Äôre bored and need a new challenge. Or you‚Äôre sick of all the bureaucracy and just want to focus on designing kick-ass software.\n\nWhatever the reason, we want you to know that LT is different. And not just air quotes ‚Äúdifferent,‚Äù but more like ‚Äúbreathing easy for the first time in a long time‚Äù different.\n\nIt‚Äôs a place where you can write your own story and make a difference along the way. At LT, you‚Äôll have the freedom and flexibility to do what you think needs to be done, and you‚Äôll get to do it while working alongside a team of other curious individuals who love a good challenge too.\n\nWe‚Äôre currently looking to add a DevOps Engineer to our crew of nerds. If you‚Äôre someone who has 5+ years of DevOps experience, we'd love to chat!"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337132819?position=10&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=IDhA1Yg1Rxg0CC5up6SJmw%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "DevOps Engineer (35 LPA - 55 LPA)",
    "company": "CodeRound AI",
    "location": "Greater Bloomington Area",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-35-lpa-55-lpa-at-coderound-ai-4308183910?position=11&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=hpkjwgTlAEYKBTppU8xHzA%3D%3D",
    "description": "üöÄ What We‚Äôre Building\n\n\nCodeRound AI matches top 5% tech talent to fastest growing VC funded AI startups.\nCandidates apply once and get UPTO 10 remote as well as onsite interview opportunities IF selected!\nTop-tier product startups in US, UAE & India have hired top engineers & ML folk using CodeRound\n\n\nüß© What You‚Äôll Do\n\n\nBuild and optimize our cloud infrastructure ‚Äî scalable, secure, and cost-effective (mostly AWS).\nSet up and manage CI/CD pipelines to ensure smooth deployment across backend, AI services, and mobile.\nContainerize backend services (FastAPI, Rails) and optimize them for performance.\nImplement monitoring, alerting, and logging to catch issues before users do.\nOptimize database performance (Postgres, Redis) and manage backups and scaling.\nCollaborate with backend, AI, and product teams to deploy new features safely and quickly.\nChampion infra-as-code and automation wherever possible.\n\n\nüí• Why this is exciting\n\n\nYou'll own DevOps for a high-usage, real-world AI platform ‚Äî not just internal tools.\nYou‚Äôll work on real-time, high-stakes flows ‚Äî interviews, scoring, hiring decisions.\nYou‚Äôll work closely with founders, ship weekly, and see the direct impact of your work.\n\n\n‚úÖ You‚Äôll Be Great At This If You\n\n\nHave 4+ years of experience as a DevOps engineer, SRE, or infrastructure engineer.\nAre strong with AWS services (EC2, RDS, ECS/EKS, S3, CloudWatch).\nCan write clean, reusable Terraform or CloudFormation code.\nHave experience setting up CI/CD pipelines and optimizing build/release flows.\nAre comfortable with Docker, Linux servers, and basic networking (VPCs, security groups).\nUnderstand application and database scaling (horizontal/vertical).\n\n\n‚ö° Bonus If You\n\n\nHave experience supporting AI/ML pipelines in production (fine-tuning infra, vector DBs, etc.).\nKnow cost optimization tricks for cloud infra (spot instances, autoscaling groups, etc.).\nAre excited to eventually build a small infra team"
  },
  {
    "title": "Devops Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341915759?position=12&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=LjLS%2FYPh4IcJm0dQGUyyTQ%3D%3D",
    "description": "Job Description:\n\n\nServe as a subject matter expert to develop and support DevOps Web Access Management solutions\nInstall, configure, and maintain automation solutions, in support of KeyBank infrastructure\nDevelop Standard Operating Procedures, maintenance plans and provide status reports as required\nPerform daily operational tasks as required for the Web Access Management team\n\n\nQualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "Huntington Beach, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337182535?position=13&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=GdEMG4W3PVOCpXvkjhoVYA%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "Devops Engineer",
    "company": "Hoplite Solutions LLC",
    "location": "Bethesda, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hoplite-solutions-llc-4336082750?position=14&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=yHvqvTLgNvsoaDmAvu4mFQ%3D%3D",
    "description": "Hoplite Solutions is hiring DevOps Engineers at all experience levels to join our team in Bethesda, MD. In this mission-critical role, you will provide essential system support to our customer while collaborating closely with software development teams and other key technology stakeholders. You will help maintain, enhance, and support a range of IC enterprise products‚Äîboth legacy systems and new solutions‚Äîwithin an Agile SAFe environment.\n\nAs a DevOps Engineer, you will work hand-in-hand with software engineering teams to deploy and operate systems, automate and optimize processes, and build and maintain tools that support deployment, monitoring, and ongoing operations. You will also troubleshoot and resolve issues across development, test, and production environments, ensuring reliability, efficiency, and continuous improvement across the enterprise.\n\nPrimary Responsibilities:\n\n\nSupports software deployments, cloud infrastructure baselines, and operational availability of production systems\nManaging, building, configuring, administering, operating and maintaining all components that comprise the DevOps environment\nDefining enterprise Continuous Integration/Continuous Deployment processes and best practices\nCodifying DevOps best practices across the enterprise\nDeveloping and maintaining scripts to automate tool deployment to an AWS cloud environment and other tasks\nScripting and maintaining build environments\nWorking with project teams to integrate their products into the DevOps environment\n\n\nBasic Qualifications\n\n\nDemonstrated experience setting up one or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience troubleshooting issues with two or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience working within a software development team and supporting developers and developer activities\nBachelors degree with 4 or more years of prior relevant work experience or Masters with 2 or more years of prior relevant work experience. Will consider additional work experience in lieu of a degree\nTo be considered must have an active TS/SCI with polygraph security clearance\n\n\nPreferred Qualifications\n\n\nAWS Associate Certification (Developer, Solution Architect, or Sys Ops Administrator)\nAWS Professional Certification (DevOps Engineer or Solutions Architect)\nDemonstrated experience in container orchestration using Docker, Vagrant, Kubernetes, or AWS ECS/ECR\nDemonstrated experience with Languages including Java, Python, JavaScript, Ruby, PHP, and Unix shell Scripting\nDemonstrated experience with Ansible, or Puppet\n\n\nHoplite Solutions offers very competitive salaries and an excellent benefits package, to include a 7% employer 401k contribution, fully paid healthcare for our employees, outstanding training benefits, company funded life insurance and short-term disability insurance, and many more.\n\nPowered by JazzHR\n\nwwBe8pS8mn"
  },
  {
    "title": "DevOps Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341985652?position=15&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=AWXQz9EELpZu36y0cpMgSg%3D%3D",
    "description": "Qualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments."
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "Indianapolis, IN",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4339356296?position=16&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=NyL7ZjoApv6V%2Fk5Ozm1kSw%3D%3D",
    "description": "Who we are‚Ä¶\n\nVerra Mobility is a global leader in smart mobility. We develop technology-enabled solutions that help the world move safely and easily. We are fostering the development of safe cities, working with police departments and municipalities to install over 4,000 red-light, speed, and school bus stop arm safety cameras across North America. We are also creating smart roadways, serving the world's largest commercial fleets and rental car companies to manage tolling transactions and violations for over 8.5 million vehicles. And we are a leading provider of connected systems, processing nearly 165 million transactions each year across 50+ individual tolling authorities.\n\nCulture\n\nVerra Mobility Corporation is a rapidly-growing, entrepreneurial company that operates with a people-first philosophy and approach. The company lives by its core values‚ÄîDo What's Right, Lead with Grace, Win Together, and Own It‚Äîin everything it does for its customers and team members. The company seeks to grow aggressively, both organically and through acquisition, to continue to be the undisputed market leader with these five core competencies: bias for action, customer focus, teamwork, drive for results, and commitment to excellence.\n\nPosition Overview:\n\nWe are seeking an experienced and detail oriented Devops Engineer to join our team. In this role, you will be responsible for creating, maintaining, and securing our Devops pipelines and deployment systems to ensure high levels of performance and availability. This position is ideal for someone with a strong background in CI/CD methodologies particularly in cloud environments.\n\nEssential Responsibilities:\n\n\nSpend 50% writing automation scripts in Python and Bash.\nWrite various CI/CD pipelines for code releases.\nEnsure that pipelines meet both operations and security requirements.\nPartner with developers to identify areas of improvement in the developer experience.\nDesign and implement innovations that improve software velocity, infrastructure resiliency, security and data availability.\nWork with Software and Engineering to ensure new pipelines are created in parallel to code build.\nWork with Architecture on setting the path forward and gathering changes to the technology stack.\nAbility to respond to system issues, drive and participate in high - priority incident calls and emergency activities outside of standard office hours as needed.\nCollaborate with internal and external application, business partners to gain understanding of their business needs and adapt departmental roadmap plans and priorities to address operational challenges.\nWork with QE to ensure all automated testing is run during the deployment of the code.\nAbility to participate in an on-call rotation as needed.\n\n\nQualifications:\n\n\nMust have 5 years of Devops Engineering experience.\nFamiliarity with a wide range of systems engineering tools, including source code repository hubs, continuous integration services, issue tracking, test automation, deployment automation, development team collaboration, project management.\nNeed to have strong scripting skills to create automation in Python preferred or Bash.\nExperience with Cloudformation or Terraform for infrastructure as code.\nUsed continuous integration and continuous development (CI/CD) tools such as Jenkins, Gitlab, or Github Actions, preferred.\nKnowledge of DevOps tools such as, GitHub Actions, CloudFormation, GIT, SVN, Jenkins, JIRA, Rally, Greenhopper, Puppet/Chef Vagrant, Selenium, Azure DevOps (for sprint planning).\nUnderstanding of enterprise GIT repositories including branching and forking.\nHands-on Familiarity with AWS CloudWatch, AWS CloudTrail, AWS X-Ray, Grafana, and Prometheus.\nHands-on experience with Veracode and SonarQube are a plus.\nMust be located in Phoenix, AZ, Indianapolis, IN, or NY and be willing to commute into office 3 days a week.\n\n\nThis position is not eligible for sponsorship now or in the future and is only considering local Arizona, New York, or Indiana talent.\n\n\n\nVerra Mobility Values\n\n\n\nAn ideal candidate for this role naturally works in alignment with the Verra Mobility Core Values:\n\n\nOwn It. We focus on high performance and drive toward breakthrough outcomes. Our employees ensure accountability, optimize and align work, focus on the customer, and cultivate innovation.\nDo What's Right. We champion integrity and good character. Our team members model ethical behavior, demonstrate good judgment and are courageous.\nLead with Grace. We express humility and compassion, and we are authentic and candid. Our employees demonstrate self-awareness, care for others, instill trust, and communicate effectively.\nWin Together. We believe in growing and inspiring people together. We seek people who collaborate, value differences, think and act globally, foster an engaging work environment, and recognize and develop others.\n\n\n\n\nWith your explicit consent which you provided as part of the application process, we will retain candidate personal data solely for the business purpose for which it was collected. In no event will we retain such data more than two (2) years following the closure of the recruitment process relating to the role for which you applied or in the event other related job opportunities arise within the company. Verra Mobility Applicant Privacy Notice\n\nVerra Mobility is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status."
  },
  {
    "title": "DevOps Engineer",
    "company": "Protege",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-protege-4331315574?position=17&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=F%2BXW79xICFlj%2B5PnCwsL7g%3D%3D",
    "description": "Company Overview:\n\nWe are building Protege to solve the biggest unmet need in AI ‚Äî getting access to the right training data. The process today is time intensive, incredibly expensive, and often ends in failure. The Protege platform facilitates the secure, efficient, and privacy-centric exchange of AI training data.\n\nSolving AI‚Äôs data problem is a generational opportunity. We‚Äôre backed by world-class investors and already powering partnerships with some of the most ambitious teams in AI. The company that succeeds will be one of the largest in AI ‚Äî and in tech.\n\nWe‚Äôre a lean, fast-moving, high-trust team of builders who are obsessed with velocity and impact. Our culture is built for people who thrive on ambiguity, own outcomes, and want to shape the future of data and AI.\n\nKey Responsibilities and Scope:\n\n\nAs a DevOps Engineer, you will be a critical part of our engineering team, responsible for safeguarding our AI/ML platforms, data pipelines, and cloud infrastructure\nYou will implement and develop monitoring strategies, and drive controls to protect our most valuable assets\n\n\nQualifications:\n\n\n4+ years of hands-on experience in a DevOps, Architecture, SecOps or Engineering role\nStrong experience with major cloud platforms and building cloud-native services including containerization, threat detection, vulnerability, governance, compliance, etc. with AWS preferred\nProficient in scripting languages like Python, SQL, Typescript or similar\nStrong experience with infra‚Äëas‚Äëcode, monitoring, and reliability for pipelines; contributing to platform guardrails, governance, compliance, etc\nExperience working with cross-functional partners to develop tools and playbooks for best-practices related to Operations\n\n\nAbout You:\n\n\nYou are curious, tenacious, and proactive\nYou are not bothered by ambiguity but embrace finding patterns in complex environments\nExcellent problem-solving skills and adaptability in a dynamic and evolving tech landscape\nExcited to work in a company that deals with moving and transforming large volumes of data\n\n\nBonus if you have these attributes:\n\n\nExperience with cloud providers like GCP and Azure\nPrior startup experience\nSecurity operations and automation experience"
  },
  {
    "title": "Devops Engineer",
    "company": "PDG Consulting",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pdg-consulting-4321885957?position=18&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=1MGktynIGYq7IYavpSgrrQ%3D%3D",
    "description": "Overview\n\nWe are seeking a DevOps Engineer to set up, manage, and automate software development operations and processes. The ideal candidate will have strong experience in CI/CD pipelines, cloud service management, and infrastructure monitoring to support efficient, secure, and scalable software delivery.\n\nResponsibilities\n\n\nDesign, implement, and manage CI/CD pipelines to streamline software deployment and integration.\nOversee cloud-based systems and infrastructure management, ensuring reliability and performance.\nAutomate workflows for system administration, documentation, and monitoring.\nSupport the development and deployment of AI chatbot infrastructures and related frameworks.\nCollaborate with developers, QA engineers, and IT teams to optimize the software lifecycle.\n\n\nRequirements\n\n\nMinimum 4 years of experience in ICT systems support, including system administration, documentation, and monitoring.\nHands-on experience with cloud platforms, especially Amazon Web Services (AWS).\nProven experience creating and maintaining AI chatbot infrastructures or similar automation frameworks.\nPrevious experience working within the UN system is an advantage.\nExcellent command of English (required).\nKnowledge of French and Arabic is considered an advantage.\n\n\nPowered by JazzHR"
  },
  {
    "title": "DevOps Engineer - 100% Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-100%25-remote-at-the-dignify-solutions-llc-4347005722?position=19&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=AlC1S4uqiKVApgZzevrgPQ%3D%3D",
    "description": "Summary: The main function of a DevOps Engineer is to design, develop, implement, test, and maintain business and computer applications software or specialized utility programs including mainframe and client/server applications, and major enhancement of existing systems\n\nJob Responsibilities: Fine-tune and improve a variety of sophisticated software implementation projects Gather and analyze system requirements, document specifications, and develop software solutions to meet client needs and data Analyze and review enhancement requests and specifications Implement system software and customize to client requirements Prepare the detailed software specifications and test plans Code new programs to client's specifications and create test data for testing Modify existing programs to new standards and conduct unit testing of developed programs Create migration packages for system testing, user testing, and implementation Provide quality assurance reviews Perform post-implementation validation of software and resolve any bugs found during testing\n\n\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nSAC Experience (1-2 YOE)\nAriba ATHENA Report generation (Some experience)"
  },
  {
    "title": "CloudOps Engineer",
    "company": "Protera",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/cloudops-engineer-at-protera-4336621571?position=20&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=Lt7VQbc%2F3SHqEcoNuS%2Buxg%3D%3D",
    "description": "Summary\n\nAs a CloudOps Engineer at Protera, you will play a crucial role in maintaining and optimizing our cloud infrastructure. You will be responsible for monitoring and managing cloud services, ensuring the performance and reliability of our cloud applications, and automating processes to enhance operational efficiency. You will work closely with development teams to improve deployment practices and manage incidents in a fast-paced environment.\n\nKey Responsibilities\n\n\nMonitor cloud infrastructure performance and reliability to ensure optimal service delivery\nAutomate deployment processes using Infrastructure as Code (IaC) tools like Terraform\nImplement and manage CI/CD pipelines to streamline application releases\nCollaborate with development teams to integrate DevOps practices into application lifecycles\nTroubleshoot cloud architecture and application issues to ensure minimal downtime\nConduct security assessments and implement best practices to secure cloud environments\nDocument processes and maintain configurations and operational standards\n\n\nRequirements\n\nSkills & Qualifications\n\nExperience:\n\n\n3+ years of experience in cloud operations, DevOps, or system administration\n\n\nTechnical Skills:\n\n\nProficient with AWS services and cloud architecture\nExperience with Infrastructure as Code (IaC) tools, particularly Terraform\nStrong understanding of containerization technologies like Docker and orchestration tools such as Kubernetes\nFamiliarity with CI/CD tools such as Jenkins, GitLab CI, or similar\nKnowledge of monitoring tools and log management solutions\nSolid troubleshooting skills across cloud-based systems\n\n\nEducation:\n\n\nBachelor's degree in Computer Science, Information Technology, or a related field is preferred\n\n\nCertifications (Preferred):\n\n\nAWS Certified Solutions Architect or related cloud certification\nDevOps or Kubernetes certifications are a plus\n\n\nPersonal Attributes:\n\n\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills\nAbility to work in a fast-paced environment and handle multiple tasks\n\n\nAbout Protera\n\nProtera Technologies (www.protera.com) is a leading provider of total IT outsourcing solutions for SAP-centric organizations. Founded in the mid-1990s, we are pioneers in providing SAP services on the cloud, managing thousands of workloads across various cloud platforms. With headquarters in Chicago and offices in Greece and India, we are committed to delivering exceptional cloud hosting, application management, and professional services globally.\n\nBenefits\n\nProtera offers a variety of health and wellbeing programs. Benefit options include two PPO Medical plans, Dental, Vision, Health Savings Account, Flexible Spending Accounts, Dependent Care FSA, 401k retirement savings plan, company paid Life Insurance, Flexible PTO policy, Paid Holidays."
  },
  {
    "title": "DevOps Systems Engineer",
    "company": "TensorWave",
    "location": "Las Vegas, NV",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-tensorwave-4338727303?position=21&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=JFjgIHhpYOy2%2FmwGLi1rPw%3D%3D",
    "description": "At TensorWave, we‚Äôre leading the charge in AI compute, building a versatile cloud platform that‚Äôs driving the next generation of AI innovation. We‚Äôre focused on creating a foundation that empowers cutting-edge advancements in intelligent computing, pushing the boundaries of what‚Äôs possible in the AI landscape.\n\nAbout The Role\n\nWe are seeking a highly skilled DevOps & Infrastructure Management Engineer to join our growing infrastructure team. This role is ideal for someone who thrives in hardware-centric environments, enjoys hands-on datacenter and system administration work, and can build reliable automation around large-scale infrastructure. You will be responsible for managing enterprise hardware, monitoring systems, network operations, infrastructure automation, and supporting our compute clusters across multiple data centers.\n\nThis role touches every layer of modern infrastructure‚Äîfrom bare metal provisioning, to OS and Kubernetes management, to monitoring and troubleshooting hardware. If you are detail-oriented, resourceful, and comfortable working with both low-level hardware systems and higher-level DevOps tooling, we‚Äôd love to talk.\n\nKey Responsibilities\n\nHardware & Infrastructure Management\n\n\nManage and maintain enterprise-grade server hardware and infrastructure components.\nUtilize out-of-band management systems (iLO, iDRAC, IPMI, Redfish, etc.) for remote operations.\nUse automated hardware management tools (BMC/Redfish-based) to streamline provisioning and maintenance.\nPerform hardware diagnostics and troubleshooting (CPU, memory, disks, PSUs, NICs, etc.).\nHandle vendor interactions, including RMAs, part replacements, and inventory tracking.\nOversee datacenter hardware operations, including racking, cabling, PDU installation, and physical layout.\n\n\nDatacenter & DCIM\n\n\nUse Data Center Infrastructure Management (DCIM) tools for inventory, capacity planning, and environmental tracking.\nManage power delivery and consumption across racks and nodes.\nConfigure and monitor managed PDU systems for power cycling, monitoring, and alerts.\nCollaborate with colocation providers on connectivity, power, security, and maintenance tasks.\n\n\nMonitoring & Observability\n\n\nBuild and maintain infrastructure monitoring and alerting using tools such as Prometheus/Grafana, SNMP, Nagios, CheckMK, or similar platforms.\nImplement automated alerting for hardware health, network status, power issues, and service-level metrics.\nCreate dashboards to give internal teams visibility into system performance and reliability.\n\n\nNetwork Operations\n\n\nManage and configure firewalls, routing, and network segmentation.\nConfigure and troubleshoot VPN technologies (IPsec, OpenVPN, WireGuard).\nOversee subnetting, IP address allocation, and network architecture planning.\nConfigure managed switches, VLANs, port settings, and trunking.\nManage NAT, port forwarding, and related gateway/edge network configurations.\n\n\nSystem Administration (Linux)\n\n\nInstall, configure, and manage Linux servers (Ubuntu/Debian preferred).\nPerform system-level troubleshooting (boot issues, login problems, service failures).\nManage networking configuration (static IPs, DHCP).\nConfigure and maintain filesystems: partitioning, MD RAID, ext4/XFS, LVM, resizing/growing volumes.\nImplement secure access using public key authentication and proper SSH hardening.\nManage certificates for internal systems, including issuance, revocation, HTTPS installation, and rotation.\nHandle basic BIOS configuration relevant to bare metal provisioning or system bring-up.\n\n\nBare Metal Provisioning\n\n\nDeploy and manage hardware provisioning tools such as MAAS, Foreman, or similar systems.\nConfigure and troubleshoot network boot mechanisms (PXE, UEFI Boot, HTTP Boot).\nAutomate provisioning pipelines to rapidly bring new nodes online.\n\n\nContainerization & Orchestration\n\n\nWork with Kubernetes clusters at a foundational level (cluster access, basic resource troubleshooting).\nDeploy workloads using Helm charts and maintain cluster application lifecycle.\nAssist with cluster scaling, node replacements, and security hardening.\n\n\nAutomation & Scripting\n\n\nWrite shell scripts (bash) for automation of system tasks, monitoring, or provisioning.\nUse CLI tooling such as jq, sed, awk, grep, and rsync.\nOptionally automate workflows using languages like Python, Go, PHP, or Perl.\n\n\nRequired Qualifications\n\n\nProven experience managing enterprise-grade hardware at scale.\nStrong understanding of out-of-band management systems (IPMI/BMC/Redfish).\nHands-on expertise with monitoring systems (Prometheus, Grafana, SNMP, Nagios, CheckMK, or similar).\nSolid knowledge of network administration, including firewalls, routing, VPNs, NAT, and managed switches.\nLinux system administration experience (installation, configuration, troubleshooting).\nExperience with filesystems, RAID, partitioning, and general storage management.\nFamiliarity with certificate management, key-based auth, and basic cryptographic functions.\nExperience with bare metal provisioning (MAAS, Foreman, or similar).\nUnderstanding of PXE/UEFI/HTTP boot systems.\nAbility to write functional, maintainable bash scripts for automation.\n\n\nNice to Have\n\n\nExperience with Kubernetes beyond the basics (operators, cluster scaling, CRDs).\nExperience with Helm chart customization.\nFamiliarity with automation languages such as Python, Go, PHP, or Perl.\nPrevious datacenter operations or colocation management experience.\nExposure to high-availability or distributed compute environments.\nKnowledge of infrastructure security and hardening practices.\n\n\nWhat We Bring\n\n\nStock Options\n100% paid Medical, Dental, and Vision insurance\nLife and Voluntary Supplemental Insurance\nShort Term Disability Insurance\nFlexible Spending Account\n401(k)\nFlexible PTO\nPaid Holidays\nParental Leave\nMental Health Benefits through Spring Health"
  },
  {
    "title": "DevOps Administrator",
    "company": "The Amatriot Group",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-administrator-at-the-amatriot-group-4310974393?position=22&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=rfpXoHTMNKHeWpuG18qkvA%3D%3D",
    "description": "DevOps Administrator\n\nSalary: $135,000 ‚Äì 170,000\n\nContract Length: 12-month SOW\n\nLocation: Dallas, TX - in-office presence requirement 3 days weekly or more as needed\n\n\nThis represents the potential salary range for this position depending on education level, years of experience and/or certifications in addition to other position specific requirements which may impact salary\n\n\nWe‚Äôre seeking an experienced Administrator to join our Code Management team. The right candidate for this role will lead and execute strategic migrations, optimize CI/CD workflows, and drive infrastructure modernization. They will be critical in moving our automation ecosystem from legacy tools (Jenkins, Bitbucket, Automic) to GitLab, Ansible Automation Platform and Terraform, ensuring robust, scalable, and secure pipelines.\n\nRequired Skills And Experience\n\n\n8+ years of experience in Administering different & complex applications and tools used in the Enterprise\nExperience administering GitLab, Artifactory, Xray, & SonarQube\nExperience with infrastructure-as-code tools (Terraform, Ansible, etc.)\nSolid understanding of containerization (Docker) and orchestration (Kubernetes)\nFamiliarity with cloud platforms (AWS, Azure, IBM Cloud) and cloud-native tooling\nStrong communication skills and a track record of cross-team collaboration\nKnowledge of JFrog Artifactory, BitBucket / GIT, SVN and other SCM tools\nWorking knowledge of different Software Development Lifecycle Methodologies\nKnowledge of desired state configuration, automated deployment, continuous integration, and release engineering tools like Puppet, Chef, Jenkins, Bamboo, Maven, Ant etc\nConfigure and manage GitLab Runners, Groups, Projects, and Permissions at scale\nHarden GitLab for enterprise usage (SAML/SSO, LDAP, RBAC, backup/restore)\nDesign, implement, and optimize complex GitLab CI/CD pipelines using YAML best practices\nLeverage Terraform, Ansible, or similar to provision and manage self-hosted GitLab and runners\nImplement GitOps practices to manage infrastructure and environment configurations\nAutomate operational tasks and incident remediation via pipelines and scripts\nPartner with application teams to onboard them onto GitLab workflows and best practices\nDevelop and maintain clear runbooks, wiki pages, and pipeline templates\nIntegrate monitoring (Prometheus/Grafana, ELK) for GitLab health and pipeline performance\nImplement policies and guardrails to ensure code quality, compliance, and security posture\nTroubleshoot and resolve CI/CD or migration-related incidents in a timely manner\nAvailable for 24/7 On-call support\n\n\nPreferred\n\n\nA BS in Computer Science or equivalent work experience with good scripting/programming skills\nGitLab Certified Administrator\nPrior software experience with build management, configuration management and/or quality testing\nExperience with SCM practices including Agile, continuous integration (CI) and continuous deployment (CD)\n\n\nTeam Culture\n\nOur team is fast paced, fun, highly energetic, motivated and hardworking. We expect our candidates to be integrated into our results-driven and solution-oriented culture from the get-go. Our team attains high-quality results on challenging projects; the belief that outcomes are linked to one's effort rather than chance and the tendency to personally set challenging yet realistic goals."
  },
  {
    "title": "DevOps Engineer",
    "company": "Arize AI",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-arize-ai-4332964631?position=23&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=MEAUmMh5h3aGjrOZluMQSg%3D%3D",
    "description": "About Arize\n\nAI is rapidly transforming the world. As generative AI reshapes industries, teams need powerful ways to monitor, troubleshoot, and optimize their AI systems. That‚Äôs where we come in. Arize AI is the leading AI & Agent Engineering observability and evaluation platform, empowering AI engineers to ship high-performing, reliable agents and applications. From first prototype to production scale, Arize AX unifies build, test, and run in a single workspace‚Äîso teams can ship faster with confidence.\n\nWe‚Äôre a Series C company backed by top-tier investors, with over $135M in funding and a rapidly growing customer base of 150+ leading enterprises and Fortune 500 companies. Customers like Booking.com, Uber, Siemens, and PepsiCo leverage Arize to deliver AI that works.\n\nThe Team\n\nOur On-Prem engineering team is responsible for the deployment of Arize in customer environments. In addition to working with customers in defining infrastructure requirements, the team designs and develops software and tooling that enables the management of these systems at large scale. The On-Prem team has grown to be expert in Kubernetes and cloud deployment on GCP, Azure, and AWS as well as dealing with networking and security aspects of on-premise deployments. The team is dynamic and relies on few talented individuals with a high degree of autonomy and initiative.\n\nWhat You‚Äôll Do\n\n\nWork hands-on with the infrastructure that supports our distributed & highly scalable services in both SaaS and on-prem offerings\nGather requirements from customers and adapt manifests and software to support new environments\nUse and augment monitoring tools to observe platform health, ensure performance and reliability\nInteract with the product team to test new features and package new on-prem releases\nAutomate and optimize the release pipeline to make it as frictionless as possible\nExhibit continuous curiosity for emerging technology that could solve our challenges\n\n\nWhat will set you apart:\n\n\n3+ years of experience as a DevOps Engineer, Cloud Engineer, Infrastructure Engineer or similar\nExcellent communication skills and ability to work directly with customers to understand and address their infrastructure needs\nExperience and fluency in Kubernetes\nA self starter with an ability to thrived in a fast paced environment\nExperience working with multiple cloud providers (AWS, GCP, Azure) and understanding how to adapt cloud-native architectures for on-premises environments\nStrong troubleshooting skills\n\n\nThe estimated annual salary for this role is between $100,000 - $185,000, plus a competitive equity package. Actual compensation is determined based upon a variety of job related factors that may include: transferable work experience, skill sets, and qualifications. Total compensation also includes a comprehensive benefit package, including: medical, dental, vision, 401(k) plan, unlimited paid time off, generous parental leave plan, and others for mental and wellness support.\n\nWhile we are a remote-first company, we have opened offices in New York City and the San Francisco Bay Area, as an option for those in those cities who wish to work in-person. For all other employees, there is a WFH monthly stipend to pay for co-working spaces.\n\nMore About Arize\n\nArize‚Äôs mission is to make the world‚Äôs AI work‚Äîand work for people.\n\nOur founders came together through a shared frustration: while investments in AI are growing rapidly across every industry, organizations face a critical challenge‚Äîunderstanding whether AI is performing and how to improve it at scale.\n\nLearn more about what we're doing here:\n\nhttps://techcrunch.com/2025/02/20/arize-ai-hopes-it-has-first-mover-advantage-in-ai-observability/\n\nhttps://arize.com/blog/arize-ai-raises-70m-series-c-to-build-the-gold-standard-for-ai-evaluation-observability/\n\nDiversity & Inclusion @ Arize\n\nOur company's mission is to make AI work and make AI work for the people, we hope to make an impact in bias industry-wide and that's a big motivator for people who work here. We actively hope that individuals contribute to a good culture\n\n\nRegularly have chats with industry experts, researchers, and ethicists across the ecosystem to advance the use of responsible AI\nCulturally conscious events such as LGBTQ trivia during pride month\nWe have an active Lady Arizers subgroup"
  },
  {
    "title": "DevOps Engineer",
    "company": "Sustainment",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-sustainment-4335637240?position=24&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=CBylbeFN0DaQFSQqw1fcvw%3D%3D",
    "description": "Company Overview: Sustainment is an AI-native software platform that helps US-based manufacturers easily find and work with the critical suppliers they need to build and manage their supply chains. Our vision is to reimagine American manufacturing as a hyperconnected, secure, and resilient ecosystem of local and regional suppliers who can more easily connect, interact, and do business with the industry and government customers that rely on them. We are a dual-use technology platform that supports both DoD and commercial customers in pursuit of our vision.\n\nJob Overview: We are looking for a DevOps/MLOps Engineer to drive the reliability, scalability, and performance of our AI-native procurement platform. The primary focus of the role is to build and maintain robust infrastructure, automate ML model deployment pipelines, and ensure database performance and reliability. You will be responsible for high-quality, secure deliverables that meet stringent compliance requirements (SOC 2, FedRAMP, CMMC Level 2) and for helping to create, evangelize, and enforce the standards necessary to meet team and company goals for operational excellence and mission-critical uptime.\n\nResponsibilities:\n\n\nBuild partnerships and work collaboratively with engineering, AI, and product teams to meet shared objectives\nOperate effectively in ambiguous situations, especially when scaling AI workloads and managing complex infrastructure transitions\nBuild and optimize DevOps pipelines including ML model training, versioning, deployment, monitoring, and retraining workflows\nAdminister and optimize PostgreSQL databases including performance tuning, query optimization, backup/recovery, and high availability configurations\nTroubleshoot and resolve infrastructure, database, and pipeline issues in a resilient, performant manner\nImplement and maintain infrastructure as code using tools like Terraform or Cloudformation\nMonitor system health, performance, and database metrics using observability tools and respond to alerts proactively\nEnsure security best practices and compliance requirements are met across all infrastructure and database layers\nParticipate in multi-resource projects in an agile environment\nEvaluate and recommend industry standards, tools, and methods for DevOps, MLOps, and database management\nDocument infrastructure architecture, runbooks, and contribute to architecture reviews\n\n\nQualifications:\n\n\nBachelor's degree (computer science, engineering, or related) or equivalent work experience\n2+ years of experience with cloud infrastructure (AWS preferred), container orchestration (Kubernetes), and CI/CD tools\n2+ years of database administration experience with PostgreSQL or similar relational databases\nExperience with ML model deployment, monitoring, and lifecycle management (MLOps)\nStrong understanding of infrastructure as code (Terraform), GitOps practices, and declarative configuration management\nExperience with security compliance frameworks (SOC 2, FedRAMP, or CMMC is a plus)\nProduct-driven mindset with deep empathy for internal developer experience and system reliability\nStrong desire to work in a startup with interest to take on projects from zero to one with collaboration with the rest of the team\nLove working hard and enjoy a fast-paced, ambiguous environment\nExperience with distributed systems, microservices architecture, and reactive systems\nOpen mindset to exploring new tools and frameworks in the rapidly evolving DevOps/MLOps landscape\nPassion for operational excellence and automation\nExperience supporting cross-team efforts to roll out new infrastructure capabilities or ML features\nPassion for learning and continuous improvement\nStrong written and verbal communication skills, and ability to explain complex technical concepts\nExperience working in a Scrum/agile environment\nExperience with AWS GovCloud, defense/government sector compliance, or working in an early startup environment on SaaS products is a plus\n\n\nCore Technologies:\n\n\nAWS (including GovCloud), Kubernetes, Docker, Terraform\nPostgreSQL\nGitLab CI/CD, ArgoCD, Tilt\nModel versioning, experiment tracking, ML pipeline orchestration\nDatadog, CloudWatch\nPython, Bash, experience with .NET ecosystem a plus\nIAM, secrets management, encryption, audit logging, compliance automation\n\n\nSustainment offers a competitive benefits package for full time employees including medical, dental, vision, paid time off, company holidays, and 401K matching.\n\nSustainment is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.\n\nApplicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.\n\nSustainment participates in E-Verify."
  },
  {
    "title": "Devops",
    "company": "The Dignify Solutions, LLC",
    "location": "Phoenix, AZ",
    "link": "https://www.linkedin.com/jobs/view/devops-at-the-dignify-solutions-llc-4347025595?position=25&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=jM%2FGCdo9AbcNJ%2F2rOHVVbg%3D%3D",
    "description": "Must have is\n\n\nAssociate should be in Phoenix from day 1 of the project\nAt least 5 years of experience in Devops area.\nStrong skill in CI/CD pipeline, Jenkins, Github\nAdditional knowledge on any build related tools is an added advantage.\n\n\nJava 8 knowledge\n\nDocker\n\nKaffka\n\nKibana"
  },
  {
    "title": "DevOps Engineer I",
    "company": "Trustwell",
    "location": "Portland, OR",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-i-at-trustwell-4321600458?position=26&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=D%2B0L3krDtsL0ibglUbtdxw%3D%3D",
    "description": "Role{{:}} DevOps Engineer I\n\nFLSA{{:}} Full Time | Exempt | Salaried | Remote\n\nReports to{{:}} Director of DevOps\n\nNote{{:}} Candidate preferred to reside in PST. If not, candidate will be required to support PST working hours.\n\nTrustwell is looking for ambitious, energetic problem-solvers who enjoy a fast-paced team environment filled with challenges and career growth opportunities in a rapidly growing tech firm. Trustwell is on a mission to change the food industry. Combining FoodLogiQ's supply chain management software with Genesis' nutritional analysis and label development solution, the Trustwell Connect platform creates the food industry's only full-scale solution connecting product development and regulatory-compliant labeling with supplier compliance, enhanced traceability, and automated recall management. From food and supplement manufacturers to retail grocers and restaurant chains, more than 2,500 food companies around the world use Trustwell software as their trusted source for compliance and quality solutions in the food industry. For more information, visit www.trustwell.com.\n\nScope of Position{{:}} The DevOps Engineer I will be part of a dynamic and agile team responsible for building and maintaining the platforms, systems, and services that power our customer-facing products. Working closely with Software Engineers and Engineering Leadership, you'll help shape architecture, implement best practices, and stay ahead of the curve in DevOps principles. You'll champion operational excellence and continuous improvement across the team.\n\nEssential Duties & Responsibilities include but not limited to{{:}}\n\n\nContribute actively to an Agile delivery team, ensuring consistent, high-quality, and reliable software releases.\nCollaborate closely with developers and cross-functional partners to design, build, and deploy best-in-class, scalable software solutions.\nChampion an automation-first, code-centric mindset, driving efficiency and consistency across deployment, monitoring, and maintenance processes.\nSupport production operations through participation in incident response, troubleshooting, and on-call rotations to maintain system reliability and uptime.\nDevelop, implement, and maintain monitoring and alerting tools to ensure optimal application performance, health, and availability.\nDesign infrastructure and deployment solutions with scalability, resilience, and long-term maintainability as core principles‚Äîavoiding short-term workarounds.\nProactively identify and eliminate operational bottlenecks and unnecessary complexity, contributing to continuous improvement initiatives.\nEngage in architectural reviews and solution design discussions, providing input that enhances performance, reliability, and security.\nPerform other related duties as assigned, contributing to the overall success of the DevOps function and technology organization.\n\n\nEducation/Experience{{:}}\n\n\nBachelor's degree in Computer Science, Engineering, or a related field required. Will consider relevant experience/certifications in lieu of degree.\n2+ years of experience in an SRE (Site Reliability Engineer) or equivalent engineering role\n2+ years of experience as a DevOps Engineer or in a similar capacity\n3+ years of hands-on experience managing and supporting production cloud environments (AWS, Azure, or GCP)\nExtensive experience with DataDog, including APM, RUM, Synthetic Monitoring, Infrastructure Monitoring, and Dashboard development\n\n\nRequired Skills/Abilities{{:}}\n\n\nStrong, hands-on experience with Infrastructure-as-Code (IaC) and configuration management tools such as Terraform, CloudFormation, and/or Ansible\nProven experience designing and managing cloud architectures in AWS and Microsoft Azure, with expertise in containerization and orchestration (Docker, Kubernetes, etc.)\nDemonstrated experience building, maintaining, and optimizing CI/CD pipelines using tools such as CircleCI, TeamCity, GitHub Actions, or Jenkins\nBackground in delivering infrastructure initiatives within an Agile development environment\nCollaborative mindset with the ability to partner effectively across cross-functional teams to achieve shared goals\nStrong \"automation-first\" mindset with a focus on scalability, reliability, and efficiency\n\n\nTotal Rewards Package{{:}}\n\n\nFull healthcare benefits, including medical, dental, and vision.\nSupplemental benefits, including STD, LTD, HSA, 401k, etc.\nResponsible Time Off (PTO) + Holiday Pay\nExcellent culture, growth opportunities, plus much more...\n\n\nWhat to expect - the Hiring Process!\n\n\nInterview with Human Resources\nInterview with Hiring Manager\nPeer Panel Interview(s)\nOffer of Employment (Background Screening/References)\n\n\nHiring Eligibility{{:}} This is a fully remote position open to candidates located anywhere within the United States. Eligibility to work remotely is subject to company policy and applicable state laws. Candidates must have work authorization to work for any U.S. based employer. Please note that certain benefits, taxes, or employment terms may vary by state.\n\nCompensation{{:}} The compensation for this position starts at $80,000 per annum, with the potential for higher placement based on a candidate's experience, education, and overall qualifications. In addition to base salary, this role is bonus eligible‚Äîup to 10% annually, contingent on company performance and achievement of organizational objectives.\n\nTrustwell is an equal employment opportunity employer committed to hiring and retaining a diverse workforce. Applicants receive fair and impartial consideration without regard to race, sex, sexual orientation, gender identity, color, religion, national origin, age, disability, veteran status, religion, or other legally protected class. If you need accommodation for any part of the employment process due to a medical condition, or any disability, please contact a member of our human resources team.\n\nAcceptable Background and References Required; Upon any conditional offers made by Trustwell.\n\nEqual Opportunity Employer/ DFWP/ Affirmative Action"
  },
  {
    "title": "DevOps Engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4338475165?position=27&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=4JmDyUsuEYdJArzBGOVeTA%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Remote\n\nEmployment Type: Full-Time\n\nJob Summary\n\nWe are looking for a skilled DevOps Engineer to join our technology team. The ideal candidate will design, implement, and manage CI/CD pipelines, automate infrastructure, and ensure smooth deployment processes across development and production environments. This role requires strong knowledge of cloud platforms, containerization, and scripting.\n\nKey Responsibilities\n\n\nDesign, build, and maintain CI/CD pipelines for application deployment.\nAutomate infrastructure provisioning using tools like Terraform or Ansible.\nManage containerized environments using Docker and Kubernetes.\nMonitor system performance and implement proactive solutions for scalability and reliability.\nCollaborate with development and operations teams to streamline workflows.\nEnsure security and compliance in cloud and on-prem environments.\nTroubleshoot and resolve issues in production and staging environments.\n\n\nQualifications\n\n\nBachelor's degree in Computer Science, Engineering, or related field.\n25 years of experience in DevOps or related roles.\nProficiency in cloud platforms (AWS, Azure, GCP).\nHands-on experience with CI/CD tools (Jenkins, GitLab CI, GitHub Actions).\nStrong knowledge of containerization (Docker, Kubernetes).\nFamiliarity with Infrastructure as Code (Terraform, Ansible).\nScripting skills in Python, Bash, or similar languages.\n\n\nPreferred Skills\n\n\nExperience with monitoring tools (Prometheus, Grafana).\nKnowledge of security best practices in DevOps.\nFamiliarity with microservices architecture.\n\n\nBenefits\n\n\nCompetitive salary and performance bonuses.\nHealth insurance and retirement plans.\nFlexible work options and professional development opportunities."
  },
  {
    "title": "DevOps Engineer (JIRA)",
    "company": "Rubix Solutions",
    "location": "Washington, DC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-jira-at-rubix-solutions-4335995983?position=28&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=Hty9ucbFu%2BW2%2BArnksc9EQ%3D%3D",
    "description": "The DevOps Engineer (specifically Jira Platform Engineer) will serve as the technical owner of our Jira SaaS in government cloud, and is responsible for administering, configuring, and integrating the application within our enterprise technology ecosystem. This position plays a pivotal role in consolidating our collaborative planning tools‚Äîtransitioning from Azure DevOps (on-prem) and GitLab (on-prem) to Jira‚Äîwhile enabling and scaling Agile practices across the organization.\n\nThe ideal candidate is both technically proficient and strategically minded, with a strong understanding of Agile methodologies, DevSecOps workflows, and enterprise system\n\nintegration.\n\nResponsibilities\n\n\nAdminister, configure, and optimize Jira SaaS to meet enterprise project management and Agile delivery needs.\nDesign and maintain custom workflows, issue types, screens, fields, and automation rules aligned with organizational Agile frameworks and guidelines.\nManage user permissions, group roles, and security schemes to ensure governance and compliance.\nMonitor Jira license utilization, user growth, and application usage to ensure efficient use of subscriptions.\nCollaborate with procurement teams to support renewal, optimization, and budget decisions.\nDesign, implement, and maintain seamless integrations between Jira with other enterprise systems, such as GitLab (source control & CI/CD), and ServiceNow (ITSM), using Okta,REST APIs, webhooks, middleware, and scripting.\nAutomate data synchronization across platforms to support traceability from planning to release.\nTroubleshoot and optimize integration pipelines to ensure performance, security, and Scalability.\nPartner with infrastructure and cybersecurity teams to align integrations with enterprise security and compliance standards.\nDevelop and maintain technical documentation, standards, and best practices.\nPartner with the Agile Transformation Office to translate Agile practices into effective Jira configurations and usage patterns.\nProvide technical guidance and mentoring to Scrum Masters, Product Owners, and teams on best-practice tool utilization.\nSupport reporting and analytics initiatives, ensuring reliable Agile metrics (velocity, burndown, cycle time, etc.).\n\n\nRequirements\n\n\nMust be able to obtain and maintain Moderate Risk Public Trust (MRPT) facility credentials/authorization. Note: US Citizenship is required for MRPT facility credentials/authorization at this work location.\nBachelor‚Äôs degree in Computer Science, Information Systems, or a related field (or equivalent experience).\n3+ years of hands-on experience administering and/or engineering Jira (Self-hosting or SaaS).\nProven, hands-on experience developing custom integrations with enterprise platforms, especially GitLab and ServiceNow.\nProficiency in scripting (Python, PowerShell, or JavaScript) and REST API integration.\nStrong understanding of Agile methodologies (Scrum, Kanban, SAFe) and DevSecOps principles.\nExperience in enterprise migrations from Azure DevOps or similar tools to Jira.\nFamiliarity with Atlassian ecosystem (Confluence, Bitbucket) and marketplace apps.\nExperience working in a DevSecOps or Platform Engineering environment.\nExperience with information security, privacy, and risk assessment standards including FISMA, SOX, FedRAMP, etc. is preferred.\nFederal government experience is preferred.\nAtlassian Certified Professional (ACP-620, ACP-120, or equivalent) preferred."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4291046434?position=29&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=my010xvqUkDpG30wys1Mow%3D%3D",
    "description": "About Chartmetric\n\nChartmetric, Inc. is a 10-year-old startup specializing in music data analytics. We are trusted by Universal MusicGroup, Sony, Warner, and Apple Music, as well as hundreds of other music companies and industry professionals. Our team has created a self-service data dashboard for the music industry to better understand the activity happening around artists. Together, we combine hundreds of thousands of real-time data points across iTunes, Spotify, YouTube, Google, Amazon, X, and others through our beautifully designed tool in order to make sense of the increasingly complex landscape of the music industry.\n\nAbout The Role\n\n\nWe are seeking a talented DevOps / Developer Experience Engineer to join our team and play a pivotal role in enhancing our development infrastructure and streamlining the developer workflow. This position combines traditional DevOps responsibilities with a focus on creating exceptional developer experiences through tooling, automation, and process optimization.\n\n\nWhat You'll Do\n\n\nInfrastructure & Operations\nDesign, implement, and maintain scalable cloud infrastructure using Infrastructure as Code (IaC) principles\nManage CI/CD pipelines and deployment processes across multiple environments\nMonitor system performance, reliability, and security, implementing proactive solutions\nAutomate operational tasks and eliminate manual toil through scripting and tooling\nEnsure high availability and disaster recovery capabilities\n\n\nDeveloper Experience\nBuild and maintain internal developer tools and platforms that improve productivity\nStreamline onboarding processes for new developers and reduce time-to-first-commit\nDesign and implement developer-friendly APIs, SDKs, and documentation\nCreate self-service capabilities that reduce dependencies and waiting times\nGather feedback from development teams and iterate on tooling based on pain points\n\n\nCollaboration & Process Improvement\nWork closely with engineering teams to understand workflow challenges and requirements\nChampion best practices for code deployment, testing, and monitoring\nLead initiatives to improve development velocity and reduce friction\nParticipate in incident response and post-mortem analysis\nMentor team members on DevOps practices and tooling\n\nWhat We're Looking For\n\n\nTechnical Skills\n3+ years of experience in DevOps, SRE, or Platform Engineering roles\nStrong proficiency with cloud platforms (AWS, GCP, or Azure)\nExperience with Infrastructure as Code tools (Terraform, CloudFormation, or Pulumi)\nHands-on experience with containerization (Docker) and orchestration (Kubernetes)\nProficiency in CI/CD tools (Jenkins, GitLab CI, GitHub Actions, or similar)\nStrong scripting skills in Python, Bash, or Go\nExperience with monitoring and observability tools (Prometheus, Grafana, ELK stack, or similar)\n\n\nDeveloper Experience Focus\nExperience building internal tools and platforms for development teams\nUnderstanding of software development lifecycle and common developer pain points\nFamiliarity with API design and developer-facing documentation\nExperience with version control systems and Git workflows\nKnowledge of testing frameworks and quality assurance processes\n\n\nSoft Skills\nStrong problem-solving abilities and analytical thinking\nExcellent communication skills and ability to work with cross-functional teams\nCustomer-focused mindset with emphasis on developer productivity\nProactive approach to identifying and resolving issues\nAbility to balance technical debt with feature delivery\n\n\nPreferred Qualifications\nKnowledge of security best practices and compliance frameworks\nBackground in software development or engineering\nFamiliarity with cost optimization strategies for cloud infrastructure\nPrevious experience in a high-growth or scaling environment\n\nWhat We Offer\n\n\nCompetitive salary and equity package\nComprehensive health, dental, and vision insurance\nOpportunity to shape developer experience across the organization\nAccess to cutting-edge tools and technologies\n\n\nTeam Culture\n\n\nWe believe that great developer experiences lead to better products and happier teams. Our DevOps/DX team operates as enablers and force multipliers, working collaboratively to remove friction from the development process. We value automation, measurement, and continuous improvement, always asking \"how can we make this better for our developers?\"\n\n\nThe Pay Range For This Role Is\n\n135,000 - 165,000 USD per year(San Mateo)\n\n120,000 - 150,000 USD per year(New York)"
  },
  {
    "title": "DevOps Engineer",
    "company": "Broad Reach Partners",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-broad-reach-partners-4303987210?position=30&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=uTp8%2B9FtKMx0r8DAwjC6AQ%3D%3D",
    "description": "We are seeking a Senior DevOps Engineer to join our team and play a critical role in designing, building, and optimizing the CI/CD pipelines that power our software delivery across on-prem and cloud environments.\n\nIn this role, you will work hand-in-hand with our development, operations, and security teams worldwide to implement best practices, automate deployments, and ensure our platforms are reliable, secure, and scalable. If you thrive on solving complex technical challenges, have a passion for automation, and want to influence how enterprise platforms evolve and modernize, this is an ideal opportunity for you.\n\nAs a Senior DevOps Engineer, your expertise will drive the continuous integration, delivery, and deployment (CI/CD) pipelines delivering software to both on-prem and cloud (AWS primarily) environments. You will work closely with our development, operations, and security teams distributed across the globe. This role requires a deep understanding of DevSecOps best practices and a strong ability to troubleshoot complex issues.\n\nYour Responsibilities In This Role Will Include\n\n\nDesign, Develop and Maintain automated build and deployment pipelines using GitLab/GitHub/Jenkins to enhance software delivery.\nIdentify opportunities for automation and ensure continuous security, quality in application development by automating security checks, test executions in build and deployment pipelines.\nDeploy and manage Kubernetes workloads to AWS EKS(A) using Helm, ArgoCD\nCollaborate with development, operations and security team to build secure, optimized and efficient pipelines.\nCreate comprehensive documentation on pipeline functionality and provide training to required members.\nProactively monitor system performance and identify potential issues before they become critical.\nParticipate in on-call rotation.\nEngage in continuous learning and actively advocate for Dev(Sec)Ops, GitOps best practices and standards across the team.\n\n\nWe are looking for you to have the following skills and experience:\n\n\n8+ years of experience as a DevOps Engineer, Site Reliability Engineer, or equivalent\nStrong knowledge of DevOps practices, continuous integration, continuous delivery, and related tools.\n3+ years of experience with Amazon Web Services (AWS) or Microsoft Azure\n3+ years of experience with Kubernetes clusters\nProficiency with public cloud environments (AWS preferred)\nExperience with tools like New Relic and Graylog\nAdvanced proficiency working with CI/CD pipelines such as GitHub Actions/GitLab/Jenkins\nExpert in containerization technologies such as Docker and orchestration tools like Kubernetes.\nProficiency in scripting language, like Bash, Groovy, Python\nExcellent debugging and troubleshooting skills.\nAbility to prioritize tasks efficiently and independently under minimal supervision.\n\n\nNice to Have\n\n\nAWS Cloud certification\nFamiliar with .NET applications.\nKnowledge in Terraform, Ansible, monitoring tools\n\n\nWe are located in the Alpharetta/Cumming area of Atlanta and are working in the office several days each week so YOU MUST LIVE WITHIN COMMUTING DISTANCE OF ALPHARETTA, GA to be considered for this role. We cannot sponsor at this time.\n\nIf this opportunity is a good match for your skills, experience and interest, please apply now so we can follow up with you with more details."
  },
  {
    "title": "AWS DevOps Specialist",
    "company": "Focus School Software",
    "location": "St. Petersburg, FL",
    "link": "https://www.linkedin.com/jobs/view/aws-devops-specialist-at-focus-school-software-4333597594?position=31&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=8d1zYP%2FdPBLOcWlVWgcpKQ%3D%3D",
    "description": "Focus School Software is a fast-growing school management software company. We thrive on creating some of the most innovative features on the market today, helping educators to meet their evolving needs in classrooms, district management, state reporting compliance, and other facets of student-centered education and technology.\n\nWe are seeking an experienced and proactive AWS DevOps Specialist to join our growing infrastructure team. This role is ideal for someone passionate about automation, cloud infrastructure, and scalable, secure systems. The ideal candidate brings expertise in AWS services, infrastructure as code, cost reduction strategies and DevOps best practices. You will play a key role in improving system performance, reliability, and security, while contributing to CI/CD pipelines and participating in an on-call rotation. This is a great opportunity for anyone who has a multitude of skills in DevOps and System Administration and can wear many hats and loves problem solving.\n\nKey Responsibilities\n\n\nAutomation & Configuration Management\nDesign, develop, and maintain automation using Ansible and Ansible Tower.\nDeploy and configure RHEL based systems.\nDatabase maintenance and performance, routing, pooling and role management.\n\n\nCloud Infrastructure (AWS)\n\n\nArchitect and manage services including RDS (PostgreSQL), EC2, EKS, CloudFormation, CloudFront, GuardDuty, AWS VPN, AWS AD, and more.\nImplement Autoscaling strategies and container orchestration with EC2 Autoscaling or EKS. Continuously monitor performance and feedback on systems.\nMonitor and improve database performance, sharding strategies, and health metrics.\nMonitor backups and maintain recovery point objectives for disaster recovery.\n\n\nSecurity & Compliance\n\n\nSupport SOC II compliance initiatives through infrastructure hardening, monitoring, and alerting.\nLeverage AWS security tools and best practices to ensure compliance and threat mitigation.\n\n\nNetworking & Connectivity\n\n\nManage VPCs, subnets, security groups, VPNs, and endpoint connectivity for both internal and external integrations.\nManaging routes, DNS and VPN connectivity. Help internal users maintain their VPN connections.\nCost Optimization\nAnalyze AWS billing, usage reports, and recommend cost-saving strategies.\n\n\nDevOps & CI/CD\n\n\nBuild and maintain CI/CD pipelines, enabling delivery of automation code from development to production.\nEnsure high availability and zero-downtime deployments through automation and best practices.\nDevelop and maintain local dev environments for developers.\n\n\nCollaboration & Culture\n\n\nWork cooperatively in cross-functional teams, embracing a culture where the best ideas win.\nProactively identify infrastructure problems and lead with creative, scalable solutions.\n\n\nEndpoint & Systems Management\n\n\nOversee and manage end-user systems and infrastructure endpoints to maintain security and stability.\nPatch management and remediation, ensure established timelines and policies are followed.\n\n\nOn-Call Participation\n\n\nParticipate in an on-call rotation to respond to production incidents and infrastructure issues.\n\n\nRequirements\n\n\nAnsible, Ansible Tower, working in RHEL based environments.\nLinux/RHEL expert, be able to design, deploy and fix everything from a systemd service to managing SFTP.\nAbility to manage a Git repository, and perform peer review on automation code.\nMonitoring tools such as Splunk, Grafana or similar.\nWorking knowledge of NGINX, basic webserver stacks.\nUnderstanding of AWS, particularly RDS (PostgreSQL), CloudFormation, EC2, EKS\nKubernetes and containerization\nExperience with database sharding and performance tuning\nCI/CD pipeline design and implementation\nFamiliarity with SOC II compliance frameworks\nExperience managing AWS networking, VPNs, and AWS AD\nSecurity-first mindset with experience using AWS Org, AWS Tower, CloudFront, and related tools to ensure compliance.\nStrong interpersonal skills with a collaborative mindset.\n\n\nNice-to-Have\n\n\nLAMP/LNPP Stack experience\nSVN Familiarity\nActive Directory experience, basic Windows management\nExperience with endpoint management platforms\nBackground in proactive monitoring/observability tooling\nPrior involvement in security audits or compliance initiatives\n\n\nFocus School Software‚Äôs compensation package offers the following benefits:\n\n\nMedical Insurance\nDental/Vision Insurance\nLife Insurance\nShort and Long Term Disability Insurance\n401(k) after 6 months\nPaid Holidays\nPaid Vacation and Sick Time\nRemote Position"
  },
  {
    "title": "DevOps Engineer",
    "company": "Rain",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rain-4318510257?position=32&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=i3yFe70EGV3wFWMnYunqEQ%3D%3D",
    "description": "Rain is empowering the next generation of money and financial products globally. We‚Äôre a lean and mighty team of passionate builders and veteran founders. We are looking for a DevOps engineer to join us in building a cutting edge platform at the intersection of real-world payments and digital money. You will have the opportunity to deliver massive impact at a small and quickly growing company that is funded by some of the top investors in fintech and crypto. Rain is backed by great investors including Lightspeed, Norwest, Khosla, along with great companies like Coinbase, Circle, and Uniswap.\n\nMany of our engineers are based in NYC but we are open to fully remote candidates.\n\nOur Ethos\n\nWe believe in an open and flat structure. You will be able to grow into the role that most aligns with your goals. Our team members at all levels have the freedom to explore ideas and impact the roadmap and vision of our company.\n\nWhat You'll Do\n\n\nBe a critical part of the technical infrastructure roadmap\nManage our cloud environments across GCP and AWS\nScale our infrastructure to millions of end users globally\nHelp drive the architectural decisions of a rapidly evolving product\nLead the creation and maintenance of our CI/CD pipelines to enable rapid, reliable deployments\nCollaborate with the engineering team to improve infrastructure performance\nBuild infrastructure to interact with millions of smart contracts across dozens of blockchains\nAutomate security controls and compliance processes to protect sensitive financial data\n\n\nWhat We're Looking For\n\n\nStrong experience with Infrastructure as Code, particularly Terraform, for managing cloud resources at scale\nProven track record designing and implementing CI/CD pipelines and automation workflows\nExperience managing production environments in cloud providers\nExperience with monitoring, logging, and observability tools\n\n\nNice to haves, but not mandatory\n\n\nExperience in fintech (neobank or card issuing experience gets extra brownie points)\nExperience with blockchain infrastructure\n\n\nOur perks enable working at Rain to be a fulfilling, healthy and happy experience.\n\nUnlimited time off üõº Unlimited vacation can be daunting, so at Rain we require our teammates to take 10 days minimum for themselves.\n\nFlexible working ‚òï We support a flexible workplace, if you feel comfortable at home please work from home. If you‚Äôd like to work with others in an office feel free to come in. We want everyone to be able to work in the environment in which they are their most confident and productive selves.\n\nFlexible Benefits üß† Easy-to-access benefits, for all employees based in the US, Rain pays a percentage of your benefits for the employee and for your dependents. We offer comprehensive health, dental and vision plans as well as a 100% company-subsidized life insurance plan.\n\nEquity plan üì¶ On top of a competitive salary, we offer every Rain employee an equity option plan so we can all can benefit from our success.\n\nRain Cards üåßÔ∏è We want our teammates to be knowledgeable about our core products and services and to support this mission we issue a card for our team to utilize the card for testing.\n\nHealth and Wellness üìö High performance begins from within. Our members are welcome to use their company card for eligible health and wellness spending like gym memberships, fitness classes and other wellness items.\n\nTeam summits ‚ú® Summits play an important role at Rain! Time spent together helps us get to know each other, strengthen our relationships, and build a common destiny. Stay tuned for upcoming destinations!"
  },
  {
    "title": "DevOps Engineer",
    "company": "Jasper",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-jasper-4318500931?position=33&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=LEBaDqUoCvC6FxbgZXNAqA%3D%3D",
    "description": "Jasper is the leading AI marketing platform, enabling the world's most innovative companies to reimagine their end-to-end marketing workflows and drive higher ROI through increased brand consistency, efficiency, and personalization at scale.\n\nJasper has been recognized as \"one of the Top 15 Most Innovative AI Companies of 2024\" by Fast Company and is trusted by nearly 20% of the Fortune 500 ‚Äì including Prudential, Ulta Beauty, and Wayfair. Founded in 2021, Jasper is a remote-first organization with team members across the US, France, and Australia.\n\nAbout The Role\n\nWe're looking for an experienced DevOps Engineer to join our Platform team. This is a highly autonomous, high-impact role that blends Ops practices, infrastructure engineering, and delivery pipeline optimization. You'll work with a focused, collaborative, and fast-moving team where your contributions will directly impact system reliability, developer velocity, and our ability to safely deliver AI-powered products at scale. Candidates should also have a solid background in Cloud, IaC, and Kubernetes, and a drive to produce excellent solutions for a variety of challenges.\n\nThis fully remote role reports to the Staff Dev Ops Engineer and is open to candidates located anywhere in the continental US.\n\nWhat You‚Äôll Do\n\n\nDesign, implement, and operate cloud-native infrastructure that scales efficiently, fails gracefully, and optimizes for performance and cost.\nBuild and refine software delivery pipelines to enable safe, fast, and frequent deployments with robust testing, rollback, and progressive release mechanisms.\nDevelop infrastructure-as-code solutions using Terraform and Helm to create self-healing, automated, and observable systems.\nCollaborate with ML and product teams to support AI model training and inference through scalable compute and storage infrastructure.\nIdentify and eliminate single points of failure, performance bottlenecks, and scalability limits through proactive monitoring and reliability engineering practices.\nImplement and enforce security best practices, including secrets management, access control, and compliance across all infrastructure layers.\n\n\nWhat You‚Äôll Bring\n\n\nDeep experience running Kubernetes in production (cluster management, networking, storage, security).\nExpertise with Terraform, Helm, and configuration management to build reproducible, version-controlled infrastructure.\nProven success designing and maintaining CI/CD pipelines (GitHub Actions, Argo CD, Jenkins, etc.) balancing speed and safety.\nStrong background in observability (especially Datadog) ‚Äî skilled at instrumentation, dashboard creation, and intelligent alerting.\nSolid scripting skills in Python, Go, or Bash, with a focus on automation and operational efficiency.\nPractical knowledge of Google Cloud Platform and cloud-native architectures.\nExperience supporting multi-language environments (TypeScript, Python, Go) and AI/ML workloads, including GPU-based compute.\nFamiliarity with container security, secrets management, and policy enforcement.\n(Bonus) History of open source contributions in infrastructure, CI/CD, or observability projects.\n\n\nCompensation Range\n\nAt Jasper, we believe in pay transparency and are committed to providing our employees and candidates with access to information about our compensation practices. The expected base salary range offered for this role is $170,000 - $200,000. Compensation may vary based on relevant experience, skills, competencies, and certifications.\n\nBenefits & Perks\n\n\nComprehensive Health, Dental, and Vision coverage beginning on the first day for employees and their families\n401(k) program with up to 2% company matching\nEquity grant participation\nFlexible PTO with a FlexExperience budget ($900 annually) to help you make the most of your time away from work\nFlexWellness program ($1,800 annually) to help support your personal health goals\nGenerous budget for home office set up\n$1,500 annual learning and development stipend\n16 weeks of paid parental leave\n\n\nOur goal is to be a diverse workforce that is representative at all job levels as we know the more inclusive we are, the better our product will be. We are committed to celebrating and supporting our differences and that diversity is essential to innovation and makes us better able to serve our customers. We hire people of all levels and backgrounds who are excited to learn and develop their skills.\n\nWe are an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state or federal laws.\n\nBy submitting this application, you acknowledge that you have reviewed and agree to Jasper's CCPA Notice to Candidates, available at legal.jasper.ai/#ccpa."
  },
  {
    "title": "DevOps / Systems Engineer",
    "company": "Collate",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-collate-4302854141?position=34&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=o8Gc9eygV%2Fd5alNHHdhp0Q%3D%3D",
    "description": "About Collate\n\nCollate is an AI document generation platform for life sciences. We automate paperwork with AI, helping our customers get life-saving innovations to patients years faster. Collate is an end-to-end solution, powering every step of drug, diagnostic, and medical device development‚Äîfrom concept to market.\n\nOur CEO Surbhi Sarna is a former General Partner at Y Combinator. Surbhi founded nVision Medical, which developed a new method to detect ovarian cancer and was acquired by Boston Scientific. Our CTO Nate Smith is a former Visiting Partner at Y Combinator and founder of Lever. Our AI researchers, engineers, and designers have worked at Google, Nvidia, Meta, Netflix, Amazon, AirBnB, Hippocratic AI, and Grail, and 40% of our team are former founders.\n\nWe‚Äôre a small, elite team, with over $30M in seed funding from top investors (Redpoint, First Round Capital, Conviction, and Y Combinator) and leaders in healthcare and AI. This is a rare chance to join at the ground floor of a company with world-changing potential, experienced founders, and resources to execute at scale.\n\nAbout The Role\n\nWe‚Äôre looking for a DevOps / Systems Engineer to own the infrastructure that powers Collate‚Äôs products. You‚Äôll build the systems and tooling that keep our platform reliable, secure, and fast as we scale.\n\nThis role is broad by design ‚Äî from managing CI/CD pipelines and cloud infrastructure to handling light security responsibilities like certificate management. You‚Äôll partner closely with backend, AI, and product engineers to ensure our systems are both easy to develop on and safe to deploy at scale.\n\nAt Collate, infrastructure isn‚Äôt just about uptime ‚Äî it‚Äôs about trust. The work you do will help ensure that the AI we build for healthcare runs with reliability and security in mind.\n\nWhat You‚Äôll Do\n\n\nDesign and maintain cloud infrastructure to support Collate‚Äôs products as we grow from prototypes to production scale\nDevelop CI/CD pipelines and automation that accelerate developer velocity and reduce operational friction\nManage core system reliability, including monitoring, logging, and incident response\nTake on light security responsibilities, such as handling certificates, secrets management, and supporting compliance needs\nCollaborate closely with engineering teams to design infrastructure that balances speed, safety, and scale\nContinuously improve internal tooling and workflows, helping the team move faster with confidence\nLeverage tooling including AWS, Terraform, Kubernetes, Helm, ArgoCD, Grafana, and Github Actions\n\n\n\nWhat We‚Äôre Looking For\n\n\nHands-on experience with cloud infrastructure (AWS, GCP, or similar) and modern DevOps practices\nProficiency with infrastructure-as-code and CI/CD tooling\nFamiliarity with monitoring, observability, and incident management\nInterest or experience in light security work, including certificates, secrets management, or compliance support\nA pragmatic approach: able to balance iteration speed with building for long-term reliability\nMotivation to work in an early-stage startup where your infrastructure decisions shape the foundation of the company\n\n\n\nWhy Join Collate?\n\nImpact: Build systems and experiences that touch real patients and providers, improving healthcare outcomes.\n\nOwnership: Shape both our product experience and our engineering culture from the start.\n\nLearning: Collaborate with a uniquely interdisciplinary team‚ÄîAI researchers, healthcare leaders, and experienced startup builders.\n\nUpside: Join a company early enough to have meaningful equity and career-defining impact.\n\nThe base salary range for this role is $150,000‚Äì$300,000 USD annually, depending on experience and level (Tier 1, San Francisco)\n\nWe may use artificial intelligence (AI) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. These tools assist our recruitment team but do not replace human judgment. Final hiring decisions are ultimately made by humans. If you would like more information about how your data is processed, please contact us."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "San Mateo, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4304688090?position=35&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=NTCnXUMajmPBeVaE74vjhw%3D%3D",
    "description": "About Chartmetric\n\nChartmetric, Inc. is a 10-year-old startup specializing in music data analytics. We are trusted by Universal MusicGroup, Sony, Warner, and Apple Music, as well as hundreds of other music companies and industry professionals. Our team has created a self-service data dashboard for the music industry to better understand the activity happening around artists. Together, we combine hundreds of thousands of real-time data points across iTunes, Spotify, YouTube, Google, Amazon, X, and others through our beautifully designed tool in order to make sense of the increasingly complex landscape of the music industry.\n\nAbout The Role\n\n\nWe are seeking a talented DevOps / Developer Experience Engineer to join our team and play a pivotal role in enhancing our development infrastructure and streamlining the developer workflow. This position combines traditional DevOps responsibilities with a focus on creating exceptional developer experiences through tooling, automation, and process optimization.\n\n\nWhat You'll Do\n\n\nInfrastructure & Operations\nDesign, implement, and maintain scalable cloud infrastructure using Infrastructure as Code (IaC) principles\nManage CI/CD pipelines and deployment processes across multiple environments\nMonitor system performance, reliability, and security, implementing proactive solutions\nAutomate operational tasks and eliminate manual toil through scripting and tooling\nEnsure high availability and disaster recovery capabilities\n\n\nDeveloper Experience\nBuild and maintain internal developer tools and platforms that improve productivity\nStreamline onboarding processes for new developers and reduce time-to-first-commit\nDesign and implement developer-friendly APIs, SDKs, and documentation\nCreate self-service capabilities that reduce dependencies and waiting times\nGather feedback from development teams and iterate on tooling based on pain points\n\n\nCollaboration & Process Improvement\nWork closely with engineering teams to understand workflow challenges and requirements\nChampion best practices for code deployment, testing, and monitoring\nLead initiatives to improve development velocity and reduce friction\nParticipate in incident response and post-mortem analysis\nMentor team members on DevOps practices and tooling\n\nWhat We're Looking For\n\n\nTechnical Skills\n3+ years of experience in DevOps, SRE, or Platform Engineering roles\nStrong proficiency with cloud platforms (AWS, GCP, or Azure)\nExperience with Infrastructure as Code tools (Terraform, CloudFormation, or Pulumi)\nHands-on experience with containerization (Docker) and orchestration (Kubernetes)\nProficiency in CI/CD tools (Jenkins, GitLab CI, GitHub Actions, or similar)\nStrong scripting skills in Python, Bash, or Go\nExperience with monitoring and observability tools (Prometheus, Grafana, ELK stack, or similar)\n\n\nDeveloper Experience Focus\nExperience building internal tools and platforms for development teams\nUnderstanding of software development lifecycle and common developer pain points\nFamiliarity with API design and developer-facing documentation\nExperience with version control systems and Git workflows\nKnowledge of testing frameworks and quality assurance processes\n\n\nSoft Skills\nStrong problem-solving abilities and analytical thinking\nExcellent communication skills and ability to work with cross-functional teams\nCustomer-focused mindset with emphasis on developer productivity\nProactive approach to identifying and resolving issues\nAbility to balance technical debt with feature delivery\n\n\nPreferred Qualifications\nKnowledge of security best practices and compliance frameworks\nBackground in software development or engineering\nFamiliarity with cost optimization strategies for cloud infrastructure\nPrevious experience in a high-growth or scaling environment\n\nWhat We Offer\n\n\nCompetitive salary and equity package\nComprehensive health, dental, and vision insurance\nOpportunity to shape developer experience across the organization\nAccess to cutting-edge tools and technologies\n\n\nTeam Culture\n\n\nWe believe that great developer experiences lead to better products and happier teams. Our DevOps/DX team operates as enablers and force multipliers, working collaboratively to remove friction from the development process. We value automation, measurement, and continuous improvement, always asking \"how can we make this better for our developers?\"\n\n\nThe Pay Range For This Role Is\n\n135,000 - 165,000 USD per year(San Mateo)\n\n120,000 - 150,000 USD per year(New York)"
  },
  {
    "title": "Cloud DevOps With Azure Experience -100%Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "New Jersey, United States",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-with-azure-experience-100%25remote-at-the-dignify-solutions-llc-4341845867?position=36&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=%2FLn%2FykQcRqEOxTZ23mh%2FEA%3D%3D",
    "description": "In-depth knowledge of Azure services, including Azure Networking, Storage, Firewall, Compute, Function, Backup and Azure DevOps\nAutomating routine tasks using Ansible, Python, Terraform and Azure CLI for enhanced efficiency\nMinimum 4 years of specialized experience DEVOPS engineer role with full automation using Ansible .\nMinimum 2 years of Azure Infrastructure automation with Terraform.\nMust have experience working with Terraform and Ansible for infrastructure provisioning and configuration management, respectively.\nExperience configuring and managing CI/CD pipelines with tools such as GitLab, Jenkins, etc. for enterprise-level deployments.\nConfiguration-level knowledge for Linux sysadmin with RHEL.\nAdvanced troubleshooting and problem-solving skills, related to cloud and network infrastructure\nWorking-level experience in Azure Cloud (Azure/Azure GOV Cloud)\nAdvanced experience in large scale cloud architecture, design, and integration with a focus on automation, networking and NextGen Firewall\nExperience with Terraform, Ansible, and Python\nLinux OS\nProficiency in multi-cloud environments, especially in AWS, is highly desirable."
  },
  {
    "title": "DevOps Engineer - All Levels",
    "company": "CodeRabbit",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-all-levels-at-coderabbit-4318518267?position=37&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=fAQ3v5xY%2FV1rM615%2B9ySOg%3D%3D",
    "description": "About CodeRabbit\n\nCodeRabbit is an innovative research and development company focused on building extraordinarily productive human-machine collaboration systems. Our primary goal is to create the next generation of Gen AI-driven code reviewers: a symbiotic partnership between humans and advanced algorithms that significantly outperforms individual engineers. We combine language models with human ingenuity to push the boundaries of software development efficiency and quality.\n\nRole Overview\n\nAs a DevOps Engineer at CodeRabbit, you‚Äôll play a key role in scaling, securing, and hardening the infrastructure that powers our AI-enabled developer tools. You‚Äôll work closely with our platform engineers, backend team, and applied AI teams to ensure that our systems are resilient, observable, fast, and easy to deploy.\n\nThis is a hands-on, IC-focused role for someone who thrives in fast-paced environments, takes ownership of critical infrastructure, and wants to build tooling that unblocks an ambitious engineering team.\n\nResponsibilities\n\n\nDesign, implement, and maintain scalable CI/CD pipelines\nDevelop and manage infrastructure as code (e.g., Terraform, Pulumi)\nImprove system reliability through monitoring, alerting, logging, and failover strategies\nWork with platform and backend teams to identify and resolve performance bottlenecks\nContribute to deployment workflows, environment automation, and developer tooling\nEnsure infrastructure security and compliance practices are in place\n\n\nQualifications\n\n\nEducation: Degree in Computer Science, Engineering, or a related technical field, or equivalent practical experience\nExperience: 3+ years in a DevOps, Infrastructure, or SRE role at a fast-paced tech company or startup\nTooling: Expert-level proficiency with CI/CD systems (GitHub Actions, ArgoCD, etc.), Docker, and Kubernetes\nInfrastructure: Expert with cloud providers (AWS/GCP), distributed systems architecture and implementation, IaC tools (Terraform, Pulumi), and secrets management (Vault, SSM, etc.)\nObservability: Strong understanding of logging, metrics, and monitoring in large-scale distributed systems (e.g., Grafana, Prometheus, ELK, Datadog)\nCollaboration: Effective at partnering with backend and ML teams to deliver stable, high-velocity systems\nSecurity: Experience building with best practices in cloud and application-level security\n\n\nBonus Points\n\n\nExperience supporting AI or ML workloads in production\nExperience with ephemeral environments and preview deployments\nContributions to internal platform tools or DevOps open-source projects\nPast ownership of high-uptime systems or regulated environments\n\n\nWhy Join Us?\n\n\nBuild the Future: We‚Äôre redefining code review with AI. You‚Äôll help shape a new development paradigm with cutting-edge technology that has real-world impact.\nReal Ownership: Every engineer at CodeRabbit owns projects end-to-end ‚Äî from proposal to production.\nCollaborative & Innovative Environment: Join a tight-knit team of engineers, designers, and researchers who are passionate about building transformative products.\nProfessional Growth: We invest in our people ‚Äî through mentorship, responsibility, and development opportunities.\nCompetitive Compensation: We offer a strong salary, equity, and benefits package.\nHybrid Work Culture: We collaborate in person in the Bay Area weekly, with flexibility for heads-down remote work.\n\n\nOur Values\n\n\nü§ù Collaborative Humans: Prioritizing collective intelligence\nüöÄ Fearless Innovators: Turning obstacles into growth opportunities\nüí™ Persistent, Passionate Developers: Thriving on complex, long-term challenges\nüéØ Impact-Driven Creators: Crafting intuitive tools for developers\nüß† Rapid Learners and Un-learners: Adapting quickly in our fast-paced technological world\n\n\nBase pay range for this role is $180k-260k. Actual salary will be based on job-related skills, experience, and location.\n\nApply Now ‚Äî If you're passionate about building high-impact infrastructure and enabling AI-powered developer experiences, we‚Äôd love to hear from you.\n\nCompensation Range: $175K - $275K"
  },
  {
    "title": "DevOps Engineer",
    "company": "Uffizio",
    "location": "Michigan, United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-uffizio-4324397378?position=38&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=YSLOEqRc1YgYn4fPu8JSPw%3D%3D",
    "description": "Uffizio Group is looking for a proactive, ownership-driven DevOps Engineer who can lead our DevOps culture to the next level.\n\nWhat We‚Äôre Looking For\n\nSomeone who doesn‚Äôt wait for tasks ‚Äî they identify improvements and execute.\n\nA mindset of continuous innovation in CI/CD, infrastructure automation, logging, and monitoring.\n\nStrong leadership to guide a DevOps team, set standards, and ensure timely delivery.\n\nHigh accountability ‚Äî taking responsibility for stability, performance, and incident response.\n\nAbility to introduce new tools, processes, and best practices.\n\nYour Responsibilities\n\nOwn end-to-end CI/CD pipelines (GitHub/GitLab/Bitbucket).\n\nAutomate deployments across multi-cloud/AWS environments.\n\nImprove observability (monitoring, alerting, logging).\n\nLead the team in incident management and RCA.\n\nBuild a DevOps roadmap that increases reliability and performance"
  },
  {
    "title": "DevOps Assistant (Entry-Level)",
    "company": "45PRESS",
    "location": "Canfield, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-assistant-entry-level-at-45press-4301017192?position=39&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=lPnuZs%2F%2ByuFUkOA9HISy8Q%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Senior DevOps Engineer",
    "company": "CEIPAL",
    "location": "Charlotte, NC",
    "link": "https://www.linkedin.com/jobs/view/senior-devops-engineer-at-ceipal-4305453358?position=40&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=W1bkuueGHlB1h6GRkvQQ8Q%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4310657957?position=41&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=XT9RpEj6THmaPWLkBGA9bw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Hudu",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hudu-4323191230?position=42&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=iXEsYPpv8aMtDnNuUxsi0w%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4335667666?position=43&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=jnWBf0rgQyaycHAmII9oyA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Staff Engineer: DevOps",
    "company": "Dispel",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/staff-engineer-devops-at-dispel-4339045806?position=44&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=Ap2VVboFpZ4wRsyx8kIc5g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "IT Automation LLC",
    "location": "Cary, NC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-it-automation-llc-4324192032?position=45&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=615ZTYqlEkm5Em8XU75EhA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CHEQUESPREAD PLC",
    "location": "Valley Forge, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chequespread-plc-4288904252?position=46&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=noGXoCzS7FT4qNjjXoECUw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "eSimplicity",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-esimplicity-4315888714?position=47&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=PphkPA0yVhqXK4xy8vCLNQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud/DevOps Engineer",
    "company": "Tagup, Inc.",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-engineer-at-tagup-inc-4333051833?position=48&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=DoQDWCi7dGwydOSfTz08GQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mark43",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mark43-4309062970?position=49&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=3YffZDVjwK7%2BsaeXvZQMvg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CMG (Capital Markets Gateway)",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cmg-capital-markets-gateway-4338419750?position=50&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=RaaHa91zjQaoIZ1QjhFUiA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mintlify",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mintlify-4318506680?position=51&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=5cxXfQvinZjydeIixok4%2BA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Support Engineer",
    "company": "Porter",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-support-engineer-at-porter-4295124575?position=52&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=y9MfjuV%2BE03S5bfjsFUU9g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Northstrat Incorporated",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-northstrat-incorporated-4304125676?position=53&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=rehAqo9XOxPIIrWAkqxfvA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Paramount",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-paramount-4335876548?position=54&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=Z6MHiQiUYGbERrg125c3Sw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "SmartVault",
    "location": "Houston, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-smartvault-4297941616?position=55&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=EZRCsOeKwSmcsyZ2kMnZ0g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "RSC2, Inc.",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rsc2-inc-4311252822?position=56&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=ZNx8nGyYky%2FxfCza5DDkQw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps Support Engineer",
    "company": "Nihon Kohden Digital Health Solutions",
    "location": "Irvine, CA",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-support-engineer-at-nihon-kohden-digital-health-solutions-4295710052?position=57&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=hkKmitbc3fBHWtAc41aiig%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "PingWind",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pingwind-4316019938?position=58&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=7LygKl3SeSvaoRXJldFx2g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Cymertek Corporation",
    "location": "San Antonio, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cymertek-corporation-4336305401?position=59&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=M3%2BUOgoDbogFEyUWRx0XDw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Ryan",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ryan-4284462825?position=60&pageNum=0&refId=HF0O54QfEZonUB1uX955kA%3D%3D&trackingId=1nHfrFs95c9UumceRu7PhQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Mid-Junior DevOps Engineer - USA",
    "company": "HERE",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/mid-junior-devops-engineer-usa-at-here-4347377348?position=1&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=Ha%2F3UMzDjODeG%2BVwTazkIw%3D%3D",
    "description": "Mid-Junior DevOps Engineer\n\nLocation: New York, NY / Hybrid Remote / Remote within USA (EST / CST time zone)\n\nWe have an office in New York City and this position can either be based in the office, hybrid remote, or remote within the EST/CST time zones (subject to your existing legal right to work in the jurisdiction).\n\nAbout HERE\n\nEverything works right here‚Ñ¢.\n\nTraditional browsers weren't built for work. In today's enterprise environment‚Äîwhere security threats are constant and productivity is critical‚Äîlegacy browsers fall short. That's why we built HERE, the browser purpose-built for work.\n\nPowered by Chromium, HERE Enterprise Browser combines enterprise-grade security, seamless productivity, and native AI integration in one secure, intelligent workspace. Designed for regulated industries, HERE offers deep policy controls, identity-based access, secure workspace isolation, and full interoperability across SaaS, legacy, and virtualized environments. Our platform enables teams to work faster, more securely, and more intelligently‚Äîwithout compromise.\n\nHERE technology is trusted by 90% of global banks and also used within the U.S. Intelligence Community and other sectors. We're backed by some of the world's most respected financial institutions and venture firms, including Bain Capital Ventures, Bank of America, J.P. Morgan, Wells Fargo and IQT, the not-for-profit strategic investor that accelerates the introduction of groundbreaking technologies to enhance the national security of America and its allies.\n\nAbout the Role\n\nHERE is seeking a mid-junior DevOps Engineer to join our infrastructure team! The primary responsibilities for this role will span CI/CD pipeline engineering and cloud operations, maintaining and improving our GitHub and GitLab CI/CD pipelines, and supporting our AWS cloud infrastructure. In this role, you will gain hands-on experience with real production build systems and cloud platforms- while having the opportunity to work on practical projects that directly impact both our development velocity and operational reliability.\n\nWe're actively evolving toward a cloud-agnostic, multi-cloud architecture and migrating to Kubernetes for container orchestration. While current AWS and ECS experience is essential, having exposure to Azure, GCP, and Kubernetes will position you well for our infrastructure roadmap.\n\nThis role offers the opportunity to collaborate with senior engineers who will provide guidance and mentorship, whilst giving you ownership of projects across the DevOps lifecycle. This is an excellent platform for building practical experience with modern build engineering (CI/CD automation, cloud infrastructure, and deployment practices) within a production environment.\n\nResponsibilities\n\n\nCI/CD Pipeline Development:\nBuild, maintain, and optimize GitLab CI/CD pipelines for multi-platform builds (Windows, macOS, Linux).\nWork with YAML configurations, pipeline stages, artifacts, and deployment workflows.\nCloud Infrastructure Operations:\nHelp maintain and improve AWS infrastructure including ECS/Fargate deployments, RDS databases, Route53 DNS, VPC networking, and IAM policies.\nSupport multi-tenant and multi-region architecture.\nContainer & Deployment Management:\nWork with Docker containers, ECS task definitions, and ECR registries.\nDeploy and manage containerized Node.js applications in production environments.\nRelease Management:\nHelp manage release processes including version promotion, release channels (canary, beta, stable), and automated deployment to staging and production environments.\nDatabase Operations:\nSupport PostgreSQL on AWS RDS‚Äîbackups, SSH tunneling through bastion hosts, read-only user management, and database configuration for multi-tenant environments.\nAutomation & Scripting:\nWrite and maintain automation scripts in Bash, PowerShell, Python, and Node.js.\nBuild tools to improve infrastructure reliability and developer experience.\nInternal Tools Support:\nHelp maintain web-based DevOps tools built with Express.js, React, and TypeScript‚Äîtools for cloud settings management, tenant provisioning, and deployment monitoring.\n\nWhat We're Looking For\n\nIdeally 2 to 4 years of experience with the following core requirements:\n\n\nGitLab CI/CD: Experience with GitLab CI/CD pipelines‚ÄîYAML configuration, stages, jobs, artifacts, rules, dependencies.\nUnderstanding of CI/CD best practices and pipeline optimization.\nAWS Cloud Fundamentals: Practical experience with core AWS services‚ÄîEC2, ECS/Fargate, RDS, Route53, VPC, IAM, Secrets Manager, CloudWatch. Comfortable navigating the AWS Console and CLI.\nMulti-Platform Scripting: Solid scripting skills in Bash (Linux) and PowerShell (Windows). Ability to write maintainable automation scripts for both platforms.\nContainerization: Hands-on Docker experience‚Äîbuilding images, writing Dockerfiles, docker-compose, understanding container networking, and working with ECS/ECR.\nBuild Systems: Experience with build tools and package managers‚Äînpm/Node.js, .NET/NuGet, Python packaging. Understanding of dependency management and build artifacts.\nVersion Control: Strong Git fundamentals‚Äîbranching strategies, merge requests, tagging. Experience with GitHub (or GitLab) workflows and code review practices.\nLinux/Unix & Windows: Comfortable in both environments‚ÄîSSH, file permissions, package managers, systemd, PowerShell. Understanding of cross-platform operational challenges.\nNode.js/JavaScript: Comfortable reading and writing JavaScript/Node.js code. Experience with npm, package.json, and basic Express.js applications for tooling.\n\n\nNice to Have\n\n\nKubernetes experience (EKS, GKE, AKS) or willingness to learn, we're migrating from ECS to K8s\nMulti-cloud experience (Azure, GCP) or cloud-agnostic architecture knowledge\nGitLab Runner administration and configuration\nAWS CDK or CloudFormation for Infrastructure as Code\nTerraform for multi-cloud infrastructure management\nTypeScript development experience\nPostgreSQL database administration and optimization\n.NET build systems and NuGet package management\nReact or frontend framework experience\nAirflow or workflow orchestration tools\nHelm charts and Kubernetes manifest management\n\n\nWhat We're Offering\n\nBenefits -\n\n\nGenerous Paid Time Off, Paid Holidays & Sick Time\nCompetitive & Comprehensive Health Insurance\nThoughtfully-Planned Paid Parental Leave\nFinancial Well-Being Plans (FSA) (401k) (Life Insurance)\nStock Options\nProfessional Development Courses\nEmployee Resource Groups\n\n\nAdditional Perks -\n\n\nOne Medical - Free Membership\nTalkspace - Mental Health Therapy 24/7\nTeam Lunches\nCasual dress code\nCommuter Benefits (NYC employees only)\nCitibike (NYC employees only)\n\n\nLife at HERE\n\nAt HERE, we pride ourselves on fostering a friendly, collaborative, and supportive culture that truly respects the diversity of thought. Our goal is to create a space where employees can learn and innovate, and overall, have a good time doing it. We value and appreciate that our employees have a wide set of interests and experiences and put importance on taking the time to get to know one another and form relationships. From virtual socials and in-person events, to informal meetings and employee resource groups, we make it easy to engage and connect. Our environment promotes a productive, enjoyable learning experience - aligned together, working to create compelling solutions for our clients. Everything works right here.‚Ñ¢\n\nWe are HERE - Read about our recent rebrand from OpenFin to HERE\n\nRecent Awards\n\n\nVoted \"Enterprise Browser of the Year\" by CIO Review (2025)\nVoted \"100 Best Midsize Companies to Work For in NYC\" by BuiltIn (2025)\nVoted \"Top 10 Contact Center Technologies & Capabilities of 2024\" by CX Today (2024)\nVoted \"Best Enterprise Environment for Interoperability\" by TradingTech Insight Awards Europe (2024)\nVoted \"Top 50 Best Startups to Work for in the US\" & \"Top 50 Best Startups to Work for in New York\" by BuiltIn (2024)\nVoted as a \"Best Employer Award\" finalist at the UK FinTech Awards (2023)\nVoted \"Best FinTech Company CEO\" at the FinTech Breakthrough Awards (2023)\nVoted \"Best Internal Talent Team\" by Financial Technologist (2023)\nVoted \"Best Solution for Workflow Automation\" at the Trading Tech Insight Awards (2023)\nVoted \"Top Innovator Across Financial Markets\" in TabbFORUM NOVA Awards (2023)\nVoted \"Best User Interface Innovation\" in the Risk Markets Technology Awards (2023)\nVoted \"Top 100 Most Promising Private FinTech Companies\" by CB Insights (2023)\nVoted \"Most Influential Financial Technology Firm\" by Harrington Starr (2023)\n\n\nRECRUITERS NOTICE: Recruiters - if you wish to reach out to us regarding this job posting, you may reach out to externalrecruitment@here.io in order for your communication to be reviewed. HERE will review these communications if external help is needed for a position. Agencies may not contact individuals within our organization with solicitations. Firms that do not follow these guidelines risk having all communication from their firm being blocked. We thank you in advance for your cooperation in following our process.\n\nSponsorship - While we highly value all of our candidates, we are not offering sponsorship for this role.\n\nSalary Range: $70k - $120k\n\nSalary Range Disclaimer: This base salary range represents the low and high end salary range for this particular position; not all encompassing of the total compensation package. Actual salaries may vary depending upon but not limited to experience, special skill set, education and location. This range represents only one aspect of HERE's total compensation package offered to employees. Other forms of compensation may be stock options, commissions, paid time off and other variable benefits. Learn more about additional HERE compensation benefits above."
  },
  {
    "title": "junior devops engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-ova-work-4309344701?position=2&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=xp56e1zQnReDrjUkXdvMHw%3D%3D",
    "description": "Job Title: Junior DevOps Engineer\n\nLocation: Remote\n\nJob Type: Full-time\n\nExperience Level: Entry-Level (0-2 years)\n\nDepartment: IT / Engineering / DevOps\n\nJob Summary\n\nWe are looking for a motivated and detail-oriented Junior DevOps Engineer to join our growing DevOps team. This role is ideal for someone with a foundational understanding of DevOps practices and a passion for automation, cloud technologies, and continuous integration/deployment. You will assist in maintaining and improving our infrastructure, deployment pipelines, and monitoring systems.\n\nKey Responsibilities\n\n\nAssist in the setup, maintenance, and monitoring of CI/CD pipelines.\nSupport cloud infrastructure (AWS, Azure, GCP) and help manage deployments.\nCollaborate with development and operations teams to ensure reliable software delivery.\nWrite scripts and automation tools to streamline operations and deployments.\nMonitor system performance and troubleshoot issues in development and production environments.\nMaintain documentation for infrastructure and deployment processes.\nLearn and apply best practices in security, scalability, and reliability.\n\n\nRequired Qualifications\n\n\nBachelor's degree in Computer Science, Information Technology, or related field.\nBasic understanding of DevOps principles and software development lifecycle.\nFamiliarity with Linux/Unix systems and shell scripting.\nExposure to cloud platforms (AWS, Azure, or GCP).\nExperience with version control systems (e.g., Git).\nKnowledge of CI/CD tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nStrong problem-solving and communication skills.\nEagerness to learn and grow in a fast-paced environment.\n\n\nPreferred Qualifications\n\n\nInternship or project experience in DevOps or system administration.\nFamiliarity with containerization tools (Docker) and orchestration (Kubernetes).\nExperience with Infrastructure as Code (Terraform, Ansible).\nBasic knowledge of monitoring tools (Prometheus, Grafana, ELK Stack).\n\n\nBenefits\n\n\nCompetitive salary and growth opportunities.\nMentorship from senior engineers.\nHealth and wellness benefits.\nFlexible work hours and remote work options.\nAccess to training and certification programs."
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4341955705?position=3&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ILjgxWHsSeteyDbH1DQ0nw%3D%3D",
    "description": "Over 12 -15 years of overall expereince needed.\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nKubernetes (3-4 YOE) and Fieldglass Experience (1-2 YOE)"
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4347005704?position=4&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=sjeh2pA%2BkooCDR0fnm1G7A%3D%3D",
    "description": "Bachelor's degree in a technical field such as computer science, computer engineering or related field required 0-2 years experience required.\n1-2 years of experience with Kubernetes.\nISBN experience preferred.\nA solid foundation in computer science , with strong competencies in data structures, algorithms, and software design large systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems experience in programming and experience with problem diagnosis and resolution."
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "GliaCell Technologies",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-gliacell-technologies-4338894490?position=5&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=TEXDQhFIVyt3BKV2szGAFA%3D%3D",
    "description": "An active or rein-statable TS/SCI with Polygraph security clearance is REQUIRED. Please do not apply if you currently do not possess this level of clearance.***\n\n\nAre you a Junior DevOps Engineer who is ready for a new challenge that will launch your career to the next level?\n\n\nTired of being treated like a company drone?\nTired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?\nOur engineers were certainly tired of the same.\n\n\nAt GliaCell our slogan is ‚ÄúWe make It happen‚Äù.\n\n\nWe will immerse you in the latest technologies.\nWe will develop and support your own personalized training program to continue your individual growth.\nWe will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.\n\n\nCulture isn‚Äôt something you need to talk about‚Ä¶if it just exists.\n\nIf this sounds interesting to you, then we‚Äôd like to have a discussion regarding your next adventure! If you want to be a drone, this isn‚Äôt the place for you.\n\nWe Make It Happen!\n\nGliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.\n\nGliaCell‚Äôs Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.\n\nWe Offer\n\n\nLong term job security\nCompetitive salaries & bonus opportunities\nChallenging work you are passionate about\nAbility to work with some amazingly talented people\n\n\nJob Description\n\nGliaCell is seeking a Junior DevOps Engineer on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.\n\nResponsibilities\n\n\nEstablishing a test framework and automated tests utilizing Cucumber and Cypru\nKnowledgeable in Microservices design & architecture, CI/CD, Test frameworks and automation, Agile Methodology.\nExecute load and performance testing, chaos testing, functional testing and end-to-end testin\nAgile development and delivery of software\nCommunication and collaboration: Software Development is a team-oriented discipline. Engineers need to be able to communicate and collaborate effectively with other team members, as well as with stakeholders.\n\n\nRequired Skills\n\n\nPython and Cucumber\n\n\nDesired Skills:\n\n\nAWS services such as Lambdas, Step Functions, EC2 and S3\n\n\nKey Requirements\n\nTo be considered for this position you must have the following:\n\n\nPossess an active or rein-statable TS/SCI with Polygraph security clearance.\nU.S. Citizenship.\nWorks well independently as well as on a team.\n6+ years experience as a Developer in programs and contracts of similar scope, type, and complexity is required. A bachelor‚Äôs degree in a technical discipline from an accredited college or university is required. Five (4) years of development experience may be substituted for a bachelor‚Äôs degree.\n\n\nLocation: Annapolis Junction, MD\n\nSalary Range: The salary range for this full-time position is $50,000 to $120,000. Our salary ranges are determined by position, level, skills, professional experience, relevant education and certifications. The range displayed on each job posting reflects the minimum and maximum target salaries for this position across our projects. Within the range, your salary is determined by your individual benefits package selection. Your recruiter can share more about the specific salary range for your preferred position during the hiring process.\n\nBenefits\n\n\nMedical, Dental, and Vision Coverage for Employee and Dependents\nUp to 25 Days of Paid Time Off\nUp to 40 hours of PTO Carryover\n11 Federal Government Holidays\nWork From Home Opportunities\n401K Company Contribution, Fully Vested Day 1\nDiscretionary, Certification, and Sign-On Bonus Potential\nEmployee Referral Bonus Program\nAnnual Professional Development\n100% Premium Covered for Life & Disability Insurances\nAdditional Voluntary Life Insurance Coverage Available\nEmployee Assistance Program\nTravel Protection Program\nFinancial Planning Assistance\nBereavement and Jury Duty Leave\nMonthly Team and Family Events\nTechnology Budget\nGlobal Entry\nAnnual Swag Budget\n\n\nLearn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/\n\nGliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
  },
  {
    "title": "DevOps Cloud Engineer Based in U.S.A",
    "company": "Advancio",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-cloud-engineer-based-in-u-s-a-at-advancio-4324442139?position=6&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=z1H4CHucl7AE7QrKxqedQg%3D%3D",
    "description": "This is a remote position.\n\nWho We Are:\n\n\nAt Advancio, we are passionate about technology and its ability to transform the world. We are rapidly expanding and building a company where we serve exceptional businesses, hire top talent, and have a lot of fun doing what we love!\n\n\nJob Summary:\n\nWe are seeking a skilled DevOps Cloud Engineer to design, implement, and manage scalable cloud-based infrastructure and DevOps processes. The ideal candidate will have extensive experience with cloud platforms, CI/CD pipelines, and automation tools, ensuring the efficient deployment and operation of applications.\n\n\nWhat will you do:\n\n\nDesign, deploy, and manage cloud infrastructure on platforms such as AWS, Azure, or Google Cloud Platform (GCP).\n\nBuild and maintain CI/CD pipelines to streamline development and deployment processes.\n\nAutomate infrastructure provisioning, configuration, and monitoring using tools like Terraform, Ansible, or similar.\n\nEnsure system reliability, availability, and performance through robust monitoring and alerting.\n\nCollaborate with development teams to optimize the delivery and scalability of applications.\n\nManage containerized workloads using Docker and orchestration platforms such as Kubernetes.\n\nImplement security best practices for cloud environments, including identity management, encryption, and compliance adherence.\n\nStay updated with the latest DevOps tools and methodologies to enhance team efficiency.\n\n\n\n\nRequirements\n\n\n\n\n\n\n5+ years of experience in DevOps, cloud engineering, or related roles.\n\nAdvanced English communication skills, both verbal and written.\n\nProficiency in at least one major cloud platform (AWS, Azure, or GCP).\n\nHands-on experience with CI/CD tools (e.g., Jenkins, GitLab CI/CD, CircleCI).\n\nStrong scripting skills in Python, Bash, or similar languages.\n\nSolid knowledge of infrastructure-as-code (IaC) tools like Terraform or CloudFormation.\n\nExperience with containerization (Docker) and orchestration (Kubernetes).\n\nFamiliarity with monitoring and logging tools like Prometheus, Grafana, or ELK Stack.\n\nStrong understanding of networking, security, and system architecture."
  },
  {
    "title": "DeVops Engineer",
    "company": "Pittsburgh Robotics Network",
    "location": "Pittsburgh, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pittsburgh-robotics-network-4347073200?position=7&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=5nFnL%2B%2Bqx2DFcyhlCKZJPA%3D%3D",
    "description": "DevOps Engineer\n\nTDK SensEI\n\nPittsburgh, PA\n\nThis position is for our Pittsburgh, PA office - only apply if you are based there or willing to relocate.\n\nAt TDK SensEI, we are transforming how industrial customers utilize and interact with sensor data. We specialize in developing advanced AI solutions capable of running directly on edge devices. By processing data locally, TDK SensEI enhances real-time decision-making, privacy, security, and cost efficiency. Our offerings include automated machine learning tools, AI-powered condition-based monitoring systems, and various sensor devices optimized for low latency and power consumption. Collaborating with leading global companies, we empower teams to effortlessly devise and implement machine learning solutions for industrial applications, all without the need for coding.\n\nWe are seeking a Dev Ops engineer to join our team. In this position, the candidate will be responsible for managing, operating, and provisioning cloud environments such as AWS, Azure, Google cloud. You will work with development, security, and operations teams to deploy, scale, and operate dev environments. You are also responsible for improving and automating the dev environment. You will establish configuration management, automate our infrastructure, implement continuous integration, and train the team in DevOps best practices.\n\nAs a Dev Ops Engineer, Your Responsibilities Will Include\n\n\nDesigning, implementing, and maintaining tools and processes for continuous integration, delivery, and deployment of software\nWorking with developers to deploy and manage code changes\nWorking with operations staff to ensure that systems are up and running smoothly\nAutomating, monitoring, testing, configuring, networking, and Infrastructure as Code (IaC)\nStreamlining and automating processes while troubleshooting existing development procedures\nManaging the creation, release, and configuration of production systems\nArchitecting and optimizing several service components running on AWS environment\n\n\nSkills & Requirements\n\n\nBachelor‚Äôs degree or equivalent experience\nMinimum 2 years of experience in DevOps, infrastructure automation or similar role\nKnowledge of Linux/UNIX administration\nProficiency in Python, JavaScript and other script environments (e.g. bash)\nExperience with containerization technologies, Docker and associated tooling\nExperience designing and implementing CI/CD pipelines\nExperience operating databases such as PostgreSQL or MySQL, especially in cloud-native services like RDS\nAwareness of critical concepts in DevOps and Agile principles\nUS work authorization\n\n\nNice To Have\n\n\nAWS certifications (e.g., AWS Certified DevOps Engineer, AWS Certified Solutions Architect).\nFamiliarity with other cloud providers (Azure, Google Cloud).\nExperience with container orchestration systems such as Kubernetes/EKS"
  },
  {
    "title": "DevOps Engineer",
    "company": "Princeton IT Services, Inc",
    "location": "Englewood Cliffs, NJ",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-princeton-it-services-inc-4338714288?position=8&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=sQGsbxvArwRVDK7ZdvFnaw%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Englewood Cliffs, NJ\n\nEmployment Type: W2 Only\n\nJob Summary\n\nWe are seeking a DevOps Engineer with strong hands-on experience in Linux, Docker, and Kubernetes to support and optimize our deployment environment in Englewood Cliffs, NJ. This is a W2-only role requiring solid skills in automation, CI/CD, and container orchestration. The ideal candidate will ensure smooth application releases, maintain system stability, and collaborate closely with development teams.\n\nKey Responsibilities\n\n\nManage and support Linux-based systems in production and staging environments.\nBuild, maintain, and optimize CI/CD pipelines for automated deployments.\nCreate, manage, and troubleshoot Docker containers and images.\nDeploy, monitor, and tune Kubernetes clusters and workloads.\nAutomate infrastructure tasks using Shell or Python scripts.\nImplement and manage monitoring and logging tools (Prometheus, Grafana, ELK, etc.).\nTroubleshoot system, container, and cluster-level issues end-to-end.\nWork cross-functionally with development and QA teams to ensure smooth releases.\n\n\nRequired Skills\n\n\n8+ years of DevOps or related experience.\nStrong hands-on experience with Linux administration.\nSolid experience working with Docker for containerization.\nStrong working knowledge of Kubernetes (deployments, scaling, troubleshooting).\nExperience building CI/CD pipelines (Jenkins, GitLab CI, GitHub Actions).\nStrong scripting skills in Shell/Bash/Python.\nExperience with monitoring and logging tools."
  },
  {
    "title": "DevOps Engineer",
    "company": "Lean TECHniques",
    "location": "Johnston, IA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lean-techniques-4336685413?position=9&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=%2BdnXKNTigPZfZBHNsXiIIQ%3D%3D",
    "description": "Maybe you‚Äôre bored and need a new challenge. Or you‚Äôre sick of all the bureaucracy and just want to focus on designing kick-ass software.\n\nWhatever the reason, we want you to know that LT is different. And not just air quotes ‚Äúdifferent,‚Äù but more like ‚Äúbreathing easy for the first time in a long time‚Äù different.\n\nIt‚Äôs a place where you can write your own story and make a difference along the way. At LT, you‚Äôll have the freedom and flexibility to do what you think needs to be done, and you‚Äôll get to do it while working alongside a team of other curious individuals who love a good challenge too.\n\nWe‚Äôre currently looking to add a DevOps Engineer to our crew of nerds. If you‚Äôre someone who has 5+ years of DevOps experience, we'd love to chat!"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337132819?position=10&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=8lvYJiKfAw9Ew3DpLGX9RA%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "DevOps Engineer (35 LPA - 55 LPA)",
    "company": "CodeRound AI",
    "location": "Greater Bloomington Area",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-35-lpa-55-lpa-at-coderound-ai-4308183910?position=11&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=IRNQ6tjRvKZFpco2Jirdbw%3D%3D",
    "description": "üöÄ What We‚Äôre Building\n\n\nCodeRound AI matches top 5% tech talent to fastest growing VC funded AI startups.\nCandidates apply once and get UPTO 10 remote as well as onsite interview opportunities IF selected!\nTop-tier product startups in US, UAE & India have hired top engineers & ML folk using CodeRound\n\n\nüß© What You‚Äôll Do\n\n\nBuild and optimize our cloud infrastructure ‚Äî scalable, secure, and cost-effective (mostly AWS).\nSet up and manage CI/CD pipelines to ensure smooth deployment across backend, AI services, and mobile.\nContainerize backend services (FastAPI, Rails) and optimize them for performance.\nImplement monitoring, alerting, and logging to catch issues before users do.\nOptimize database performance (Postgres, Redis) and manage backups and scaling.\nCollaborate with backend, AI, and product teams to deploy new features safely and quickly.\nChampion infra-as-code and automation wherever possible.\n\n\nüí• Why this is exciting\n\n\nYou'll own DevOps for a high-usage, real-world AI platform ‚Äî not just internal tools.\nYou‚Äôll work on real-time, high-stakes flows ‚Äî interviews, scoring, hiring decisions.\nYou‚Äôll work closely with founders, ship weekly, and see the direct impact of your work.\n\n\n‚úÖ You‚Äôll Be Great At This If You\n\n\nHave 4+ years of experience as a DevOps engineer, SRE, or infrastructure engineer.\nAre strong with AWS services (EC2, RDS, ECS/EKS, S3, CloudWatch).\nCan write clean, reusable Terraform or CloudFormation code.\nHave experience setting up CI/CD pipelines and optimizing build/release flows.\nAre comfortable with Docker, Linux servers, and basic networking (VPCs, security groups).\nUnderstand application and database scaling (horizontal/vertical).\n\n\n‚ö° Bonus If You\n\n\nHave experience supporting AI/ML pipelines in production (fine-tuning infra, vector DBs, etc.).\nKnow cost optimization tricks for cloud infra (spot instances, autoscaling groups, etc.).\nAre excited to eventually build a small infra team"
  },
  {
    "title": "Devops Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341915759?position=12&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=CHn4FZN0YYMlGNMBkwHytg%3D%3D",
    "description": "Job Description:\n\n\nServe as a subject matter expert to develop and support DevOps Web Access Management solutions\nInstall, configure, and maintain automation solutions, in support of KeyBank infrastructure\nDevelop Standard Operating Procedures, maintenance plans and provide status reports as required\nPerform daily operational tasks as required for the Web Access Management team\n\n\nQualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "Huntington Beach, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337182535?position=13&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=%2FH%2FoVwUmfbLVhKExV8ebZQ%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "Devops Engineer",
    "company": "Hoplite Solutions LLC",
    "location": "Bethesda, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hoplite-solutions-llc-4336082750?position=14&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=SDTG1%2FS%2F5qZr2aOl4RyEhA%3D%3D",
    "description": "Hoplite Solutions is hiring DevOps Engineers at all experience levels to join our team in Bethesda, MD. In this mission-critical role, you will provide essential system support to our customer while collaborating closely with software development teams and other key technology stakeholders. You will help maintain, enhance, and support a range of IC enterprise products‚Äîboth legacy systems and new solutions‚Äîwithin an Agile SAFe environment.\n\nAs a DevOps Engineer, you will work hand-in-hand with software engineering teams to deploy and operate systems, automate and optimize processes, and build and maintain tools that support deployment, monitoring, and ongoing operations. You will also troubleshoot and resolve issues across development, test, and production environments, ensuring reliability, efficiency, and continuous improvement across the enterprise.\n\nPrimary Responsibilities:\n\n\nSupports software deployments, cloud infrastructure baselines, and operational availability of production systems\nManaging, building, configuring, administering, operating and maintaining all components that comprise the DevOps environment\nDefining enterprise Continuous Integration/Continuous Deployment processes and best practices\nCodifying DevOps best practices across the enterprise\nDeveloping and maintaining scripts to automate tool deployment to an AWS cloud environment and other tasks\nScripting and maintaining build environments\nWorking with project teams to integrate their products into the DevOps environment\n\n\nBasic Qualifications\n\n\nDemonstrated experience setting up one or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience troubleshooting issues with two or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience working within a software development team and supporting developers and developer activities\nBachelors degree with 4 or more years of prior relevant work experience or Masters with 2 or more years of prior relevant work experience. Will consider additional work experience in lieu of a degree\nTo be considered must have an active TS/SCI with polygraph security clearance\n\n\nPreferred Qualifications\n\n\nAWS Associate Certification (Developer, Solution Architect, or Sys Ops Administrator)\nAWS Professional Certification (DevOps Engineer or Solutions Architect)\nDemonstrated experience in container orchestration using Docker, Vagrant, Kubernetes, or AWS ECS/ECR\nDemonstrated experience with Languages including Java, Python, JavaScript, Ruby, PHP, and Unix shell Scripting\nDemonstrated experience with Ansible, or Puppet\n\n\nHoplite Solutions offers very competitive salaries and an excellent benefits package, to include a 7% employer 401k contribution, fully paid healthcare for our employees, outstanding training benefits, company funded life insurance and short-term disability insurance, and many more.\n\nPowered by JazzHR\n\nwwBe8pS8mn"
  },
  {
    "title": "DevOps Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341985652?position=15&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=OIkvouN92QSjFYtLnMasfw%3D%3D",
    "description": "Qualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments."
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "Indianapolis, IN",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4339356296?position=16&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=g1l7bZ%2B8160RIZx5GEsCJA%3D%3D",
    "description": "Who we are‚Ä¶\n\nVerra Mobility is a global leader in smart mobility. We develop technology-enabled solutions that help the world move safely and easily. We are fostering the development of safe cities, working with police departments and municipalities to install over 4,000 red-light, speed, and school bus stop arm safety cameras across North America. We are also creating smart roadways, serving the world's largest commercial fleets and rental car companies to manage tolling transactions and violations for over 8.5 million vehicles. And we are a leading provider of connected systems, processing nearly 165 million transactions each year across 50+ individual tolling authorities.\n\nCulture\n\nVerra Mobility Corporation is a rapidly-growing, entrepreneurial company that operates with a people-first philosophy and approach. The company lives by its core values‚ÄîDo What's Right, Lead with Grace, Win Together, and Own It‚Äîin everything it does for its customers and team members. The company seeks to grow aggressively, both organically and through acquisition, to continue to be the undisputed market leader with these five core competencies: bias for action, customer focus, teamwork, drive for results, and commitment to excellence.\n\nPosition Overview:\n\nWe are seeking an experienced and detail oriented Devops Engineer to join our team. In this role, you will be responsible for creating, maintaining, and securing our Devops pipelines and deployment systems to ensure high levels of performance and availability. This position is ideal for someone with a strong background in CI/CD methodologies particularly in cloud environments.\n\nEssential Responsibilities:\n\n\nSpend 50% writing automation scripts in Python and Bash.\nWrite various CI/CD pipelines for code releases.\nEnsure that pipelines meet both operations and security requirements.\nPartner with developers to identify areas of improvement in the developer experience.\nDesign and implement innovations that improve software velocity, infrastructure resiliency, security and data availability.\nWork with Software and Engineering to ensure new pipelines are created in parallel to code build.\nWork with Architecture on setting the path forward and gathering changes to the technology stack.\nAbility to respond to system issues, drive and participate in high - priority incident calls and emergency activities outside of standard office hours as needed.\nCollaborate with internal and external application, business partners to gain understanding of their business needs and adapt departmental roadmap plans and priorities to address operational challenges.\nWork with QE to ensure all automated testing is run during the deployment of the code.\nAbility to participate in an on-call rotation as needed.\n\n\nQualifications:\n\n\nMust have 5 years of Devops Engineering experience.\nFamiliarity with a wide range of systems engineering tools, including source code repository hubs, continuous integration services, issue tracking, test automation, deployment automation, development team collaboration, project management.\nNeed to have strong scripting skills to create automation in Python preferred or Bash.\nExperience with Cloudformation or Terraform for infrastructure as code.\nUsed continuous integration and continuous development (CI/CD) tools such as Jenkins, Gitlab, or Github Actions, preferred.\nKnowledge of DevOps tools such as, GitHub Actions, CloudFormation, GIT, SVN, Jenkins, JIRA, Rally, Greenhopper, Puppet/Chef Vagrant, Selenium, Azure DevOps (for sprint planning).\nUnderstanding of enterprise GIT repositories including branching and forking.\nHands-on Familiarity with AWS CloudWatch, AWS CloudTrail, AWS X-Ray, Grafana, and Prometheus.\nHands-on experience with Veracode and SonarQube are a plus.\nMust be located in Phoenix, AZ, Indianapolis, IN, or NY and be willing to commute into office 3 days a week.\n\n\nThis position is not eligible for sponsorship now or in the future and is only considering local Arizona, New York, or Indiana talent.\n\n\n\nVerra Mobility Values\n\n\n\nAn ideal candidate for this role naturally works in alignment with the Verra Mobility Core Values:\n\n\nOwn It. We focus on high performance and drive toward breakthrough outcomes. Our employees ensure accountability, optimize and align work, focus on the customer, and cultivate innovation.\nDo What's Right. We champion integrity and good character. Our team members model ethical behavior, demonstrate good judgment and are courageous.\nLead with Grace. We express humility and compassion, and we are authentic and candid. Our employees demonstrate self-awareness, care for others, instill trust, and communicate effectively.\nWin Together. We believe in growing and inspiring people together. We seek people who collaborate, value differences, think and act globally, foster an engaging work environment, and recognize and develop others.\n\n\n\n\nWith your explicit consent which you provided as part of the application process, we will retain candidate personal data solely for the business purpose for which it was collected. In no event will we retain such data more than two (2) years following the closure of the recruitment process relating to the role for which you applied or in the event other related job opportunities arise within the company. Verra Mobility Applicant Privacy Notice\n\nVerra Mobility is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status."
  },
  {
    "title": "DevOps Engineer",
    "company": "Protege",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-protege-4331315574?position=17&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=XLsGNGjq9UZjOgmc1kpzyQ%3D%3D",
    "description": "Company Overview:\n\nWe are building Protege to solve the biggest unmet need in AI ‚Äî getting access to the right training data. The process today is time intensive, incredibly expensive, and often ends in failure. The Protege platform facilitates the secure, efficient, and privacy-centric exchange of AI training data.\n\nSolving AI‚Äôs data problem is a generational opportunity. We‚Äôre backed by world-class investors and already powering partnerships with some of the most ambitious teams in AI. The company that succeeds will be one of the largest in AI ‚Äî and in tech.\n\nWe‚Äôre a lean, fast-moving, high-trust team of builders who are obsessed with velocity and impact. Our culture is built for people who thrive on ambiguity, own outcomes, and want to shape the future of data and AI.\n\nKey Responsibilities and Scope:\n\n\nAs a DevOps Engineer, you will be a critical part of our engineering team, responsible for safeguarding our AI/ML platforms, data pipelines, and cloud infrastructure\nYou will implement and develop monitoring strategies, and drive controls to protect our most valuable assets\n\n\nQualifications:\n\n\n4+ years of hands-on experience in a DevOps, Architecture, SecOps or Engineering role\nStrong experience with major cloud platforms and building cloud-native services including containerization, threat detection, vulnerability, governance, compliance, etc. with AWS preferred\nProficient in scripting languages like Python, SQL, Typescript or similar\nStrong experience with infra‚Äëas‚Äëcode, monitoring, and reliability for pipelines; contributing to platform guardrails, governance, compliance, etc\nExperience working with cross-functional partners to develop tools and playbooks for best-practices related to Operations\n\n\nAbout You:\n\n\nYou are curious, tenacious, and proactive\nYou are not bothered by ambiguity but embrace finding patterns in complex environments\nExcellent problem-solving skills and adaptability in a dynamic and evolving tech landscape\nExcited to work in a company that deals with moving and transforming large volumes of data\n\n\nBonus if you have these attributes:\n\n\nExperience with cloud providers like GCP and Azure\nPrior startup experience\nSecurity operations and automation experience"
  },
  {
    "title": "Devops Engineer",
    "company": "PDG Consulting",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pdg-consulting-4321885957?position=18&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=FVf8o0EHKAGfPk8EPYus6A%3D%3D",
    "description": "Overview\n\nWe are seeking a DevOps Engineer to set up, manage, and automate software development operations and processes. The ideal candidate will have strong experience in CI/CD pipelines, cloud service management, and infrastructure monitoring to support efficient, secure, and scalable software delivery.\n\nResponsibilities\n\n\nDesign, implement, and manage CI/CD pipelines to streamline software deployment and integration.\nOversee cloud-based systems and infrastructure management, ensuring reliability and performance.\nAutomate workflows for system administration, documentation, and monitoring.\nSupport the development and deployment of AI chatbot infrastructures and related frameworks.\nCollaborate with developers, QA engineers, and IT teams to optimize the software lifecycle.\n\n\nRequirements\n\n\nMinimum 4 years of experience in ICT systems support, including system administration, documentation, and monitoring.\nHands-on experience with cloud platforms, especially Amazon Web Services (AWS).\nProven experience creating and maintaining AI chatbot infrastructures or similar automation frameworks.\nPrevious experience working within the UN system is an advantage.\nExcellent command of English (required).\nKnowledge of French and Arabic is considered an advantage.\n\n\nPowered by JazzHR"
  },
  {
    "title": "DevOps Engineer - 100% Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-100%25-remote-at-the-dignify-solutions-llc-4347005722?position=19&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=8jAprFfGNBnM2BHTcMQLMg%3D%3D",
    "description": "Summary: The main function of a DevOps Engineer is to design, develop, implement, test, and maintain business and computer applications software or specialized utility programs including mainframe and client/server applications, and major enhancement of existing systems\n\nJob Responsibilities: Fine-tune and improve a variety of sophisticated software implementation projects Gather and analyze system requirements, document specifications, and develop software solutions to meet client needs and data Analyze and review enhancement requests and specifications Implement system software and customize to client requirements Prepare the detailed software specifications and test plans Code new programs to client's specifications and create test data for testing Modify existing programs to new standards and conduct unit testing of developed programs Create migration packages for system testing, user testing, and implementation Provide quality assurance reviews Perform post-implementation validation of software and resolve any bugs found during testing\n\n\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nSAC Experience (1-2 YOE)\nAriba ATHENA Report generation (Some experience)"
  },
  {
    "title": "CloudOps Engineer",
    "company": "Protera",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/cloudops-engineer-at-protera-4336621571?position=20&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=aPEVL2qitI4%2BRVI7GM1JLw%3D%3D",
    "description": "Summary\n\nAs a CloudOps Engineer at Protera, you will play a crucial role in maintaining and optimizing our cloud infrastructure. You will be responsible for monitoring and managing cloud services, ensuring the performance and reliability of our cloud applications, and automating processes to enhance operational efficiency. You will work closely with development teams to improve deployment practices and manage incidents in a fast-paced environment.\n\nKey Responsibilities\n\n\nMonitor cloud infrastructure performance and reliability to ensure optimal service delivery\nAutomate deployment processes using Infrastructure as Code (IaC) tools like Terraform\nImplement and manage CI/CD pipelines to streamline application releases\nCollaborate with development teams to integrate DevOps practices into application lifecycles\nTroubleshoot cloud architecture and application issues to ensure minimal downtime\nConduct security assessments and implement best practices to secure cloud environments\nDocument processes and maintain configurations and operational standards\n\n\nRequirements\n\nSkills & Qualifications\n\nExperience:\n\n\n3+ years of experience in cloud operations, DevOps, or system administration\n\n\nTechnical Skills:\n\n\nProficient with AWS services and cloud architecture\nExperience with Infrastructure as Code (IaC) tools, particularly Terraform\nStrong understanding of containerization technologies like Docker and orchestration tools such as Kubernetes\nFamiliarity with CI/CD tools such as Jenkins, GitLab CI, or similar\nKnowledge of monitoring tools and log management solutions\nSolid troubleshooting skills across cloud-based systems\n\n\nEducation:\n\n\nBachelor's degree in Computer Science, Information Technology, or a related field is preferred\n\n\nCertifications (Preferred):\n\n\nAWS Certified Solutions Architect or related cloud certification\nDevOps or Kubernetes certifications are a plus\n\n\nPersonal Attributes:\n\n\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills\nAbility to work in a fast-paced environment and handle multiple tasks\n\n\nAbout Protera\n\nProtera Technologies (www.protera.com) is a leading provider of total IT outsourcing solutions for SAP-centric organizations. Founded in the mid-1990s, we are pioneers in providing SAP services on the cloud, managing thousands of workloads across various cloud platforms. With headquarters in Chicago and offices in Greece and India, we are committed to delivering exceptional cloud hosting, application management, and professional services globally.\n\nBenefits\n\nProtera offers a variety of health and wellbeing programs. Benefit options include two PPO Medical plans, Dental, Vision, Health Savings Account, Flexible Spending Accounts, Dependent Care FSA, 401k retirement savings plan, company paid Life Insurance, Flexible PTO policy, Paid Holidays."
  },
  {
    "title": "DevOps Systems Engineer",
    "company": "TensorWave",
    "location": "Las Vegas, NV",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-tensorwave-4338727303?position=21&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=MD1xhrPb5O0wx%2FPXmQLhCg%3D%3D",
    "description": "At TensorWave, we‚Äôre leading the charge in AI compute, building a versatile cloud platform that‚Äôs driving the next generation of AI innovation. We‚Äôre focused on creating a foundation that empowers cutting-edge advancements in intelligent computing, pushing the boundaries of what‚Äôs possible in the AI landscape.\n\nAbout The Role\n\nWe are seeking a highly skilled DevOps & Infrastructure Management Engineer to join our growing infrastructure team. This role is ideal for someone who thrives in hardware-centric environments, enjoys hands-on datacenter and system administration work, and can build reliable automation around large-scale infrastructure. You will be responsible for managing enterprise hardware, monitoring systems, network operations, infrastructure automation, and supporting our compute clusters across multiple data centers.\n\nThis role touches every layer of modern infrastructure‚Äîfrom bare metal provisioning, to OS and Kubernetes management, to monitoring and troubleshooting hardware. If you are detail-oriented, resourceful, and comfortable working with both low-level hardware systems and higher-level DevOps tooling, we‚Äôd love to talk.\n\nKey Responsibilities\n\nHardware & Infrastructure Management\n\n\nManage and maintain enterprise-grade server hardware and infrastructure components.\nUtilize out-of-band management systems (iLO, iDRAC, IPMI, Redfish, etc.) for remote operations.\nUse automated hardware management tools (BMC/Redfish-based) to streamline provisioning and maintenance.\nPerform hardware diagnostics and troubleshooting (CPU, memory, disks, PSUs, NICs, etc.).\nHandle vendor interactions, including RMAs, part replacements, and inventory tracking.\nOversee datacenter hardware operations, including racking, cabling, PDU installation, and physical layout.\n\n\nDatacenter & DCIM\n\n\nUse Data Center Infrastructure Management (DCIM) tools for inventory, capacity planning, and environmental tracking.\nManage power delivery and consumption across racks and nodes.\nConfigure and monitor managed PDU systems for power cycling, monitoring, and alerts.\nCollaborate with colocation providers on connectivity, power, security, and maintenance tasks.\n\n\nMonitoring & Observability\n\n\nBuild and maintain infrastructure monitoring and alerting using tools such as Prometheus/Grafana, SNMP, Nagios, CheckMK, or similar platforms.\nImplement automated alerting for hardware health, network status, power issues, and service-level metrics.\nCreate dashboards to give internal teams visibility into system performance and reliability.\n\n\nNetwork Operations\n\n\nManage and configure firewalls, routing, and network segmentation.\nConfigure and troubleshoot VPN technologies (IPsec, OpenVPN, WireGuard).\nOversee subnetting, IP address allocation, and network architecture planning.\nConfigure managed switches, VLANs, port settings, and trunking.\nManage NAT, port forwarding, and related gateway/edge network configurations.\n\n\nSystem Administration (Linux)\n\n\nInstall, configure, and manage Linux servers (Ubuntu/Debian preferred).\nPerform system-level troubleshooting (boot issues, login problems, service failures).\nManage networking configuration (static IPs, DHCP).\nConfigure and maintain filesystems: partitioning, MD RAID, ext4/XFS, LVM, resizing/growing volumes.\nImplement secure access using public key authentication and proper SSH hardening.\nManage certificates for internal systems, including issuance, revocation, HTTPS installation, and rotation.\nHandle basic BIOS configuration relevant to bare metal provisioning or system bring-up.\n\n\nBare Metal Provisioning\n\n\nDeploy and manage hardware provisioning tools such as MAAS, Foreman, or similar systems.\nConfigure and troubleshoot network boot mechanisms (PXE, UEFI Boot, HTTP Boot).\nAutomate provisioning pipelines to rapidly bring new nodes online.\n\n\nContainerization & Orchestration\n\n\nWork with Kubernetes clusters at a foundational level (cluster access, basic resource troubleshooting).\nDeploy workloads using Helm charts and maintain cluster application lifecycle.\nAssist with cluster scaling, node replacements, and security hardening.\n\n\nAutomation & Scripting\n\n\nWrite shell scripts (bash) for automation of system tasks, monitoring, or provisioning.\nUse CLI tooling such as jq, sed, awk, grep, and rsync.\nOptionally automate workflows using languages like Python, Go, PHP, or Perl.\n\n\nRequired Qualifications\n\n\nProven experience managing enterprise-grade hardware at scale.\nStrong understanding of out-of-band management systems (IPMI/BMC/Redfish).\nHands-on expertise with monitoring systems (Prometheus, Grafana, SNMP, Nagios, CheckMK, or similar).\nSolid knowledge of network administration, including firewalls, routing, VPNs, NAT, and managed switches.\nLinux system administration experience (installation, configuration, troubleshooting).\nExperience with filesystems, RAID, partitioning, and general storage management.\nFamiliarity with certificate management, key-based auth, and basic cryptographic functions.\nExperience with bare metal provisioning (MAAS, Foreman, or similar).\nUnderstanding of PXE/UEFI/HTTP boot systems.\nAbility to write functional, maintainable bash scripts for automation.\n\n\nNice to Have\n\n\nExperience with Kubernetes beyond the basics (operators, cluster scaling, CRDs).\nExperience with Helm chart customization.\nFamiliarity with automation languages such as Python, Go, PHP, or Perl.\nPrevious datacenter operations or colocation management experience.\nExposure to high-availability or distributed compute environments.\nKnowledge of infrastructure security and hardening practices.\n\n\nWhat We Bring\n\n\nStock Options\n100% paid Medical, Dental, and Vision insurance\nLife and Voluntary Supplemental Insurance\nShort Term Disability Insurance\nFlexible Spending Account\n401(k)\nFlexible PTO\nPaid Holidays\nParental Leave\nMental Health Benefits through Spring Health"
  },
  {
    "title": "DevOps Administrator",
    "company": "The Amatriot Group",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-administrator-at-the-amatriot-group-4310974393?position=22&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=JksOKcArCujA8xpxKxxJPg%3D%3D",
    "description": "DevOps Administrator\n\nSalary: $135,000 ‚Äì 170,000\n\nContract Length: 12-month SOW\n\nLocation: Dallas, TX - in-office presence requirement 3 days weekly or more as needed\n\n\nThis represents the potential salary range for this position depending on education level, years of experience and/or certifications in addition to other position specific requirements which may impact salary\n\n\nWe‚Äôre seeking an experienced Administrator to join our Code Management team. The right candidate for this role will lead and execute strategic migrations, optimize CI/CD workflows, and drive infrastructure modernization. They will be critical in moving our automation ecosystem from legacy tools (Jenkins, Bitbucket, Automic) to GitLab, Ansible Automation Platform and Terraform, ensuring robust, scalable, and secure pipelines.\n\nRequired Skills And Experience\n\n\n8+ years of experience in Administering different & complex applications and tools used in the Enterprise\nExperience administering GitLab, Artifactory, Xray, & SonarQube\nExperience with infrastructure-as-code tools (Terraform, Ansible, etc.)\nSolid understanding of containerization (Docker) and orchestration (Kubernetes)\nFamiliarity with cloud platforms (AWS, Azure, IBM Cloud) and cloud-native tooling\nStrong communication skills and a track record of cross-team collaboration\nKnowledge of JFrog Artifactory, BitBucket / GIT, SVN and other SCM tools\nWorking knowledge of different Software Development Lifecycle Methodologies\nKnowledge of desired state configuration, automated deployment, continuous integration, and release engineering tools like Puppet, Chef, Jenkins, Bamboo, Maven, Ant etc\nConfigure and manage GitLab Runners, Groups, Projects, and Permissions at scale\nHarden GitLab for enterprise usage (SAML/SSO, LDAP, RBAC, backup/restore)\nDesign, implement, and optimize complex GitLab CI/CD pipelines using YAML best practices\nLeverage Terraform, Ansible, or similar to provision and manage self-hosted GitLab and runners\nImplement GitOps practices to manage infrastructure and environment configurations\nAutomate operational tasks and incident remediation via pipelines and scripts\nPartner with application teams to onboard them onto GitLab workflows and best practices\nDevelop and maintain clear runbooks, wiki pages, and pipeline templates\nIntegrate monitoring (Prometheus/Grafana, ELK) for GitLab health and pipeline performance\nImplement policies and guardrails to ensure code quality, compliance, and security posture\nTroubleshoot and resolve CI/CD or migration-related incidents in a timely manner\nAvailable for 24/7 On-call support\n\n\nPreferred\n\n\nA BS in Computer Science or equivalent work experience with good scripting/programming skills\nGitLab Certified Administrator\nPrior software experience with build management, configuration management and/or quality testing\nExperience with SCM practices including Agile, continuous integration (CI) and continuous deployment (CD)\n\n\nTeam Culture\n\nOur team is fast paced, fun, highly energetic, motivated and hardworking. We expect our candidates to be integrated into our results-driven and solution-oriented culture from the get-go. Our team attains high-quality results on challenging projects; the belief that outcomes are linked to one's effort rather than chance and the tendency to personally set challenging yet realistic goals."
  },
  {
    "title": "DevOps Engineer",
    "company": "Arize AI",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-arize-ai-4332964631?position=23&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=dY4IGRO678a23dhnVYp3qw%3D%3D",
    "description": "About Arize\n\nAI is rapidly transforming the world. As generative AI reshapes industries, teams need powerful ways to monitor, troubleshoot, and optimize their AI systems. That‚Äôs where we come in. Arize AI is the leading AI & Agent Engineering observability and evaluation platform, empowering AI engineers to ship high-performing, reliable agents and applications. From first prototype to production scale, Arize AX unifies build, test, and run in a single workspace‚Äîso teams can ship faster with confidence.\n\nWe‚Äôre a Series C company backed by top-tier investors, with over $135M in funding and a rapidly growing customer base of 150+ leading enterprises and Fortune 500 companies. Customers like Booking.com, Uber, Siemens, and PepsiCo leverage Arize to deliver AI that works.\n\nThe Team\n\nOur On-Prem engineering team is responsible for the deployment of Arize in customer environments. In addition to working with customers in defining infrastructure requirements, the team designs and develops software and tooling that enables the management of these systems at large scale. The On-Prem team has grown to be expert in Kubernetes and cloud deployment on GCP, Azure, and AWS as well as dealing with networking and security aspects of on-premise deployments. The team is dynamic and relies on few talented individuals with a high degree of autonomy and initiative.\n\nWhat You‚Äôll Do\n\n\nWork hands-on with the infrastructure that supports our distributed & highly scalable services in both SaaS and on-prem offerings\nGather requirements from customers and adapt manifests and software to support new environments\nUse and augment monitoring tools to observe platform health, ensure performance and reliability\nInteract with the product team to test new features and package new on-prem releases\nAutomate and optimize the release pipeline to make it as frictionless as possible\nExhibit continuous curiosity for emerging technology that could solve our challenges\n\n\nWhat will set you apart:\n\n\n3+ years of experience as a DevOps Engineer, Cloud Engineer, Infrastructure Engineer or similar\nExcellent communication skills and ability to work directly with customers to understand and address their infrastructure needs\nExperience and fluency in Kubernetes\nA self starter with an ability to thrived in a fast paced environment\nExperience working with multiple cloud providers (AWS, GCP, Azure) and understanding how to adapt cloud-native architectures for on-premises environments\nStrong troubleshooting skills\n\n\nThe estimated annual salary for this role is between $100,000 - $185,000, plus a competitive equity package. Actual compensation is determined based upon a variety of job related factors that may include: transferable work experience, skill sets, and qualifications. Total compensation also includes a comprehensive benefit package, including: medical, dental, vision, 401(k) plan, unlimited paid time off, generous parental leave plan, and others for mental and wellness support.\n\nWhile we are a remote-first company, we have opened offices in New York City and the San Francisco Bay Area, as an option for those in those cities who wish to work in-person. For all other employees, there is a WFH monthly stipend to pay for co-working spaces.\n\nMore About Arize\n\nArize‚Äôs mission is to make the world‚Äôs AI work‚Äîand work for people.\n\nOur founders came together through a shared frustration: while investments in AI are growing rapidly across every industry, organizations face a critical challenge‚Äîunderstanding whether AI is performing and how to improve it at scale.\n\nLearn more about what we're doing here:\n\nhttps://techcrunch.com/2025/02/20/arize-ai-hopes-it-has-first-mover-advantage-in-ai-observability/\n\nhttps://arize.com/blog/arize-ai-raises-70m-series-c-to-build-the-gold-standard-for-ai-evaluation-observability/\n\nDiversity & Inclusion @ Arize\n\nOur company's mission is to make AI work and make AI work for the people, we hope to make an impact in bias industry-wide and that's a big motivator for people who work here. We actively hope that individuals contribute to a good culture\n\n\nRegularly have chats with industry experts, researchers, and ethicists across the ecosystem to advance the use of responsible AI\nCulturally conscious events such as LGBTQ trivia during pride month\nWe have an active Lady Arizers subgroup"
  },
  {
    "title": "DevOps Engineer",
    "company": "Sustainment",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-sustainment-4335637240?position=24&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=oMmcvMJrVjSze0mKTHDcUg%3D%3D",
    "description": "Company Overview: Sustainment is an AI-native software platform that helps US-based manufacturers easily find and work with the critical suppliers they need to build and manage their supply chains. Our vision is to reimagine American manufacturing as a hyperconnected, secure, and resilient ecosystem of local and regional suppliers who can more easily connect, interact, and do business with the industry and government customers that rely on them. We are a dual-use technology platform that supports both DoD and commercial customers in pursuit of our vision.\n\nJob Overview: We are looking for a DevOps/MLOps Engineer to drive the reliability, scalability, and performance of our AI-native procurement platform. The primary focus of the role is to build and maintain robust infrastructure, automate ML model deployment pipelines, and ensure database performance and reliability. You will be responsible for high-quality, secure deliverables that meet stringent compliance requirements (SOC 2, FedRAMP, CMMC Level 2) and for helping to create, evangelize, and enforce the standards necessary to meet team and company goals for operational excellence and mission-critical uptime.\n\nResponsibilities:\n\n\nBuild partnerships and work collaboratively with engineering, AI, and product teams to meet shared objectives\nOperate effectively in ambiguous situations, especially when scaling AI workloads and managing complex infrastructure transitions\nBuild and optimize DevOps pipelines including ML model training, versioning, deployment, monitoring, and retraining workflows\nAdminister and optimize PostgreSQL databases including performance tuning, query optimization, backup/recovery, and high availability configurations\nTroubleshoot and resolve infrastructure, database, and pipeline issues in a resilient, performant manner\nImplement and maintain infrastructure as code using tools like Terraform or Cloudformation\nMonitor system health, performance, and database metrics using observability tools and respond to alerts proactively\nEnsure security best practices and compliance requirements are met across all infrastructure and database layers\nParticipate in multi-resource projects in an agile environment\nEvaluate and recommend industry standards, tools, and methods for DevOps, MLOps, and database management\nDocument infrastructure architecture, runbooks, and contribute to architecture reviews\n\n\nQualifications:\n\n\nBachelor's degree (computer science, engineering, or related) or equivalent work experience\n2+ years of experience with cloud infrastructure (AWS preferred), container orchestration (Kubernetes), and CI/CD tools\n2+ years of database administration experience with PostgreSQL or similar relational databases\nExperience with ML model deployment, monitoring, and lifecycle management (MLOps)\nStrong understanding of infrastructure as code (Terraform), GitOps practices, and declarative configuration management\nExperience with security compliance frameworks (SOC 2, FedRAMP, or CMMC is a plus)\nProduct-driven mindset with deep empathy for internal developer experience and system reliability\nStrong desire to work in a startup with interest to take on projects from zero to one with collaboration with the rest of the team\nLove working hard and enjoy a fast-paced, ambiguous environment\nExperience with distributed systems, microservices architecture, and reactive systems\nOpen mindset to exploring new tools and frameworks in the rapidly evolving DevOps/MLOps landscape\nPassion for operational excellence and automation\nExperience supporting cross-team efforts to roll out new infrastructure capabilities or ML features\nPassion for learning and continuous improvement\nStrong written and verbal communication skills, and ability to explain complex technical concepts\nExperience working in a Scrum/agile environment\nExperience with AWS GovCloud, defense/government sector compliance, or working in an early startup environment on SaaS products is a plus\n\n\nCore Technologies:\n\n\nAWS (including GovCloud), Kubernetes, Docker, Terraform\nPostgreSQL\nGitLab CI/CD, ArgoCD, Tilt\nModel versioning, experiment tracking, ML pipeline orchestration\nDatadog, CloudWatch\nPython, Bash, experience with .NET ecosystem a plus\nIAM, secrets management, encryption, audit logging, compliance automation\n\n\nSustainment offers a competitive benefits package for full time employees including medical, dental, vision, paid time off, company holidays, and 401K matching.\n\nSustainment is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.\n\nApplicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.\n\nSustainment participates in E-Verify."
  },
  {
    "title": "Devops",
    "company": "The Dignify Solutions, LLC",
    "location": "Phoenix, AZ",
    "link": "https://www.linkedin.com/jobs/view/devops-at-the-dignify-solutions-llc-4347025595?position=25&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ce%2FoReYTlt5dTX23pQI90g%3D%3D",
    "description": "Must have is\n\n\nAssociate should be in Phoenix from day 1 of the project\nAt least 5 years of experience in Devops area.\nStrong skill in CI/CD pipeline, Jenkins, Github\nAdditional knowledge on any build related tools is an added advantage.\n\n\nJava 8 knowledge\n\nDocker\n\nKaffka\n\nKibana"
  },
  {
    "title": "DevOps Engineer I",
    "company": "Trustwell",
    "location": "Portland, OR",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-i-at-trustwell-4321600458?position=26&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ulG1HPjSCU5dZ4lcM3Sw5w%3D%3D",
    "description": "Role{{:}} DevOps Engineer I\n\nFLSA{{:}} Full Time | Exempt | Salaried | Remote\n\nReports to{{:}} Director of DevOps\n\nNote{{:}} Candidate preferred to reside in PST. If not, candidate will be required to support PST working hours.\n\nTrustwell is looking for ambitious, energetic problem-solvers who enjoy a fast-paced team environment filled with challenges and career growth opportunities in a rapidly growing tech firm. Trustwell is on a mission to change the food industry. Combining FoodLogiQ's supply chain management software with Genesis' nutritional analysis and label development solution, the Trustwell Connect platform creates the food industry's only full-scale solution connecting product development and regulatory-compliant labeling with supplier compliance, enhanced traceability, and automated recall management. From food and supplement manufacturers to retail grocers and restaurant chains, more than 2,500 food companies around the world use Trustwell software as their trusted source for compliance and quality solutions in the food industry. For more information, visit www.trustwell.com.\n\nScope of Position{{:}} The DevOps Engineer I will be part of a dynamic and agile team responsible for building and maintaining the platforms, systems, and services that power our customer-facing products. Working closely with Software Engineers and Engineering Leadership, you'll help shape architecture, implement best practices, and stay ahead of the curve in DevOps principles. You'll champion operational excellence and continuous improvement across the team.\n\nEssential Duties & Responsibilities include but not limited to{{:}}\n\n\nContribute actively to an Agile delivery team, ensuring consistent, high-quality, and reliable software releases.\nCollaborate closely with developers and cross-functional partners to design, build, and deploy best-in-class, scalable software solutions.\nChampion an automation-first, code-centric mindset, driving efficiency and consistency across deployment, monitoring, and maintenance processes.\nSupport production operations through participation in incident response, troubleshooting, and on-call rotations to maintain system reliability and uptime.\nDevelop, implement, and maintain monitoring and alerting tools to ensure optimal application performance, health, and availability.\nDesign infrastructure and deployment solutions with scalability, resilience, and long-term maintainability as core principles‚Äîavoiding short-term workarounds.\nProactively identify and eliminate operational bottlenecks and unnecessary complexity, contributing to continuous improvement initiatives.\nEngage in architectural reviews and solution design discussions, providing input that enhances performance, reliability, and security.\nPerform other related duties as assigned, contributing to the overall success of the DevOps function and technology organization.\n\n\nEducation/Experience{{:}}\n\n\nBachelor's degree in Computer Science, Engineering, or a related field required. Will consider relevant experience/certifications in lieu of degree.\n2+ years of experience in an SRE (Site Reliability Engineer) or equivalent engineering role\n2+ years of experience as a DevOps Engineer or in a similar capacity\n3+ years of hands-on experience managing and supporting production cloud environments (AWS, Azure, or GCP)\nExtensive experience with DataDog, including APM, RUM, Synthetic Monitoring, Infrastructure Monitoring, and Dashboard development\n\n\nRequired Skills/Abilities{{:}}\n\n\nStrong, hands-on experience with Infrastructure-as-Code (IaC) and configuration management tools such as Terraform, CloudFormation, and/or Ansible\nProven experience designing and managing cloud architectures in AWS and Microsoft Azure, with expertise in containerization and orchestration (Docker, Kubernetes, etc.)\nDemonstrated experience building, maintaining, and optimizing CI/CD pipelines using tools such as CircleCI, TeamCity, GitHub Actions, or Jenkins\nBackground in delivering infrastructure initiatives within an Agile development environment\nCollaborative mindset with the ability to partner effectively across cross-functional teams to achieve shared goals\nStrong \"automation-first\" mindset with a focus on scalability, reliability, and efficiency\n\n\nTotal Rewards Package{{:}}\n\n\nFull healthcare benefits, including medical, dental, and vision.\nSupplemental benefits, including STD, LTD, HSA, 401k, etc.\nResponsible Time Off (PTO) + Holiday Pay\nExcellent culture, growth opportunities, plus much more...\n\n\nWhat to expect - the Hiring Process!\n\n\nInterview with Human Resources\nInterview with Hiring Manager\nPeer Panel Interview(s)\nOffer of Employment (Background Screening/References)\n\n\nHiring Eligibility{{:}} This is a fully remote position open to candidates located anywhere within the United States. Eligibility to work remotely is subject to company policy and applicable state laws. Candidates must have work authorization to work for any U.S. based employer. Please note that certain benefits, taxes, or employment terms may vary by state.\n\nCompensation{{:}} The compensation for this position starts at $80,000 per annum, with the potential for higher placement based on a candidate's experience, education, and overall qualifications. In addition to base salary, this role is bonus eligible‚Äîup to 10% annually, contingent on company performance and achievement of organizational objectives.\n\nTrustwell is an equal employment opportunity employer committed to hiring and retaining a diverse workforce. Applicants receive fair and impartial consideration without regard to race, sex, sexual orientation, gender identity, color, religion, national origin, age, disability, veteran status, religion, or other legally protected class. If you need accommodation for any part of the employment process due to a medical condition, or any disability, please contact a member of our human resources team.\n\nAcceptable Background and References Required; Upon any conditional offers made by Trustwell.\n\nEqual Opportunity Employer/ DFWP/ Affirmative Action"
  },
  {
    "title": "DevOps Engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4338475165?position=27&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=0OJH81O8zlAtsvYYki8qRA%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Remote\n\nEmployment Type: Full-Time\n\nJob Summary\n\nWe are looking for a skilled DevOps Engineer to join our technology team. The ideal candidate will design, implement, and manage CI/CD pipelines, automate infrastructure, and ensure smooth deployment processes across development and production environments. This role requires strong knowledge of cloud platforms, containerization, and scripting.\n\nKey Responsibilities\n\n\nDesign, build, and maintain CI/CD pipelines for application deployment.\nAutomate infrastructure provisioning using tools like Terraform or Ansible.\nManage containerized environments using Docker and Kubernetes.\nMonitor system performance and implement proactive solutions for scalability and reliability.\nCollaborate with development and operations teams to streamline workflows.\nEnsure security and compliance in cloud and on-prem environments.\nTroubleshoot and resolve issues in production and staging environments.\n\n\nQualifications\n\n\nBachelor's degree in Computer Science, Engineering, or related field.\n25 years of experience in DevOps or related roles.\nProficiency in cloud platforms (AWS, Azure, GCP).\nHands-on experience with CI/CD tools (Jenkins, GitLab CI, GitHub Actions).\nStrong knowledge of containerization (Docker, Kubernetes).\nFamiliarity with Infrastructure as Code (Terraform, Ansible).\nScripting skills in Python, Bash, or similar languages.\n\n\nPreferred Skills\n\n\nExperience with monitoring tools (Prometheus, Grafana).\nKnowledge of security best practices in DevOps.\nFamiliarity with microservices architecture.\n\n\nBenefits\n\n\nCompetitive salary and performance bonuses.\nHealth insurance and retirement plans.\nFlexible work options and professional development opportunities."
  },
  {
    "title": "DevOps Engineer (JIRA)",
    "company": "Rubix Solutions",
    "location": "Washington, DC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-jira-at-rubix-solutions-4335995983?position=28&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=44llbql724e3TZr%2F1o2aBA%3D%3D",
    "description": "The DevOps Engineer (specifically Jira Platform Engineer) will serve as the technical owner of our Jira SaaS in government cloud, and is responsible for administering, configuring, and integrating the application within our enterprise technology ecosystem. This position plays a pivotal role in consolidating our collaborative planning tools‚Äîtransitioning from Azure DevOps (on-prem) and GitLab (on-prem) to Jira‚Äîwhile enabling and scaling Agile practices across the organization.\n\nThe ideal candidate is both technically proficient and strategically minded, with a strong understanding of Agile methodologies, DevSecOps workflows, and enterprise system\n\nintegration.\n\nResponsibilities\n\n\nAdminister, configure, and optimize Jira SaaS to meet enterprise project management and Agile delivery needs.\nDesign and maintain custom workflows, issue types, screens, fields, and automation rules aligned with organizational Agile frameworks and guidelines.\nManage user permissions, group roles, and security schemes to ensure governance and compliance.\nMonitor Jira license utilization, user growth, and application usage to ensure efficient use of subscriptions.\nCollaborate with procurement teams to support renewal, optimization, and budget decisions.\nDesign, implement, and maintain seamless integrations between Jira with other enterprise systems, such as GitLab (source control & CI/CD), and ServiceNow (ITSM), using Okta,REST APIs, webhooks, middleware, and scripting.\nAutomate data synchronization across platforms to support traceability from planning to release.\nTroubleshoot and optimize integration pipelines to ensure performance, security, and Scalability.\nPartner with infrastructure and cybersecurity teams to align integrations with enterprise security and compliance standards.\nDevelop and maintain technical documentation, standards, and best practices.\nPartner with the Agile Transformation Office to translate Agile practices into effective Jira configurations and usage patterns.\nProvide technical guidance and mentoring to Scrum Masters, Product Owners, and teams on best-practice tool utilization.\nSupport reporting and analytics initiatives, ensuring reliable Agile metrics (velocity, burndown, cycle time, etc.).\n\n\nRequirements\n\n\nMust be able to obtain and maintain Moderate Risk Public Trust (MRPT) facility credentials/authorization. Note: US Citizenship is required for MRPT facility credentials/authorization at this work location.\nBachelor‚Äôs degree in Computer Science, Information Systems, or a related field (or equivalent experience).\n3+ years of hands-on experience administering and/or engineering Jira (Self-hosting or SaaS).\nProven, hands-on experience developing custom integrations with enterprise platforms, especially GitLab and ServiceNow.\nProficiency in scripting (Python, PowerShell, or JavaScript) and REST API integration.\nStrong understanding of Agile methodologies (Scrum, Kanban, SAFe) and DevSecOps principles.\nExperience in enterprise migrations from Azure DevOps or similar tools to Jira.\nFamiliarity with Atlassian ecosystem (Confluence, Bitbucket) and marketplace apps.\nExperience working in a DevSecOps or Platform Engineering environment.\nExperience with information security, privacy, and risk assessment standards including FISMA, SOX, FedRAMP, etc. is preferred.\nFederal government experience is preferred.\nAtlassian Certified Professional (ACP-620, ACP-120, or equivalent) preferred."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4291046434?position=29&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=Qc5sYwffI5j1898EaaRwMw%3D%3D",
    "description": "About Chartmetric\n\nChartmetric, Inc. is a 10-year-old startup specializing in music data analytics. We are trusted by Universal MusicGroup, Sony, Warner, and Apple Music, as well as hundreds of other music companies and industry professionals. Our team has created a self-service data dashboard for the music industry to better understand the activity happening around artists. Together, we combine hundreds of thousands of real-time data points across iTunes, Spotify, YouTube, Google, Amazon, X, and others through our beautifully designed tool in order to make sense of the increasingly complex landscape of the music industry.\n\nAbout The Role\n\n\nWe are seeking a talented DevOps / Developer Experience Engineer to join our team and play a pivotal role in enhancing our development infrastructure and streamlining the developer workflow. This position combines traditional DevOps responsibilities with a focus on creating exceptional developer experiences through tooling, automation, and process optimization.\n\n\nWhat You'll Do\n\n\nInfrastructure & Operations\nDesign, implement, and maintain scalable cloud infrastructure using Infrastructure as Code (IaC) principles\nManage CI/CD pipelines and deployment processes across multiple environments\nMonitor system performance, reliability, and security, implementing proactive solutions\nAutomate operational tasks and eliminate manual toil through scripting and tooling\nEnsure high availability and disaster recovery capabilities\n\n\nDeveloper Experience\nBuild and maintain internal developer tools and platforms that improve productivity\nStreamline onboarding processes for new developers and reduce time-to-first-commit\nDesign and implement developer-friendly APIs, SDKs, and documentation\nCreate self-service capabilities that reduce dependencies and waiting times\nGather feedback from development teams and iterate on tooling based on pain points\n\n\nCollaboration & Process Improvement\nWork closely with engineering teams to understand workflow challenges and requirements\nChampion best practices for code deployment, testing, and monitoring\nLead initiatives to improve development velocity and reduce friction\nParticipate in incident response and post-mortem analysis\nMentor team members on DevOps practices and tooling\n\nWhat We're Looking For\n\n\nTechnical Skills\n3+ years of experience in DevOps, SRE, or Platform Engineering roles\nStrong proficiency with cloud platforms (AWS, GCP, or Azure)\nExperience with Infrastructure as Code tools (Terraform, CloudFormation, or Pulumi)\nHands-on experience with containerization (Docker) and orchestration (Kubernetes)\nProficiency in CI/CD tools (Jenkins, GitLab CI, GitHub Actions, or similar)\nStrong scripting skills in Python, Bash, or Go\nExperience with monitoring and observability tools (Prometheus, Grafana, ELK stack, or similar)\n\n\nDeveloper Experience Focus\nExperience building internal tools and platforms for development teams\nUnderstanding of software development lifecycle and common developer pain points\nFamiliarity with API design and developer-facing documentation\nExperience with version control systems and Git workflows\nKnowledge of testing frameworks and quality assurance processes\n\n\nSoft Skills\nStrong problem-solving abilities and analytical thinking\nExcellent communication skills and ability to work with cross-functional teams\nCustomer-focused mindset with emphasis on developer productivity\nProactive approach to identifying and resolving issues\nAbility to balance technical debt with feature delivery\n\n\nPreferred Qualifications\nKnowledge of security best practices and compliance frameworks\nBackground in software development or engineering\nFamiliarity with cost optimization strategies for cloud infrastructure\nPrevious experience in a high-growth or scaling environment\n\nWhat We Offer\n\n\nCompetitive salary and equity package\nComprehensive health, dental, and vision insurance\nOpportunity to shape developer experience across the organization\nAccess to cutting-edge tools and technologies\n\n\nTeam Culture\n\n\nWe believe that great developer experiences lead to better products and happier teams. Our DevOps/DX team operates as enablers and force multipliers, working collaboratively to remove friction from the development process. We value automation, measurement, and continuous improvement, always asking \"how can we make this better for our developers?\"\n\n\nThe Pay Range For This Role Is\n\n135,000 - 165,000 USD per year(San Mateo)\n\n120,000 - 150,000 USD per year(New York)"
  },
  {
    "title": "DevOps Engineer",
    "company": "Broad Reach Partners",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-broad-reach-partners-4303987210?position=30&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=7H%2FzhrS%2FCLfPsBUshtgFHQ%3D%3D",
    "description": "We are seeking a Senior DevOps Engineer to join our team and play a critical role in designing, building, and optimizing the CI/CD pipelines that power our software delivery across on-prem and cloud environments.\n\nIn this role, you will work hand-in-hand with our development, operations, and security teams worldwide to implement best practices, automate deployments, and ensure our platforms are reliable, secure, and scalable. If you thrive on solving complex technical challenges, have a passion for automation, and want to influence how enterprise platforms evolve and modernize, this is an ideal opportunity for you.\n\nAs a Senior DevOps Engineer, your expertise will drive the continuous integration, delivery, and deployment (CI/CD) pipelines delivering software to both on-prem and cloud (AWS primarily) environments. You will work closely with our development, operations, and security teams distributed across the globe. This role requires a deep understanding of DevSecOps best practices and a strong ability to troubleshoot complex issues.\n\nYour Responsibilities In This Role Will Include\n\n\nDesign, Develop and Maintain automated build and deployment pipelines using GitLab/GitHub/Jenkins to enhance software delivery.\nIdentify opportunities for automation and ensure continuous security, quality in application development by automating security checks, test executions in build and deployment pipelines.\nDeploy and manage Kubernetes workloads to AWS EKS(A) using Helm, ArgoCD\nCollaborate with development, operations and security team to build secure, optimized and efficient pipelines.\nCreate comprehensive documentation on pipeline functionality and provide training to required members.\nProactively monitor system performance and identify potential issues before they become critical.\nParticipate in on-call rotation.\nEngage in continuous learning and actively advocate for Dev(Sec)Ops, GitOps best practices and standards across the team.\n\n\nWe are looking for you to have the following skills and experience:\n\n\n8+ years of experience as a DevOps Engineer, Site Reliability Engineer, or equivalent\nStrong knowledge of DevOps practices, continuous integration, continuous delivery, and related tools.\n3+ years of experience with Amazon Web Services (AWS) or Microsoft Azure\n3+ years of experience with Kubernetes clusters\nProficiency with public cloud environments (AWS preferred)\nExperience with tools like New Relic and Graylog\nAdvanced proficiency working with CI/CD pipelines such as GitHub Actions/GitLab/Jenkins\nExpert in containerization technologies such as Docker and orchestration tools like Kubernetes.\nProficiency in scripting language, like Bash, Groovy, Python\nExcellent debugging and troubleshooting skills.\nAbility to prioritize tasks efficiently and independently under minimal supervision.\n\n\nNice to Have\n\n\nAWS Cloud certification\nFamiliar with .NET applications.\nKnowledge in Terraform, Ansible, monitoring tools\n\n\nWe are located in the Alpharetta/Cumming area of Atlanta and are working in the office several days each week so YOU MUST LIVE WITHIN COMMUTING DISTANCE OF ALPHARETTA, GA to be considered for this role. We cannot sponsor at this time.\n\nIf this opportunity is a good match for your skills, experience and interest, please apply now so we can follow up with you with more details."
  },
  {
    "title": "AWS DevOps Specialist",
    "company": "Focus School Software",
    "location": "St. Petersburg, FL",
    "link": "https://www.linkedin.com/jobs/view/aws-devops-specialist-at-focus-school-software-4333597594?position=31&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ZWjo%2BzvVD2P2SHlsBozjUQ%3D%3D",
    "description": "Focus School Software is a fast-growing school management software company. We thrive on creating some of the most innovative features on the market today, helping educators to meet their evolving needs in classrooms, district management, state reporting compliance, and other facets of student-centered education and technology.\n\nWe are seeking an experienced and proactive AWS DevOps Specialist to join our growing infrastructure team. This role is ideal for someone passionate about automation, cloud infrastructure, and scalable, secure systems. The ideal candidate brings expertise in AWS services, infrastructure as code, cost reduction strategies and DevOps best practices. You will play a key role in improving system performance, reliability, and security, while contributing to CI/CD pipelines and participating in an on-call rotation. This is a great opportunity for anyone who has a multitude of skills in DevOps and System Administration and can wear many hats and loves problem solving.\n\nKey Responsibilities\n\n\nAutomation & Configuration Management\nDesign, develop, and maintain automation using Ansible and Ansible Tower.\nDeploy and configure RHEL based systems.\nDatabase maintenance and performance, routing, pooling and role management.\n\n\nCloud Infrastructure (AWS)\n\n\nArchitect and manage services including RDS (PostgreSQL), EC2, EKS, CloudFormation, CloudFront, GuardDuty, AWS VPN, AWS AD, and more.\nImplement Autoscaling strategies and container orchestration with EC2 Autoscaling or EKS. Continuously monitor performance and feedback on systems.\nMonitor and improve database performance, sharding strategies, and health metrics.\nMonitor backups and maintain recovery point objectives for disaster recovery.\n\n\nSecurity & Compliance\n\n\nSupport SOC II compliance initiatives through infrastructure hardening, monitoring, and alerting.\nLeverage AWS security tools and best practices to ensure compliance and threat mitigation.\n\n\nNetworking & Connectivity\n\n\nManage VPCs, subnets, security groups, VPNs, and endpoint connectivity for both internal and external integrations.\nManaging routes, DNS and VPN connectivity. Help internal users maintain their VPN connections.\nCost Optimization\nAnalyze AWS billing, usage reports, and recommend cost-saving strategies.\n\n\nDevOps & CI/CD\n\n\nBuild and maintain CI/CD pipelines, enabling delivery of automation code from development to production.\nEnsure high availability and zero-downtime deployments through automation and best practices.\nDevelop and maintain local dev environments for developers.\n\n\nCollaboration & Culture\n\n\nWork cooperatively in cross-functional teams, embracing a culture where the best ideas win.\nProactively identify infrastructure problems and lead with creative, scalable solutions.\n\n\nEndpoint & Systems Management\n\n\nOversee and manage end-user systems and infrastructure endpoints to maintain security and stability.\nPatch management and remediation, ensure established timelines and policies are followed.\n\n\nOn-Call Participation\n\n\nParticipate in an on-call rotation to respond to production incidents and infrastructure issues.\n\n\nRequirements\n\n\nAnsible, Ansible Tower, working in RHEL based environments.\nLinux/RHEL expert, be able to design, deploy and fix everything from a systemd service to managing SFTP.\nAbility to manage a Git repository, and perform peer review on automation code.\nMonitoring tools such as Splunk, Grafana or similar.\nWorking knowledge of NGINX, basic webserver stacks.\nUnderstanding of AWS, particularly RDS (PostgreSQL), CloudFormation, EC2, EKS\nKubernetes and containerization\nExperience with database sharding and performance tuning\nCI/CD pipeline design and implementation\nFamiliarity with SOC II compliance frameworks\nExperience managing AWS networking, VPNs, and AWS AD\nSecurity-first mindset with experience using AWS Org, AWS Tower, CloudFront, and related tools to ensure compliance.\nStrong interpersonal skills with a collaborative mindset.\n\n\nNice-to-Have\n\n\nLAMP/LNPP Stack experience\nSVN Familiarity\nActive Directory experience, basic Windows management\nExperience with endpoint management platforms\nBackground in proactive monitoring/observability tooling\nPrior involvement in security audits or compliance initiatives\n\n\nFocus School Software‚Äôs compensation package offers the following benefits:\n\n\nMedical Insurance\nDental/Vision Insurance\nLife Insurance\nShort and Long Term Disability Insurance\n401(k) after 6 months\nPaid Holidays\nPaid Vacation and Sick Time\nRemote Position"
  },
  {
    "title": "DevOps Engineer",
    "company": "Rain",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rain-4318510257?position=32&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=8eZNlx304T4xDlWSQxyR%2Bg%3D%3D",
    "description": "Rain is empowering the next generation of money and financial products globally. We‚Äôre a lean and mighty team of passionate builders and veteran founders. We are looking for a DevOps engineer to join us in building a cutting edge platform at the intersection of real-world payments and digital money. You will have the opportunity to deliver massive impact at a small and quickly growing company that is funded by some of the top investors in fintech and crypto. Rain is backed by great investors including Lightspeed, Norwest, Khosla, along with great companies like Coinbase, Circle, and Uniswap.\n\nMany of our engineers are based in NYC but we are open to fully remote candidates.\n\nOur Ethos\n\nWe believe in an open and flat structure. You will be able to grow into the role that most aligns with your goals. Our team members at all levels have the freedom to explore ideas and impact the roadmap and vision of our company.\n\nWhat You'll Do\n\n\nBe a critical part of the technical infrastructure roadmap\nManage our cloud environments across GCP and AWS\nScale our infrastructure to millions of end users globally\nHelp drive the architectural decisions of a rapidly evolving product\nLead the creation and maintenance of our CI/CD pipelines to enable rapid, reliable deployments\nCollaborate with the engineering team to improve infrastructure performance\nBuild infrastructure to interact with millions of smart contracts across dozens of blockchains\nAutomate security controls and compliance processes to protect sensitive financial data\n\n\nWhat We're Looking For\n\n\nStrong experience with Infrastructure as Code, particularly Terraform, for managing cloud resources at scale\nProven track record designing and implementing CI/CD pipelines and automation workflows\nExperience managing production environments in cloud providers\nExperience with monitoring, logging, and observability tools\n\n\nNice to haves, but not mandatory\n\n\nExperience in fintech (neobank or card issuing experience gets extra brownie points)\nExperience with blockchain infrastructure\n\n\nOur perks enable working at Rain to be a fulfilling, healthy and happy experience.\n\nUnlimited time off üõº Unlimited vacation can be daunting, so at Rain we require our teammates to take 10 days minimum for themselves.\n\nFlexible working ‚òï We support a flexible workplace, if you feel comfortable at home please work from home. If you‚Äôd like to work with others in an office feel free to come in. We want everyone to be able to work in the environment in which they are their most confident and productive selves.\n\nFlexible Benefits üß† Easy-to-access benefits, for all employees based in the US, Rain pays a percentage of your benefits for the employee and for your dependents. We offer comprehensive health, dental and vision plans as well as a 100% company-subsidized life insurance plan.\n\nEquity plan üì¶ On top of a competitive salary, we offer every Rain employee an equity option plan so we can all can benefit from our success.\n\nRain Cards üåßÔ∏è We want our teammates to be knowledgeable about our core products and services and to support this mission we issue a card for our team to utilize the card for testing.\n\nHealth and Wellness üìö High performance begins from within. Our members are welcome to use their company card for eligible health and wellness spending like gym memberships, fitness classes and other wellness items.\n\nTeam summits ‚ú® Summits play an important role at Rain! Time spent together helps us get to know each other, strengthen our relationships, and build a common destiny. Stay tuned for upcoming destinations!"
  },
  {
    "title": "DevOps Engineer",
    "company": "Jasper",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-jasper-4318500931?position=33&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=RpsroDHV7fv9IwKXOTGPxQ%3D%3D",
    "description": "Jasper is the leading AI marketing platform, enabling the world's most innovative companies to reimagine their end-to-end marketing workflows and drive higher ROI through increased brand consistency, efficiency, and personalization at scale.\n\nJasper has been recognized as \"one of the Top 15 Most Innovative AI Companies of 2024\" by Fast Company and is trusted by nearly 20% of the Fortune 500 ‚Äì including Prudential, Ulta Beauty, and Wayfair. Founded in 2021, Jasper is a remote-first organization with team members across the US, France, and Australia.\n\nAbout The Role\n\nWe're looking for an experienced DevOps Engineer to join our Platform team. This is a highly autonomous, high-impact role that blends Ops practices, infrastructure engineering, and delivery pipeline optimization. You'll work with a focused, collaborative, and fast-moving team where your contributions will directly impact system reliability, developer velocity, and our ability to safely deliver AI-powered products at scale. Candidates should also have a solid background in Cloud, IaC, and Kubernetes, and a drive to produce excellent solutions for a variety of challenges.\n\nThis fully remote role reports to the Staff Dev Ops Engineer and is open to candidates located anywhere in the continental US.\n\nWhat You‚Äôll Do\n\n\nDesign, implement, and operate cloud-native infrastructure that scales efficiently, fails gracefully, and optimizes for performance and cost.\nBuild and refine software delivery pipelines to enable safe, fast, and frequent deployments with robust testing, rollback, and progressive release mechanisms.\nDevelop infrastructure-as-code solutions using Terraform and Helm to create self-healing, automated, and observable systems.\nCollaborate with ML and product teams to support AI model training and inference through scalable compute and storage infrastructure.\nIdentify and eliminate single points of failure, performance bottlenecks, and scalability limits through proactive monitoring and reliability engineering practices.\nImplement and enforce security best practices, including secrets management, access control, and compliance across all infrastructure layers.\n\n\nWhat You‚Äôll Bring\n\n\nDeep experience running Kubernetes in production (cluster management, networking, storage, security).\nExpertise with Terraform, Helm, and configuration management to build reproducible, version-controlled infrastructure.\nProven success designing and maintaining CI/CD pipelines (GitHub Actions, Argo CD, Jenkins, etc.) balancing speed and safety.\nStrong background in observability (especially Datadog) ‚Äî skilled at instrumentation, dashboard creation, and intelligent alerting.\nSolid scripting skills in Python, Go, or Bash, with a focus on automation and operational efficiency.\nPractical knowledge of Google Cloud Platform and cloud-native architectures.\nExperience supporting multi-language environments (TypeScript, Python, Go) and AI/ML workloads, including GPU-based compute.\nFamiliarity with container security, secrets management, and policy enforcement.\n(Bonus) History of open source contributions in infrastructure, CI/CD, or observability projects.\n\n\nCompensation Range\n\nAt Jasper, we believe in pay transparency and are committed to providing our employees and candidates with access to information about our compensation practices. The expected base salary range offered for this role is $170,000 - $200,000. Compensation may vary based on relevant experience, skills, competencies, and certifications.\n\nBenefits & Perks\n\n\nComprehensive Health, Dental, and Vision coverage beginning on the first day for employees and their families\n401(k) program with up to 2% company matching\nEquity grant participation\nFlexible PTO with a FlexExperience budget ($900 annually) to help you make the most of your time away from work\nFlexWellness program ($1,800 annually) to help support your personal health goals\nGenerous budget for home office set up\n$1,500 annual learning and development stipend\n16 weeks of paid parental leave\n\n\nOur goal is to be a diverse workforce that is representative at all job levels as we know the more inclusive we are, the better our product will be. We are committed to celebrating and supporting our differences and that diversity is essential to innovation and makes us better able to serve our customers. We hire people of all levels and backgrounds who are excited to learn and develop their skills.\n\nWe are an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state or federal laws.\n\nBy submitting this application, you acknowledge that you have reviewed and agree to Jasper's CCPA Notice to Candidates, available at legal.jasper.ai/#ccpa."
  },
  {
    "title": "DevOps / Systems Engineer",
    "company": "Collate",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-collate-4302854141?position=34&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=sVfStPLkpJJQG0HBPuEguw%3D%3D",
    "description": "About Collate\n\nCollate is an AI document generation platform for life sciences. We automate paperwork with AI, helping our customers get life-saving innovations to patients years faster. Collate is an end-to-end solution, powering every step of drug, diagnostic, and medical device development‚Äîfrom concept to market.\n\nOur CEO Surbhi Sarna is a former General Partner at Y Combinator. Surbhi founded nVision Medical, which developed a new method to detect ovarian cancer and was acquired by Boston Scientific. Our CTO Nate Smith is a former Visiting Partner at Y Combinator and founder of Lever. Our AI researchers, engineers, and designers have worked at Google, Nvidia, Meta, Netflix, Amazon, AirBnB, Hippocratic AI, and Grail, and 40% of our team are former founders.\n\nWe‚Äôre a small, elite team, with over $30M in seed funding from top investors (Redpoint, First Round Capital, Conviction, and Y Combinator) and leaders in healthcare and AI. This is a rare chance to join at the ground floor of a company with world-changing potential, experienced founders, and resources to execute at scale.\n\nAbout The Role\n\nWe‚Äôre looking for a DevOps / Systems Engineer to own the infrastructure that powers Collate‚Äôs products. You‚Äôll build the systems and tooling that keep our platform reliable, secure, and fast as we scale.\n\nThis role is broad by design ‚Äî from managing CI/CD pipelines and cloud infrastructure to handling light security responsibilities like certificate management. You‚Äôll partner closely with backend, AI, and product engineers to ensure our systems are both easy to develop on and safe to deploy at scale.\n\nAt Collate, infrastructure isn‚Äôt just about uptime ‚Äî it‚Äôs about trust. The work you do will help ensure that the AI we build for healthcare runs with reliability and security in mind.\n\nWhat You‚Äôll Do\n\n\nDesign and maintain cloud infrastructure to support Collate‚Äôs products as we grow from prototypes to production scale\nDevelop CI/CD pipelines and automation that accelerate developer velocity and reduce operational friction\nManage core system reliability, including monitoring, logging, and incident response\nTake on light security responsibilities, such as handling certificates, secrets management, and supporting compliance needs\nCollaborate closely with engineering teams to design infrastructure that balances speed, safety, and scale\nContinuously improve internal tooling and workflows, helping the team move faster with confidence\nLeverage tooling including AWS, Terraform, Kubernetes, Helm, ArgoCD, Grafana, and Github Actions\n\n\n\nWhat We‚Äôre Looking For\n\n\nHands-on experience with cloud infrastructure (AWS, GCP, or similar) and modern DevOps practices\nProficiency with infrastructure-as-code and CI/CD tooling\nFamiliarity with monitoring, observability, and incident management\nInterest or experience in light security work, including certificates, secrets management, or compliance support\nA pragmatic approach: able to balance iteration speed with building for long-term reliability\nMotivation to work in an early-stage startup where your infrastructure decisions shape the foundation of the company\n\n\n\nWhy Join Collate?\n\nImpact: Build systems and experiences that touch real patients and providers, improving healthcare outcomes.\n\nOwnership: Shape both our product experience and our engineering culture from the start.\n\nLearning: Collaborate with a uniquely interdisciplinary team‚ÄîAI researchers, healthcare leaders, and experienced startup builders.\n\nUpside: Join a company early enough to have meaningful equity and career-defining impact.\n\nThe base salary range for this role is $150,000‚Äì$300,000 USD annually, depending on experience and level (Tier 1, San Francisco)\n\nWe may use artificial intelligence (AI) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. These tools assist our recruitment team but do not replace human judgment. Final hiring decisions are ultimately made by humans. If you would like more information about how your data is processed, please contact us."
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "San Mateo, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4304688090?position=35&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=I1B106Ch8oTK2yMktbF4jA%3D%3D",
    "description": "About Chartmetric\n\nChartmetric, Inc. is a 10-year-old startup specializing in music data analytics. We are trusted by Universal MusicGroup, Sony, Warner, and Apple Music, as well as hundreds of other music companies and industry professionals. Our team has created a self-service data dashboard for the music industry to better understand the activity happening around artists. Together, we combine hundreds of thousands of real-time data points across iTunes, Spotify, YouTube, Google, Amazon, X, and others through our beautifully designed tool in order to make sense of the increasingly complex landscape of the music industry.\n\nAbout The Role\n\n\nWe are seeking a talented DevOps / Developer Experience Engineer to join our team and play a pivotal role in enhancing our development infrastructure and streamlining the developer workflow. This position combines traditional DevOps responsibilities with a focus on creating exceptional developer experiences through tooling, automation, and process optimization.\n\n\nWhat You'll Do\n\n\nInfrastructure & Operations\nDesign, implement, and maintain scalable cloud infrastructure using Infrastructure as Code (IaC) principles\nManage CI/CD pipelines and deployment processes across multiple environments\nMonitor system performance, reliability, and security, implementing proactive solutions\nAutomate operational tasks and eliminate manual toil through scripting and tooling\nEnsure high availability and disaster recovery capabilities\n\n\nDeveloper Experience\nBuild and maintain internal developer tools and platforms that improve productivity\nStreamline onboarding processes for new developers and reduce time-to-first-commit\nDesign and implement developer-friendly APIs, SDKs, and documentation\nCreate self-service capabilities that reduce dependencies and waiting times\nGather feedback from development teams and iterate on tooling based on pain points\n\n\nCollaboration & Process Improvement\nWork closely with engineering teams to understand workflow challenges and requirements\nChampion best practices for code deployment, testing, and monitoring\nLead initiatives to improve development velocity and reduce friction\nParticipate in incident response and post-mortem analysis\nMentor team members on DevOps practices and tooling\n\nWhat We're Looking For\n\n\nTechnical Skills\n3+ years of experience in DevOps, SRE, or Platform Engineering roles\nStrong proficiency with cloud platforms (AWS, GCP, or Azure)\nExperience with Infrastructure as Code tools (Terraform, CloudFormation, or Pulumi)\nHands-on experience with containerization (Docker) and orchestration (Kubernetes)\nProficiency in CI/CD tools (Jenkins, GitLab CI, GitHub Actions, or similar)\nStrong scripting skills in Python, Bash, or Go\nExperience with monitoring and observability tools (Prometheus, Grafana, ELK stack, or similar)\n\n\nDeveloper Experience Focus\nExperience building internal tools and platforms for development teams\nUnderstanding of software development lifecycle and common developer pain points\nFamiliarity with API design and developer-facing documentation\nExperience with version control systems and Git workflows\nKnowledge of testing frameworks and quality assurance processes\n\n\nSoft Skills\nStrong problem-solving abilities and analytical thinking\nExcellent communication skills and ability to work with cross-functional teams\nCustomer-focused mindset with emphasis on developer productivity\nProactive approach to identifying and resolving issues\nAbility to balance technical debt with feature delivery\n\n\nPreferred Qualifications\nKnowledge of security best practices and compliance frameworks\nBackground in software development or engineering\nFamiliarity with cost optimization strategies for cloud infrastructure\nPrevious experience in a high-growth or scaling environment\n\nWhat We Offer\n\n\nCompetitive salary and equity package\nComprehensive health, dental, and vision insurance\nOpportunity to shape developer experience across the organization\nAccess to cutting-edge tools and technologies\n\n\nTeam Culture\n\n\nWe believe that great developer experiences lead to better products and happier teams. Our DevOps/DX team operates as enablers and force multipliers, working collaboratively to remove friction from the development process. We value automation, measurement, and continuous improvement, always asking \"how can we make this better for our developers?\"\n\n\nThe Pay Range For This Role Is\n\n135,000 - 165,000 USD per year(San Mateo)\n\n120,000 - 150,000 USD per year(New York)"
  },
  {
    "title": "Cloud DevOps With Azure Experience -100%Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "New Jersey, United States",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-with-azure-experience-100%25remote-at-the-dignify-solutions-llc-4341845867?position=36&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ZKAJaVEHguFWgKCn%2F0aQOQ%3D%3D",
    "description": "In-depth knowledge of Azure services, including Azure Networking, Storage, Firewall, Compute, Function, Backup and Azure DevOps\nAutomating routine tasks using Ansible, Python, Terraform and Azure CLI for enhanced efficiency\nMinimum 4 years of specialized experience DEVOPS engineer role with full automation using Ansible .\nMinimum 2 years of Azure Infrastructure automation with Terraform.\nMust have experience working with Terraform and Ansible for infrastructure provisioning and configuration management, respectively.\nExperience configuring and managing CI/CD pipelines with tools such as GitLab, Jenkins, etc. for enterprise-level deployments.\nConfiguration-level knowledge for Linux sysadmin with RHEL.\nAdvanced troubleshooting and problem-solving skills, related to cloud and network infrastructure\nWorking-level experience in Azure Cloud (Azure/Azure GOV Cloud)\nAdvanced experience in large scale cloud architecture, design, and integration with a focus on automation, networking and NextGen Firewall\nExperience with Terraform, Ansible, and Python\nLinux OS\nProficiency in multi-cloud environments, especially in AWS, is highly desirable."
  },
  {
    "title": "DevOps Engineer - All Levels",
    "company": "CodeRabbit",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-all-levels-at-coderabbit-4318518267?position=37&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=5XrOT96CHl3GwvVFxLk7fg%3D%3D",
    "description": "About CodeRabbit\n\nCodeRabbit is an innovative research and development company focused on building extraordinarily productive human-machine collaboration systems. Our primary goal is to create the next generation of Gen AI-driven code reviewers: a symbiotic partnership between humans and advanced algorithms that significantly outperforms individual engineers. We combine language models with human ingenuity to push the boundaries of software development efficiency and quality.\n\nRole Overview\n\nAs a DevOps Engineer at CodeRabbit, you‚Äôll play a key role in scaling, securing, and hardening the infrastructure that powers our AI-enabled developer tools. You‚Äôll work closely with our platform engineers, backend team, and applied AI teams to ensure that our systems are resilient, observable, fast, and easy to deploy.\n\nThis is a hands-on, IC-focused role for someone who thrives in fast-paced environments, takes ownership of critical infrastructure, and wants to build tooling that unblocks an ambitious engineering team.\n\nResponsibilities\n\n\nDesign, implement, and maintain scalable CI/CD pipelines\nDevelop and manage infrastructure as code (e.g., Terraform, Pulumi)\nImprove system reliability through monitoring, alerting, logging, and failover strategies\nWork with platform and backend teams to identify and resolve performance bottlenecks\nContribute to deployment workflows, environment automation, and developer tooling\nEnsure infrastructure security and compliance practices are in place\n\n\nQualifications\n\n\nEducation: Degree in Computer Science, Engineering, or a related technical field, or equivalent practical experience\nExperience: 3+ years in a DevOps, Infrastructure, or SRE role at a fast-paced tech company or startup\nTooling: Expert-level proficiency with CI/CD systems (GitHub Actions, ArgoCD, etc.), Docker, and Kubernetes\nInfrastructure: Expert with cloud providers (AWS/GCP), distributed systems architecture and implementation, IaC tools (Terraform, Pulumi), and secrets management (Vault, SSM, etc.)\nObservability: Strong understanding of logging, metrics, and monitoring in large-scale distributed systems (e.g., Grafana, Prometheus, ELK, Datadog)\nCollaboration: Effective at partnering with backend and ML teams to deliver stable, high-velocity systems\nSecurity: Experience building with best practices in cloud and application-level security\n\n\nBonus Points\n\n\nExperience supporting AI or ML workloads in production\nExperience with ephemeral environments and preview deployments\nContributions to internal platform tools or DevOps open-source projects\nPast ownership of high-uptime systems or regulated environments\n\n\nWhy Join Us?\n\n\nBuild the Future: We‚Äôre redefining code review with AI. You‚Äôll help shape a new development paradigm with cutting-edge technology that has real-world impact.\nReal Ownership: Every engineer at CodeRabbit owns projects end-to-end ‚Äî from proposal to production.\nCollaborative & Innovative Environment: Join a tight-knit team of engineers, designers, and researchers who are passionate about building transformative products.\nProfessional Growth: We invest in our people ‚Äî through mentorship, responsibility, and development opportunities.\nCompetitive Compensation: We offer a strong salary, equity, and benefits package.\nHybrid Work Culture: We collaborate in person in the Bay Area weekly, with flexibility for heads-down remote work.\n\n\nOur Values\n\n\nü§ù Collaborative Humans: Prioritizing collective intelligence\nüöÄ Fearless Innovators: Turning obstacles into growth opportunities\nüí™ Persistent, Passionate Developers: Thriving on complex, long-term challenges\nüéØ Impact-Driven Creators: Crafting intuitive tools for developers\nüß† Rapid Learners and Un-learners: Adapting quickly in our fast-paced technological world\n\n\nBase pay range for this role is $180k-260k. Actual salary will be based on job-related skills, experience, and location.\n\nApply Now ‚Äî If you're passionate about building high-impact infrastructure and enabling AI-powered developer experiences, we‚Äôd love to hear from you.\n\nCompensation Range: $175K - $275K"
  },
  {
    "title": "DevOps Engineer",
    "company": "Uffizio",
    "location": "Michigan, United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-uffizio-4324397378?position=38&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=RZZy60WAIuvg2dXTSGxTVg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Assistant (Entry-Level)",
    "company": "45PRESS",
    "location": "Canfield, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-assistant-entry-level-at-45press-4301017192?position=39&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=qBkglRIdkox7xUjpmoHIDw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Senior DevOps Engineer",
    "company": "CEIPAL",
    "location": "Charlotte, NC",
    "link": "https://www.linkedin.com/jobs/view/senior-devops-engineer-at-ceipal-4305453358?position=40&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=bZVraPHMBvuSmZb227bfzA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4310657957?position=41&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=d3FhXuWzvPZzFbHZcyzOrg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Hudu",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hudu-4323191230?position=42&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=TRd%2BokkuCDqQNhERojXung%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4335667666?position=43&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=gS9COpOgvIP1m93Bqcm2Lw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Staff Engineer: DevOps",
    "company": "Dispel",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/staff-engineer-devops-at-dispel-4339045806?position=44&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=Lkisr8gi78lwKR3iwDCC%2BA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "IT Automation LLC",
    "location": "Cary, NC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-it-automation-llc-4324192032?position=45&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=zcVztF2a7No2dbToImsr%2FA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CHEQUESPREAD PLC",
    "location": "Valley Forge, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chequespread-plc-4288904252?position=46&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=Bu3USDMdxSj2wGQU8yIjIg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "eSimplicity",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-esimplicity-4315888714?position=47&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=%2FMT6woty20NhE4CdpCfiVQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud/DevOps Engineer",
    "company": "Tagup, Inc.",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-engineer-at-tagup-inc-4333051833?position=48&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=%2Bb%2F4n60y04UDQrL7gyYFeA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mark43",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mark43-4309062970?position=49&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=Hg88F9UCVAj6eTE%2Fd6PPUA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CMG (Capital Markets Gateway)",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cmg-capital-markets-gateway-4338419750?position=50&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=NwzEISJLfv7TmTs2hKY9Aw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mintlify",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mintlify-4318506680?position=51&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=urNOgoByKBq1CVSRpgzhxA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Support Engineer",
    "company": "Porter",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-support-engineer-at-porter-4295124575?position=52&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=kKPfQ57eGwFF%2Bhg7JugLQA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Northstrat Incorporated",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-northstrat-incorporated-4304125676?position=53&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=1nFlqlx8DIpZpEsiWdlZTw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Paramount",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-paramount-4335876548?position=54&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=zwiR%2F74I%2FoTnoBvd36ChVw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "SmartVault",
    "location": "Houston, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-smartvault-4297941616?position=55&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=n1nbiGuVgMFxGzNJtfyGkA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "RSC2, Inc.",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rsc2-inc-4311252822?position=56&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=GLpra%2B4wjCFY4jBAtHBr3A%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps Support Engineer",
    "company": "Nihon Kohden Digital Health Solutions",
    "location": "Irvine, CA",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-support-engineer-at-nihon-kohden-digital-health-solutions-4295710052?position=57&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=sgjTGSzQyx%2FCNETFJMTgWg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "PingWind",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pingwind-4316019938?position=58&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=mpIXb3dikS9P42uvTNRWRw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Cymertek Corporation",
    "location": "San Antonio, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cymertek-corporation-4336305401?position=59&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=kTLCWoXsQq0vLaYQzFicqg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Ryan",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ryan-4284462825?position=60&pageNum=0&refId=fsoU20dYIoQZpm2iHxIqHg%3D%3D&trackingId=ZU9763wrECt9EAl%2FpVjSlQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Mid-Junior DevOps Engineer - USA",
    "company": "HERE",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/mid-junior-devops-engineer-usa-at-here-4347377348?position=1&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=zXM5OK5nf%2BBIqoGfPQnBBg%3D%3D",
    "description": "Mid-Junior DevOps Engineer\n\nLocation: New York, NY / Hybrid Remote / Remote within USA (EST / CST time zone)\n\nWe have an office in New York City and this position can either be based in the office, hybrid remote, or remote within the EST/CST time zones (subject to your existing legal right to work in the jurisdiction).\n\nAbout HERE\n\nEverything works right here‚Ñ¢.\n\nTraditional browsers weren't built for work. In today's enterprise environment‚Äîwhere security threats are constant and productivity is critical‚Äîlegacy browsers fall short. That's why we built HERE, the browser purpose-built for work.\n\nPowered by Chromium, HERE Enterprise Browser combines enterprise-grade security, seamless productivity, and native AI integration in one secure, intelligent workspace. Designed for regulated industries, HERE offers deep policy controls, identity-based access, secure workspace isolation, and full interoperability across SaaS, legacy, and virtualized environments. Our platform enables teams to work faster, more securely, and more intelligently‚Äîwithout compromise.\n\nHERE technology is trusted by 90% of global banks and also used within the U.S. Intelligence Community and other sectors. We're backed by some of the world's most respected financial institutions and venture firms, including Bain Capital Ventures, Bank of America, J.P. Morgan, Wells Fargo and IQT, the not-for-profit strategic investor that accelerates the introduction of groundbreaking technologies to enhance the national security of America and its allies.\n\nAbout the Role\n\nHERE is seeking a mid-junior DevOps Engineer to join our infrastructure team! The primary responsibilities for this role will span CI/CD pipeline engineering and cloud operations, maintaining and improving our GitHub and GitLab CI/CD pipelines, and supporting our AWS cloud infrastructure. In this role, you will gain hands-on experience with real production build systems and cloud platforms- while having the opportunity to work on practical projects that directly impact both our development velocity and operational reliability.\n\nWe're actively evolving toward a cloud-agnostic, multi-cloud architecture and migrating to Kubernetes for container orchestration. While current AWS and ECS experience is essential, having exposure to Azure, GCP, and Kubernetes will position you well for our infrastructure roadmap.\n\nThis role offers the opportunity to collaborate with senior engineers who will provide guidance and mentorship, whilst giving you ownership of projects across the DevOps lifecycle. This is an excellent platform for building practical experience with modern build engineering (CI/CD automation, cloud infrastructure, and deployment practices) within a production environment.\n\nResponsibilities\n\n\nCI/CD Pipeline Development:\nBuild, maintain, and optimize GitLab CI/CD pipelines for multi-platform builds (Windows, macOS, Linux).\nWork with YAML configurations, pipeline stages, artifacts, and deployment workflows.\nCloud Infrastructure Operations:\nHelp maintain and improve AWS infrastructure including ECS/Fargate deployments, RDS databases, Route53 DNS, VPC networking, and IAM policies.\nSupport multi-tenant and multi-region architecture.\nContainer & Deployment Management:\nWork with Docker containers, ECS task definitions, and ECR registries.\nDeploy and manage containerized Node.js applications in production environments.\nRelease Management:\nHelp manage release processes including version promotion, release channels (canary, beta, stable), and automated deployment to staging and production environments.\nDatabase Operations:\nSupport PostgreSQL on AWS RDS‚Äîbackups, SSH tunneling through bastion hosts, read-only user management, and database configuration for multi-tenant environments.\nAutomation & Scripting:\nWrite and maintain automation scripts in Bash, PowerShell, Python, and Node.js.\nBuild tools to improve infrastructure reliability and developer experience.\nInternal Tools Support:\nHelp maintain web-based DevOps tools built with Express.js, React, and TypeScript‚Äîtools for cloud settings management, tenant provisioning, and deployment monitoring.\n\nWhat We're Looking For\n\nIdeally 2 to 4 years of experience with the following core requirements:\n\n\nGitLab CI/CD: Experience with GitLab CI/CD pipelines‚ÄîYAML configuration, stages, jobs, artifacts, rules, dependencies.\nUnderstanding of CI/CD best practices and pipeline optimization.\nAWS Cloud Fundamentals: Practical experience with core AWS services‚ÄîEC2, ECS/Fargate, RDS, Route53, VPC, IAM, Secrets Manager, CloudWatch. Comfortable navigating the AWS Console and CLI.\nMulti-Platform Scripting: Solid scripting skills in Bash (Linux) and PowerShell (Windows). Ability to write maintainable automation scripts for both platforms.\nContainerization: Hands-on Docker experience‚Äîbuilding images, writing Dockerfiles, docker-compose, understanding container networking, and working with ECS/ECR.\nBuild Systems: Experience with build tools and package managers‚Äînpm/Node.js, .NET/NuGet, Python packaging. Understanding of dependency management and build artifacts.\nVersion Control: Strong Git fundamentals‚Äîbranching strategies, merge requests, tagging. Experience with GitHub (or GitLab) workflows and code review practices.\nLinux/Unix & Windows: Comfortable in both environments‚ÄîSSH, file permissions, package managers, systemd, PowerShell. Understanding of cross-platform operational challenges.\nNode.js/JavaScript: Comfortable reading and writing JavaScript/Node.js code. Experience with npm, package.json, and basic Express.js applications for tooling.\n\n\nNice to Have\n\n\nKubernetes experience (EKS, GKE, AKS) or willingness to learn, we're migrating from ECS to K8s\nMulti-cloud experience (Azure, GCP) or cloud-agnostic architecture knowledge\nGitLab Runner administration and configuration\nAWS CDK or CloudFormation for Infrastructure as Code\nTerraform for multi-cloud infrastructure management\nTypeScript development experience\nPostgreSQL database administration and optimization\n.NET build systems and NuGet package management\nReact or frontend framework experience\nAirflow or workflow orchestration tools\nHelm charts and Kubernetes manifest management\n\n\nWhat We're Offering\n\nBenefits -\n\n\nGenerous Paid Time Off, Paid Holidays & Sick Time\nCompetitive & Comprehensive Health Insurance\nThoughtfully-Planned Paid Parental Leave\nFinancial Well-Being Plans (FSA) (401k) (Life Insurance)\nStock Options\nProfessional Development Courses\nEmployee Resource Groups\n\n\nAdditional Perks -\n\n\nOne Medical - Free Membership\nTalkspace - Mental Health Therapy 24/7\nTeam Lunches\nCasual dress code\nCommuter Benefits (NYC employees only)\nCitibike (NYC employees only)\n\n\nLife at HERE\n\nAt HERE, we pride ourselves on fostering a friendly, collaborative, and supportive culture that truly respects the diversity of thought. Our goal is to create a space where employees can learn and innovate, and overall, have a good time doing it. We value and appreciate that our employees have a wide set of interests and experiences and put importance on taking the time to get to know one another and form relationships. From virtual socials and in-person events, to informal meetings and employee resource groups, we make it easy to engage and connect. Our environment promotes a productive, enjoyable learning experience - aligned together, working to create compelling solutions for our clients. Everything works right here.‚Ñ¢\n\nWe are HERE - Read about our recent rebrand from OpenFin to HERE\n\nRecent Awards\n\n\nVoted \"Enterprise Browser of the Year\" by CIO Review (2025)\nVoted \"100 Best Midsize Companies to Work For in NYC\" by BuiltIn (2025)\nVoted \"Top 10 Contact Center Technologies & Capabilities of 2024\" by CX Today (2024)\nVoted \"Best Enterprise Environment for Interoperability\" by TradingTech Insight Awards Europe (2024)\nVoted \"Top 50 Best Startups to Work for in the US\" & \"Top 50 Best Startups to Work for in New York\" by BuiltIn (2024)\nVoted as a \"Best Employer Award\" finalist at the UK FinTech Awards (2023)\nVoted \"Best FinTech Company CEO\" at the FinTech Breakthrough Awards (2023)\nVoted \"Best Internal Talent Team\" by Financial Technologist (2023)\nVoted \"Best Solution for Workflow Automation\" at the Trading Tech Insight Awards (2023)\nVoted \"Top Innovator Across Financial Markets\" in TabbFORUM NOVA Awards (2023)\nVoted \"Best User Interface Innovation\" in the Risk Markets Technology Awards (2023)\nVoted \"Top 100 Most Promising Private FinTech Companies\" by CB Insights (2023)\nVoted \"Most Influential Financial Technology Firm\" by Harrington Starr (2023)\n\n\nRECRUITERS NOTICE: Recruiters - if you wish to reach out to us regarding this job posting, you may reach out to externalrecruitment@here.io in order for your communication to be reviewed. HERE will review these communications if external help is needed for a position. Agencies may not contact individuals within our organization with solicitations. Firms that do not follow these guidelines risk having all communication from their firm being blocked. We thank you in advance for your cooperation in following our process.\n\nSponsorship - While we highly value all of our candidates, we are not offering sponsorship for this role.\n\nSalary Range: $70k - $120k\n\nSalary Range Disclaimer: This base salary range represents the low and high end salary range for this particular position; not all encompassing of the total compensation package. Actual salaries may vary depending upon but not limited to experience, special skill set, education and location. This range represents only one aspect of HERE's total compensation package offered to employees. Other forms of compensation may be stock options, commissions, paid time off and other variable benefits. Learn more about additional HERE compensation benefits above."
  },
  {
    "title": "junior devops engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-ova-work-4309344701?position=2&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=tkz%2B%2BCRRmxgG1rz1Q3V46Q%3D%3D",
    "description": "Job Title: Junior DevOps Engineer\n\nLocation: Remote\n\nJob Type: Full-time\n\nExperience Level: Entry-Level (0-2 years)\n\nDepartment: IT / Engineering / DevOps\n\nJob Summary\n\nWe are looking for a motivated and detail-oriented Junior DevOps Engineer to join our growing DevOps team. This role is ideal for someone with a foundational understanding of DevOps practices and a passion for automation, cloud technologies, and continuous integration/deployment. You will assist in maintaining and improving our infrastructure, deployment pipelines, and monitoring systems.\n\nKey Responsibilities\n\n\nAssist in the setup, maintenance, and monitoring of CI/CD pipelines.\nSupport cloud infrastructure (AWS, Azure, GCP) and help manage deployments.\nCollaborate with development and operations teams to ensure reliable software delivery.\nWrite scripts and automation tools to streamline operations and deployments.\nMonitor system performance and troubleshoot issues in development and production environments.\nMaintain documentation for infrastructure and deployment processes.\nLearn and apply best practices in security, scalability, and reliability.\n\n\nRequired Qualifications\n\n\nBachelor's degree in Computer Science, Information Technology, or related field.\nBasic understanding of DevOps principles and software development lifecycle.\nFamiliarity with Linux/Unix systems and shell scripting.\nExposure to cloud platforms (AWS, Azure, or GCP).\nExperience with version control systems (e.g., Git).\nKnowledge of CI/CD tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nStrong problem-solving and communication skills.\nEagerness to learn and grow in a fast-paced environment.\n\n\nPreferred Qualifications\n\n\nInternship or project experience in DevOps or system administration.\nFamiliarity with containerization tools (Docker) and orchestration (Kubernetes).\nExperience with Infrastructure as Code (Terraform, Ansible).\nBasic knowledge of monitoring tools (Prometheus, Grafana, ELK Stack).\n\n\nBenefits\n\n\nCompetitive salary and growth opportunities.\nMentorship from senior engineers.\nHealth and wellness benefits.\nFlexible work hours and remote work options.\nAccess to training and certification programs."
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4341955705?position=3&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=GZgTi54cQaJ2tl1aRahtDA%3D%3D",
    "description": "Over 12 -15 years of overall expereince needed.\nA solid foundation in computer science, with strong competencies in data structures, algorithms, and software design.\nLarge systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems.\nExperience in programming and experience with problem diagnosis and resolution.\nKubernetes (3-4 YOE) and Fieldglass Experience (1-2 YOE)"
  },
  {
    "title": "DevOps Engineer - Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-remote-at-the-dignify-solutions-llc-4347005704?position=4&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=K%2BCwzeyWQbFBi9W5vcXEQQ%3D%3D",
    "description": "Bachelor's degree in a technical field such as computer science, computer engineering or related field required 0-2 years experience required.\n1-2 years of experience with Kubernetes.\nISBN experience preferred.\nA solid foundation in computer science , with strong competencies in data structures, algorithms, and software design large systems software design and development experience.\nExperience performing in-depth troubleshooting and unit testing with both new and legacy production systems experience in programming and experience with problem diagnosis and resolution."
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "GliaCell Technologies",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-gliacell-technologies-4338894490?position=5&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=kL4SPerIYpm5sA39ZbYMHw%3D%3D",
    "description": "An active or rein-statable TS/SCI with Polygraph security clearance is REQUIRED. Please do not apply if you currently do not possess this level of clearance.***\n\n\nAre you a Junior DevOps Engineer who is ready for a new challenge that will launch your career to the next level?\n\n\nTired of being treated like a company drone?\nTired of promised adventures during the hiring phase, then being dropped off on a remote contract and never seen or heard from the mothership again?\nOur engineers were certainly tired of the same.\n\n\nAt GliaCell our slogan is ‚ÄúWe make It happen‚Äù.\n\n\nWe will immerse you in the latest technologies.\nWe will develop and support your own personalized training program to continue your individual growth.\nWe will provide you with work that matters with our mission-focused customers, and surround you with a family of brilliant engineers.\n\n\nCulture isn‚Äôt something you need to talk about‚Ä¶if it just exists.\n\nIf this sounds interesting to you, then we‚Äôd like to have a discussion regarding your next adventure! If you want to be a drone, this isn‚Äôt the place for you.\n\nWe Make It Happen!\n\nGliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks.\n\nGliaCell‚Äôs Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.\n\nWe Offer\n\n\nLong term job security\nCompetitive salaries & bonus opportunities\nChallenging work you are passionate about\nAbility to work with some amazingly talented people\n\n\nJob Description\n\nGliaCell is seeking a Junior DevOps Engineer on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.\n\nResponsibilities\n\n\nEstablishing a test framework and automated tests utilizing Cucumber and Cypru\nKnowledgeable in Microservices design & architecture, CI/CD, Test frameworks and automation, Agile Methodology.\nExecute load and performance testing, chaos testing, functional testing and end-to-end testin\nAgile development and delivery of software\nCommunication and collaboration: Software Development is a team-oriented discipline. Engineers need to be able to communicate and collaborate effectively with other team members, as well as with stakeholders.\n\n\nRequired Skills\n\n\nPython and Cucumber\n\n\nDesired Skills:\n\n\nAWS services such as Lambdas, Step Functions, EC2 and S3\n\n\nKey Requirements\n\nTo be considered for this position you must have the following:\n\n\nPossess an active or rein-statable TS/SCI with Polygraph security clearance.\nU.S. Citizenship.\nWorks well independently as well as on a team.\n6+ years experience as a Developer in programs and contracts of similar scope, type, and complexity is required. A bachelor‚Äôs degree in a technical discipline from an accredited college or university is required. Five (4) years of development experience may be substituted for a bachelor‚Äôs degree.\n\n\nLocation: Annapolis Junction, MD\n\nSalary Range: The salary range for this full-time position is $50,000 to $120,000. Our salary ranges are determined by position, level, skills, professional experience, relevant education and certifications. The range displayed on each job posting reflects the minimum and maximum target salaries for this position across our projects. Within the range, your salary is determined by your individual benefits package selection. Your recruiter can share more about the specific salary range for your preferred position during the hiring process.\n\nBenefits\n\n\nMedical, Dental, and Vision Coverage for Employee and Dependents\nUp to 25 Days of Paid Time Off\nUp to 40 hours of PTO Carryover\n11 Federal Government Holidays\nWork From Home Opportunities\n401K Company Contribution, Fully Vested Day 1\nDiscretionary, Certification, and Sign-On Bonus Potential\nEmployee Referral Bonus Program\nAnnual Professional Development\n100% Premium Covered for Life & Disability Insurances\nAdditional Voluntary Life Insurance Coverage Available\nEmployee Assistance Program\nTravel Protection Program\nFinancial Planning Assistance\nBereavement and Jury Duty Leave\nMonthly Team and Family Events\nTechnology Budget\nGlobal Entry\nAnnual Swag Budget\n\n\nLearn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/\n\nGliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
  },
  {
    "title": "DevOps Cloud Engineer Based in U.S.A",
    "company": "Advancio",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-cloud-engineer-based-in-u-s-a-at-advancio-4324442139?position=6&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=ka4J%2BgcZ5%2FX0K3uL%2F8bbkg%3D%3D",
    "description": "This is a remote position.\n\nWho We Are:\n\n\nAt Advancio, we are passionate about technology and its ability to transform the world. We are rapidly expanding and building a company where we serve exceptional businesses, hire top talent, and have a lot of fun doing what we love!\n\n\nJob Summary:\n\nWe are seeking a skilled DevOps Cloud Engineer to design, implement, and manage scalable cloud-based infrastructure and DevOps processes. The ideal candidate will have extensive experience with cloud platforms, CI/CD pipelines, and automation tools, ensuring the efficient deployment and operation of applications.\n\n\nWhat will you do:\n\n\nDesign, deploy, and manage cloud infrastructure on platforms such as AWS, Azure, or Google Cloud Platform (GCP).\n\nBuild and maintain CI/CD pipelines to streamline development and deployment processes.\n\nAutomate infrastructure provisioning, configuration, and monitoring using tools like Terraform, Ansible, or similar.\n\nEnsure system reliability, availability, and performance through robust monitoring and alerting.\n\nCollaborate with development teams to optimize the delivery and scalability of applications.\n\nManage containerized workloads using Docker and orchestration platforms such as Kubernetes.\n\nImplement security best practices for cloud environments, including identity management, encryption, and compliance adherence.\n\nStay updated with the latest DevOps tools and methodologies to enhance team efficiency.\n\n\n\n\nRequirements\n\n\n\n\n\n\n5+ years of experience in DevOps, cloud engineering, or related roles.\n\nAdvanced English communication skills, both verbal and written.\n\nProficiency in at least one major cloud platform (AWS, Azure, or GCP).\n\nHands-on experience with CI/CD tools (e.g., Jenkins, GitLab CI/CD, CircleCI).\n\nStrong scripting skills in Python, Bash, or similar languages.\n\nSolid knowledge of infrastructure-as-code (IaC) tools like Terraform or CloudFormation.\n\nExperience with containerization (Docker) and orchestration (Kubernetes).\n\nFamiliarity with monitoring and logging tools like Prometheus, Grafana, or ELK Stack.\n\nStrong understanding of networking, security, and system architecture."
  },
  {
    "title": "DevOps Engineer",
    "company": "Princeton IT Services, Inc",
    "location": "Englewood Cliffs, NJ",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-princeton-it-services-inc-4338714288?position=7&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=1ZRcuADkbl44hV%2F2vLHI%2Bw%3D%3D",
    "description": "Job Title: DevOps Engineer\n\nLocation: Englewood Cliffs, NJ\n\nEmployment Type: W2 Only\n\nJob Summary\n\nWe are seeking a DevOps Engineer with strong hands-on experience in Linux, Docker, and Kubernetes to support and optimize our deployment environment in Englewood Cliffs, NJ. This is a W2-only role requiring solid skills in automation, CI/CD, and container orchestration. The ideal candidate will ensure smooth application releases, maintain system stability, and collaborate closely with development teams.\n\nKey Responsibilities\n\n\nManage and support Linux-based systems in production and staging environments.\nBuild, maintain, and optimize CI/CD pipelines for automated deployments.\nCreate, manage, and troubleshoot Docker containers and images.\nDeploy, monitor, and tune Kubernetes clusters and workloads.\nAutomate infrastructure tasks using Shell or Python scripts.\nImplement and manage monitoring and logging tools (Prometheus, Grafana, ELK, etc.).\nTroubleshoot system, container, and cluster-level issues end-to-end.\nWork cross-functionally with development and QA teams to ensure smooth releases.\n\n\nRequired Skills\n\n\n8+ years of DevOps or related experience.\nStrong hands-on experience with Linux administration.\nSolid experience working with Docker for containerization.\nStrong working knowledge of Kubernetes (deployments, scaling, troubleshooting).\nExperience building CI/CD pipelines (Jenkins, GitLab CI, GitHub Actions).\nStrong scripting skills in Shell/Bash/Python.\nExperience with monitoring and logging tools."
  },
  {
    "title": "DevOps Engineer",
    "company": "Lean TECHniques",
    "location": "Johnston, IA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lean-techniques-4336685413?position=8&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=K57KPrvB%2FpYjSE4cCDZrHg%3D%3D",
    "description": "Maybe you‚Äôre bored and need a new challenge. Or you‚Äôre sick of all the bureaucracy and just want to focus on designing kick-ass software.\n\nWhatever the reason, we want you to know that LT is different. And not just air quotes ‚Äúdifferent,‚Äù but more like ‚Äúbreathing easy for the first time in a long time‚Äù different.\n\nIt‚Äôs a place where you can write your own story and make a difference along the way. At LT, you‚Äôll have the freedom and flexibility to do what you think needs to be done, and you‚Äôll get to do it while working alongside a team of other curious individuals who love a good challenge too.\n\nWe‚Äôre currently looking to add a DevOps Engineer to our crew of nerds. If you‚Äôre someone who has 5+ years of DevOps experience, we'd love to chat!"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337132819?position=9&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=KcNF%2BD3GfDnsNzdH0U%2Fe3A%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "DevOps Engineer (35 LPA - 55 LPA)",
    "company": "CodeRound AI",
    "location": "Greater Bloomington Area",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-35-lpa-55-lpa-at-coderound-ai-4308183910?position=10&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=ehgrXv0BX1v5qf8xlDYxKw%3D%3D",
    "description": "üöÄ What We‚Äôre Building\n\n\nCodeRound AI matches top 5% tech talent to fastest growing VC funded AI startups.\nCandidates apply once and get UPTO 10 remote as well as onsite interview opportunities IF selected!\nTop-tier product startups in US, UAE & India have hired top engineers & ML folk using CodeRound\n\n\nüß© What You‚Äôll Do\n\n\nBuild and optimize our cloud infrastructure ‚Äî scalable, secure, and cost-effective (mostly AWS).\nSet up and manage CI/CD pipelines to ensure smooth deployment across backend, AI services, and mobile.\nContainerize backend services (FastAPI, Rails) and optimize them for performance.\nImplement monitoring, alerting, and logging to catch issues before users do.\nOptimize database performance (Postgres, Redis) and manage backups and scaling.\nCollaborate with backend, AI, and product teams to deploy new features safely and quickly.\nChampion infra-as-code and automation wherever possible.\n\n\nüí• Why this is exciting\n\n\nYou'll own DevOps for a high-usage, real-world AI platform ‚Äî not just internal tools.\nYou‚Äôll work on real-time, high-stakes flows ‚Äî interviews, scoring, hiring decisions.\nYou‚Äôll work closely with founders, ship weekly, and see the direct impact of your work.\n\n\n‚úÖ You‚Äôll Be Great At This If You\n\n\nHave 4+ years of experience as a DevOps engineer, SRE, or infrastructure engineer.\nAre strong with AWS services (EC2, RDS, ECS/EKS, S3, CloudWatch).\nCan write clean, reusable Terraform or CloudFormation code.\nHave experience setting up CI/CD pipelines and optimizing build/release flows.\nAre comfortable with Docker, Linux servers, and basic networking (VPCs, security groups).\nUnderstand application and database scaling (horizontal/vertical).\n\n\n‚ö° Bonus If You\n\n\nHave experience supporting AI/ML pipelines in production (fine-tuning infra, vector DBs, etc.).\nKnow cost optimization tricks for cloud infra (spot instances, autoscaling groups, etc.).\nAre excited to eventually build a small infra team"
  },
  {
    "title": "Devops Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341915759?position=11&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=oO2GiEvZ5Gm8eM0VH%2FvFqA%3D%3D",
    "description": "Job Description:\n\n\nServe as a subject matter expert to develop and support DevOps Web Access Management solutions\nInstall, configure, and maintain automation solutions, in support of KeyBank infrastructure\nDevelop Standard Operating Procedures, maintenance plans and provide status reports as required\nPerform daily operational tasks as required for the Web Access Management team\n\n\nQualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments"
  },
  {
    "title": "DevOps Engineer",
    "company": "LifeMD",
    "location": "Huntington Beach, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-lifemd-4337182535?position=12&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=tQmoRbLu97bUENWYReyqlw%3D%3D",
    "description": "About us:\n\nLifeMD is a leading digital healthcare company committed to expanding access to virtual care, pharmacy services, and diagnostics by making them more affordable and convenient for all.¬†Focused on both treatment and prevention, our unique care model is designed to optimize the patient experience and improve outcomes across more than 200 health concerns.¬†\n\nTo support our expanding patient base, LifeMD leverages a vertically-integrated, proprietary digital care platform, a 50-state affiliated medical group, a 22,500-square-foot affiliated pharmacy, and a U.S.-based patient care center.¬†Our company ‚Äî with offices in New York City; Greenville, SC; and Huntington Beach, CA ‚Äî is powered by a dynamic team of passionate professionals. From clinicians and technologists to creatives and analysts, we're united by a shared mission to revolutionize healthcare.¬†Employees enjoy a collaborative and inclusive work environment, hybrid work culture, and numerous opportunities for growth. Want your work to matter? Join us in building a future of accessible, innovative, and compassionate care.\n\n\nAbout the role:\n\nLifeMD is seeking a highly motivated and experienced DevOps Engineer to join our dynamic Technology team. This individual will serve as a critical link between software development and IT operations, playing a pivotal role in designing, implementing, and maintaining automated processes for software delivery, infrastructure management, and system monitoring. The primary objective is to accelerate our release cycles, enhance system stability, and improve overall operational efficiency across our diverse cloud infrastructure, all while strictly adhering to stringent healthcare industry compliance standards, including HIPAA and SOX.\n\n\nResponsibilities:\n\n\nDesign, implement, and manage scalable, secure, and cost-effective cloud infrastructure primarily on AWS using Terraform\nDevelop and version control Terraform modules for automated provisioning, updating, and de-provisioning of cloud resources (e.g., EC2, S3, RDS, VPC, Lambda in AWS)\nDesign, build, and optimize automated CI/CD pipelines using GitHub Actions for various applications and microservices\nIntegrate automated testing, static code analysis, security scanning, and deployment steps into CI/CD workflows for high quality and secure releases\nImplement, configure, and maintain comprehensive monitoring, logging, and alerting solutions (e.g., AWS CloudWatch, Datadog) for all environments\nDevelop custom dashboards, metrics, and alerts for real-time visibility into system health, performance, and security events\nProactively analyze logs and metrics to identify potential bottlenecks and issues\nParticipate in on-call rotations to swiftly respond to and resolve critical incidents, ensuring high service availability\nAutomate repetitive operational tasks, system configurations, and deployment processes using Python and Bash to enhance efficiency\n\n\n\nRequirements\n\n\n\nBasic Qualifications:\n\nBachelor's degree in Computer Science, Information Technology, Engineering, or a related technical field, or equivalent work experience\n3+ years of progressive experience as a DevOps Engineer, Site Reliability Engineer (SRE), or similar role in a cloud-native environment\nExpert-level proficiency in AWS services (EC2, S3, RDS, VPC, Lambda, IAM, CloudWatch, etc.). Solid understanding and working knowledge of GCP, Digital Ocean, and Azure concepts and services\nExpertise in Terraform for multi-cloud infrastructure provisioning and management, including experience with state management, modules, and workspaces\nHighly skilled in using Git and GitHub for source code management, branching strategies, and pull request workflows\nHands-on experience with implementing and managing monitoring and logging solutions (e.g., AWS CloudWatch, Datadog, ELK stack)\nSolid understanding of cloud networking concepts, including VPCs, subnets, routing tables, load balancers, DNS, and VPNs\nStrong understanding of cloud security best practices, identity and access management (IAM), security groups, network ACLs, and data protection principles\nWorking knowledge of database concepts and experience with various database types (e.g., MongoDB, PostgreSQL, MySQL)\nStrong understanding and implementation of Ansible for cloud workload automations\nHands-on experience with Linux (Ubuntu) and update/patching mechanisms\n\n\n\nPreferred Qualifications:\n\nExperience in the healthcare industry or a highly regulated environment, with a demonstrable understanding of compliance requirements (e.g., HIPAA, SOC2)\nRelevant cloud certifications (e.g., AWS Certified DevOps Engineer - Professional, AWS Certified Solutions Architect - Associate/Professional)\nIn-depth experience with GitHub Actions for designing, implementing, and maintaining automated build, test, and deployment pipelines. Familiarity with other CI/CD tools\nStrong proficiency in Python and Bash scripting for automation, system administration, and tool development.\nKnowledge of Node.js or PHP\nExperience with Docker for containerizing applications. Familiarity with container orchestration platforms (e.g., Kubernetes, AWS ECS)\nExceptional problem-solving and analytical skills with a proactive approach to identifying and resolving complex technical issues\nExcellent communication and interpersonal skills, capable of effectively collaborating with diverse cross-functional teams (developers, QA, product, security)\nStrong sense of ownership, accountability, and ability to work independently while also being a strong team player\nA continuous learning mindset, staying updated with emerging technologies, industry trends, and best practices in the DevOps space\nMeticulous attention to detail and strong documentation skills\n\n\n\nBenefits\n\n\nSalary Range: $130,000-$140,000\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited PTO Policy\nPaid Holidays\nShort Term & Long Term Disability\nTraining & Development"
  },
  {
    "title": "Devops Engineer",
    "company": "Hoplite Solutions LLC",
    "location": "Bethesda, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hoplite-solutions-llc-4336082750?position=13&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=UucqyUc2vb%2BWsUXQXIbw0A%3D%3D",
    "description": "Hoplite Solutions is hiring DevOps Engineers at all experience levels to join our team in Bethesda, MD. In this mission-critical role, you will provide essential system support to our customer while collaborating closely with software development teams and other key technology stakeholders. You will help maintain, enhance, and support a range of IC enterprise products‚Äîboth legacy systems and new solutions‚Äîwithin an Agile SAFe environment.\n\nAs a DevOps Engineer, you will work hand-in-hand with software engineering teams to deploy and operate systems, automate and optimize processes, and build and maintain tools that support deployment, monitoring, and ongoing operations. You will also troubleshoot and resolve issues across development, test, and production environments, ensuring reliability, efficiency, and continuous improvement across the enterprise.\n\nPrimary Responsibilities:\n\n\nSupports software deployments, cloud infrastructure baselines, and operational availability of production systems\nManaging, building, configuring, administering, operating and maintaining all components that comprise the DevOps environment\nDefining enterprise Continuous Integration/Continuous Deployment processes and best practices\nCodifying DevOps best practices across the enterprise\nDeveloping and maintaining scripts to automate tool deployment to an AWS cloud environment and other tasks\nScripting and maintaining build environments\nWorking with project teams to integrate their products into the DevOps environment\n\n\nBasic Qualifications\n\n\nDemonstrated experience setting up one or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience troubleshooting issues with two or more of the following tools: GitHub, Jira, Confluence, Jenkins, and Katalon Studio\nDemonstrated experience working within a software development team and supporting developers and developer activities\nBachelors degree with 4 or more years of prior relevant work experience or Masters with 2 or more years of prior relevant work experience. Will consider additional work experience in lieu of a degree\nTo be considered must have an active TS/SCI with polygraph security clearance\n\n\nPreferred Qualifications\n\n\nAWS Associate Certification (Developer, Solution Architect, or Sys Ops Administrator)\nAWS Professional Certification (DevOps Engineer or Solutions Architect)\nDemonstrated experience in container orchestration using Docker, Vagrant, Kubernetes, or AWS ECS/ECR\nDemonstrated experience with Languages including Java, Python, JavaScript, Ruby, PHP, and Unix shell Scripting\nDemonstrated experience with Ansible, or Puppet\n\n\nHoplite Solutions offers very competitive salaries and an excellent benefits package, to include a 7% employer 401k contribution, fully paid healthcare for our employees, outstanding training benefits, company funded life insurance and short-term disability insurance, and many more.\n\nPowered by JazzHR\n\nwwBe8pS8mn"
  },
  {
    "title": "DevOps Engineer",
    "company": "The Dignify Solutions, LLC",
    "location": "Brooklyn, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-the-dignify-solutions-llc-4341985652?position=14&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=CS1aiq6vHzov4ybjNaoBww%3D%3D",
    "description": "Qualifications:\n\n\nGeneral technical capabilities across all portions of the infrastructure stacks\nIndependent thinker and self-starter\nGenerates ideas, innovative\nExperienced with automation frameworks using an automation first approach\nProficient in one or more programming/scripting languages (Python, Ansible, etc.)\nProficient with one or more cloud orchestration tools (Terraform, Cloud Formation, etc.)\nConduct performance analysis and optimization\nExperienced with public cloud providers such as GCP, Azure and AWS\nComfortable operating in a Linux environment\n\n\nPreferred Skills:\n\n\nPublic and Private Cloud automation experience in production & non-production environments\nKnowledge of web access management technologies and deployments\nKnowledge of web access management technologies and deployments\nKnowledge of routing & switching technologies and configurations\nKnowledge of compute and storage solutions in data center environments\nExperience with Service Now change management and problem management platform\nAbility to balance workload amidst competing deadlines\nAbility to perform knowledge transfers with peer engineers\nContribute to the reliability, performance, supportability, and security of web access management infrastructure\nReview procedures for change and configuration management in all environments."
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "Indianapolis, IN",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4339356296?position=15&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=xtA1sr87D3XmOzJ3o%2B5eXg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Protege",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-protege-4331315574?position=16&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=Fq%2FsSJR39wzN%2FmpQTpveJA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Devops Engineer",
    "company": "PDG Consulting",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pdg-consulting-4321885957?position=17&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=obskFC8JVKVQprmXcjU9Fg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer - 100% Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "Newtown Square, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-100%25-remote-at-the-dignify-solutions-llc-4347005722?position=18&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=lELrSQDJZvSfCO5wx9ntzA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "CloudOps Engineer",
    "company": "Protera",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/cloudops-engineer-at-protera-4336621571?position=19&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=4hUyXcV1PrIt1xme1uVEnw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Systems Engineer",
    "company": "TensorWave",
    "location": "Las Vegas, NV",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-tensorwave-4338727303?position=20&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=cxYQ7i1hFbLKka85IoFM0Q%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Administrator",
    "company": "The Amatriot Group",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-administrator-at-the-amatriot-group-4310974393?position=21&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=txlyKygbhw1YFlLqVI4sbw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Arize AI",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-arize-ai-4332964631?position=22&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=lpLZIP8%2ByMr3q5ptpbenxw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Sustainment",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-sustainment-4335637240?position=23&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=a8EERfjvwLEK2hd3l6fuZw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Devops",
    "company": "The Dignify Solutions, LLC",
    "location": "Phoenix, AZ",
    "link": "https://www.linkedin.com/jobs/view/devops-at-the-dignify-solutions-llc-4347025595?position=24&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=De4L2w63LVZcSLzp8hJJBQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer I",
    "company": "Trustwell",
    "location": "Portland, OR",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-i-at-trustwell-4321600458?position=25&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=xkAR5tyvHbWNZ6RkeG5rew%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4338475165?position=26&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=itcRIh9d5EO3d0PZnZz5dQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer (JIRA)",
    "company": "Rubix Solutions",
    "location": "Washington, DC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-jira-at-rubix-solutions-4335995983?position=27&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=vhVR4LvYtfCBJQvb8saAgQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4291046434?position=28&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=QArZDHJUUhxJV%2FDjcFUCvQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Broad Reach Partners",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-broad-reach-partners-4303987210?position=29&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=OZny5Gzr8QK2YwJWUs1Sfg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "AWS DevOps Specialist",
    "company": "Focus School Software",
    "location": "St. Petersburg, FL",
    "link": "https://www.linkedin.com/jobs/view/aws-devops-specialist-at-focus-school-software-4333597594?position=30&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=dMH1qNgIOXKK90A%2FjCuiXQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Rain",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rain-4318510257?position=31&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=7Fk637abVnZizHDOmVheFw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Jasper",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-jasper-4318500931?position=32&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=Uro07j7j0dZmS5FReFoOVw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps / Systems Engineer",
    "company": "Collate",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-systems-engineer-at-collate-4302854141?position=33&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=SRX45Qe5zExP6fV5EJZWbQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Chartmetric",
    "location": "San Mateo, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chartmetric-4304688090?position=34&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=lpWi22IHuWB6SmXiZ6aStw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps With Azure Experience -100%Remote",
    "company": "The Dignify Solutions, LLC",
    "location": "New Jersey, United States",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-with-azure-experience-100%25remote-at-the-dignify-solutions-llc-4341845867?position=35&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=gQPeBEoOs0U0EFXciehPnQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer - All Levels",
    "company": "CodeRabbit",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-all-levels-at-coderabbit-4318518267?position=36&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=8cQBQy3xTlHa3BXvE3QGVw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Uffizio",
    "location": "Michigan, United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-uffizio-4324397378?position=37&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=cPYwSeINdVdjF2kpGSVSmQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Assistant (Entry-Level)",
    "company": "45PRESS",
    "location": "Canfield, OH",
    "link": "https://www.linkedin.com/jobs/view/devops-assistant-entry-level-at-45press-4301017192?position=38&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=HpNOzcMLEVAKEUFQy5BD4A%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Senior DevOps Engineer",
    "company": "CEIPAL",
    "location": "Charlotte, NC",
    "link": "https://www.linkedin.com/jobs/view/senior-devops-engineer-at-ceipal-4305453358?position=39&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=AXi1kS8HBtbDKo5mzYSNgA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps engineer",
    "company": "OVA.Work",
    "location": "Alpharetta, GA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ova-work-4310657957?position=40&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=%2BHwW%2Bwlnfj58NfLYbFWWIQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Hudu",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-hudu-4323191230?position=41&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=yWUJo1XK8E8wy14zlUcT%2FA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Verra Mobility",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-verra-mobility-4335667666?position=42&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=QyVs%2BAwmihzBxD%2BMm7sVwQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Staff Engineer: DevOps",
    "company": "Dispel",
    "location": "Austin, TX",
    "link": "https://www.linkedin.com/jobs/view/staff-engineer-devops-at-dispel-4339045806?position=43&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=oJ0MDuUbvGSD0MtScPrqMQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "IT Automation LLC",
    "location": "Cary, NC",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-it-automation-llc-4324192032?position=44&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=yTt3taAiwpxFTLScwfkz9A%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CHEQUESPREAD PLC",
    "location": "Valley Forge, PA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-chequespread-plc-4288904252?position=45&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=D4VuwLAAQAje%2BsAr1Rlhwg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Junior DevOps Engineer",
    "company": "eSimplicity",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/junior-devops-engineer-at-esimplicity-4315888714?position=46&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=2AjRyUyX9CO6sKBbB04c7g%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud/DevOps Engineer",
    "company": "Tagup, Inc.",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-engineer-at-tagup-inc-4333051833?position=47&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=452WoIW8Bc9tUB%2FejOolbw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mark43",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mark43-4309062970?position=48&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=0tYgSn1V6eO90Zpztd%2FJwg%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "CMG (Capital Markets Gateway)",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cmg-capital-markets-gateway-4338419750?position=49&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=522GVKb5Ak%2FRbe2f6D9YpQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Mintlify",
    "location": "San Francisco, CA",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-mintlify-4318506680?position=50&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=9gkruYr0dFveTD5RTzDWeA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Support Engineer",
    "company": "Porter",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-support-engineer-at-porter-4295124575?position=51&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=abB%2FSXPFqi5lcNOnzIaKYw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Northstrat Incorporated",
    "location": "Columbia, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-northstrat-incorporated-4304125676?position=52&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=YfltAawkbLHQ7T8n9vY4BA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Paramount",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-paramount-4335876548?position=53&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=lORSZZryxRM1yEL9SCMTHA%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "RSC2, Inc.",
    "location": "Hanover, MD",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-rsc2-inc-4311252822?position=54&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=SQ4IeFXOyVVhd3%2BfHpA1%2Bw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "SmartVault",
    "location": "Houston, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-smartvault-4297941616?position=55&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=GuiwHwLy3ZG2BbzYOPwT0Q%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Cloud DevOps Support Engineer",
    "company": "Nihon Kohden Digital Health Solutions",
    "location": "Irvine, CA",
    "link": "https://www.linkedin.com/jobs/view/cloud-devops-support-engineer-at-nihon-kohden-digital-health-solutions-4295710052?position=56&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=kBAuZt61srQhNV4WzjIsHQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "PingWind",
    "location": "United States",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-pingwind-4316019938?position=57&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=2iTVJo5CA8fmNaYOg1SBlw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "Senior DevOps Engineer",
    "company": "Industrial Color",
    "location": "New York, NY",
    "link": "https://www.linkedin.com/jobs/view/senior-devops-engineer-at-industrial-color-4338405772?position=58&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=Yuwsu%2B6MgxjOSi%2FOP569SQ%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Ryan",
    "location": "Dallas, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-ryan-4284462825?position=59&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=TfK5tp6iAHP47Y9VIDxbdw%3D%3D",
    "description": "N/A"
  },
  {
    "title": "DevOps Engineer",
    "company": "Cymertek Corporation",
    "location": "San Antonio, TX",
    "link": "https://www.linkedin.com/jobs/view/devops-engineer-at-cymertek-corporation-4336305401?position=60&pageNum=0&refId=fSO1Rie3CWefyAk74vn6dQ%3D%3D&trackingId=KngWF16PfjDQO0xCzJ1O2A%3D%3D",
    "description": "N/A"
  }
]